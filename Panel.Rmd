---
output:
  pdf_document: default
  html_document: default
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

# Panel Data Models {#panel}

To load the dataset and necessary functions: 

```{r, echo=TRUE, message=FALSE, results='hide'}
# This function 1. checks if the packages are installed. 2. It installs the packages if they were not in the list of installed packages. 3. It loads the packages into the workspace
# devtools::install_github("ccolonescu/PoEdata")
PACKAGES<-c(
  "tidyverse",  # for data manipulation and ggplots
  "knitr", # knit functions
  "stringr", 
  # "kableExtra", # extended knit functions for objects exported from other packages
  "huxtable", #  Regression tables, broom compatible
  "modelsummary", 
  "stargazer", # Regression tables
  "AER", #  Functions, data sets, examples, demos, and vignettes for the book Christian Kleiber and Achim Zeileis (2008)
  "PoEdata", # R data sets for "Principles of Econometrics" by Hill, Griffiths, and Lim, 4e, Wiley. https://github.com/ccolonescu/PoEdata
  "wooldridge",  # Wooldrige Datasets
  "car", 
  "plm", # Panel data models
  "pglm", # Panel maximum likelihood estimator
  "splm", # ML and GM estimation and diagnostic testing of econometric models for spatial panel data
  "pder", # Panel data datasets
  "fixest", # Panel data models
  "lmtest", # Testing linear regression models
  "lfe", # extended Panel data models
  "lme4", # Linear mixed-effects models
  "pmdplyr", # Panel maneuvers in dplyr
  "collapse", # Advanced and Fast Data Transformation in R suitable for panel data structure
  "magrittr") #  pipes
inst<-match(PACKAGES, .packages(all=TRUE))
need<-which(is.na(inst))
if (length(need)>0) install.packages(PACKAGES[need])
lapply(PACKAGES, require, character.only=T)
```

## Pooled cross-sections

Pooled cross sections consist of random samples from the same population at different points in time. 


**Changes to return to education and teh gender wage gap**

The data set `cps78_85` includes two pooled cross-sections for the year 1978 and 1985. The dummy variable **y85** is equal to one for observations in 1985 and 0 for observations in 1978. 

We estimate a model for


$$
\log(w) = \beta_0 + \delta_{0}y85 + \beta_1 educ + \delta_{1} (y85 \times educ) +  \beta_{2} exper +  \beta_{3} \frac{exper^2}{100} + \beta_{4} union + \beta_{5}female + \delta_{2} (y85 \times female) + \epsilon
$$


```{r, message=FALSE}
data("cps78_85", package="wooldridge")
attach(cps78_85)
head(cps78_85)
```



```{r}
cps_ols <- lm(lwage ~ y85*(educ + female) + exper + I((exper^2)/100) + union, data = cps78_85)
summary(cps_ols)
```


```{r}
stargazer(cps_ols, type = "text")
```



Return to education
```{r}
df_cps_ols <- tidy(cps_ols)
# Singe male
delta1 <- df_cps_ols %>%
  dplyr::filter(term == "y85:educ") %>% 
  dplyr::select(estimate) %>% 
  pull() # pull out the single coefficient value of the dataframe
```

The return to education is estimated to have increased by: 

```{r}
delta1
```


Gender wage gap
```{r}
df_cps_ols <- tidy(cps_ols)
# Singe male
beta5 <- df_cps_ols %>%
  dplyr::filter(term == "female") %>% 
  dplyr::select(estimate) %>% 
  pull() # pull out the single coefficient value of the dataframe
```

The gender wage gap in absolute value is: 
```{r}
beta5
```

Gender wage gap in 1985

```{r}
delta5 <- df_cps_ols %>%
  dplyr::filter(term == "y85:female") %>% 
  dplyr::select(estimate) %>% 
  pull() # pull out the single coefficient value of the dataframe
```

```{r}
delta5
```

The gender wage gap decreased in absolute value to:

```{r}
beta5+delta5
```



## Organizing Panel Data

For the calculations used by panel data methods, we have to make sure that the data is systemically organized. Usually panel data comes in a _long_ structure where each row corresponds to one combination of $i$ and $t$. We have to introduce an index variable to define which observations belong together. In addition to the index variable for the cross-sectional unit, an index variable for the time dimension $t$ can also be specified. 



The **plm** and the **felm** package are the widely used panel data packages in R. For certain panel data model classes (multilevel and hierarchical models) the **lme4** package is most powerful. The currently fasted panel data routines in R are implemented **fixest** ^[The package `lme4` is widely used for political science panel data models but also for computational biologists applications. Another good package doing panel data analysis is `panelr`. The `brms` package allows fitting certain panel data models using a Bayesian approach.].  One of the many packages that help you analyze panel data using `dplyr` style of manipulating the data is the package **pmdplyr**. 

Certain packages use a specific panel data type class (pdata.frame) such as the plm package whereas others work with the standard data.frame type, as long as unique panel ids (indexes) are provided.  


### Organizing Panel Data with the plm package

For unbalanced panels I can specify a panel data frame with `pdata.frame(mydf, index = c("ivar", "tvar"))` and for balanced panels one can simply define the index `pdata.frame(mydf, index = n)`. 


```{r}
data("crime2", package = "wooldridge")
attach(crime2)
head(crime2, 5)
```

Wooldrige remarks that the study observations result in a balanced panel of 46 cities, properly sorted. 

```{r}
crime2.p <- pdata.frame(crime2, index = 46)
```

```{r}
crime2.p[1:6,c("id","time","year","pop","crimes","crmrte","unem")]
```



### Organizing Panel Data with the **pmdply** package

```{r}
data("Scorecard")
```

Scorecard observations are uniquely identified by college ID *united* and year *year*. 


```{r}
Scorecard <- as_pibble(Scorecard, .i = unitid, .t = year)
```

Convert the pibble type to pdata.frame type

```{r}
Scorecard.pdata <- panel_convert(Scorecard, "pdata.frame")
```

### Panel specific computations

The the **plm** and the **pmdply** can be used to perform panel specific computations. We focus on the methods provided by the **plm** package. 



| Code           | Description                                       | Formula                      |
|----------------|---------------------------------------------------|------------------------------|
| l = lag(x)     | Lag                                               | $l_{it} = x_{it}-1$        |
| d = diff(x)    | Difference $\Delta x_{it}$                        | $d_{it} = x_{it} - x_{it-1}$ |
| b = between(x) | Between transformation $\bar{x_{i}}$ (length $n$) | $b_i = \frac{1}{T_i}\sum^{T_i}_{t=1}x_{it}$                                                                |
| B = Between(x) | Between transformation $\bar{x_{i}}$ (length $N$) | $B_{it} = b_i$               |
| w = within(x)  | Within transformation (demeaning) $\ddot{x}_{it}$ |$w_{it} = x_{it} - B_{it}$    |



```{r}
data("crime4")
```

```{r}
crime4.p <- pdata.frame(crime4, index=c("county","year") )

crime4.p$cr.l <- lag(crime4.p$crmrte)
crime4.p$cr.d <- diff(crime4.p$crmrte)
crime4.p$cr.b <- between(crime4.p$crmrte)
crime4.p$cr.B <- Between(crime4.p$crmrte)
crime4.p$cr.W <- Within(crime4.p$crmrte)
```

```{r}
crime4.p[1:8,c("county","year","crmrte","cr.l","cr.d","cr.b","cr.B","cr.W")]
```

## First Differenced Model

The first difference estimator can be calculated simply applying OLS to the differenced values. The observations for the first year with missing observation are automatically dropped from the estimation sample. 


```{r}
data(crime2, package='wooldridge')

crime2.p <- pdata.frame(crime2, index=46 )
```

```{r}
# manually calculate first differences:
crime2.p$dcrmrte <- diff(crime2.p$crmrte)
crime2.p$dunem   <- diff(crime2.p$unem)

# Display selected variables for observations 1-6:
crime2.p[1:6,c("id","time","year","crmrte","dcrmrte","unem","dunem")]
```


```{r}
# Estimate FD model with lm on differenced data:
coeftest( lm(dcrmrte~dunem, data=crime2.p))

# Estimate FD model with plm on original data:
coeftest( plm(crmrte~unem, data=crime2.p, model="fd") )

```

Another example

```{r}
data(crime4, package='wooldridge')

crime4.p <- pdata.frame(crime4, index=c("county","year") )
pdim(crime4.p)

```

```{r}
# manually calculate first differences of crime rate:
crime4.p$dcrmrte <- diff(crime4.p$crmrte)

# Display selected variables for observations 1-9:
crime4.p[1:9, c("county","year","crmrte","dcrmrte")]
```

```{r}
# Estimate FD model:
coeftest( plm(log(crmrte)~d83+d84+d85+d86+d87+lprbarr+lprbconv+ 
                  lprbpris+lavgsen+lpolpc,data=crime4.p, model="fd") )
```

## Fixed Effects Estimation


Instead of first differencing, we get rid of the unobserved individual effect $a_i$ using the within transformation. Using the `model = "within` argument from the `plm` function, we obtain degrees of freedom, which are adjusted for the demeaning which also corrects the variance-covariance matrix and standard errors. 


```{r}
data(wagepan, package='wooldridge')

# Generate pdata.frame:
wagepan.p <- pdata.frame(wagepan, index=c("nr","year") )

pdim(wagepan.p)
```

```{r}
# Estimate FE model
summary( plm(lwage~married+union+factor(year)*educ, 
                                        data=wagepan.p, model="within"))
```






## Random Effects Estimation

The random effects (RE) model assumes that the unobserved effects of $a_i$ are independent of (or at least uncorrelated) with the regressors $x_{itj}$ for all $t$ and $j=1,\dots,k$. Therefore, our main motivation for using FD or FE disappears as OLS can consistently estimate the models parameters. 
However, like in the situation with heteroskedasticity and autocorrelation, we can obtain more efficient estimates if we take into account the structure of the variances and covariances of the error term. 


To estimate a RE model using `plm`, the argument option is set to `model = "random"`. 

```{r}
data(wagepan, package='wooldridge')

# Generate pdata.frame:
wagepan.p <- pdata.frame(wagepan, index=c("nr","year") )

pdim(wagepan.p)
```

Check for each individual if its associated time periods are consecutive (no "gaps" in time dimension per individual). 

```{r}
all(is.pconsecutive(wagepan.p))
```


Check variation of variables within individuals

```{r}
pvar(wagepan.p)
```


```{r}
wagepan.p$yr<-factor(wagepan.p$year)

reg.po<- (plm(lwage~educ+black+hisp+exper+I(exper^2)+married+union+yr, 
                                      data=wagepan.p, model="pooling") ) # OLS
reg.re <- (plm(lwage~educ+black+hisp+exper+I(exper^2)+married+union+yr, 
                                      data=wagepan.p, model="random") )
reg.fe <- (plm(lwage~                      I(exper^2)+married+union+yr, 
                                      data=wagepan.p, model="within") )
```



```{r}
# Pretty table of selected results (not reporting year dummies)
stargazer(reg.po,reg.re,reg.fe, type="text", 
          column.labels=c("Pooling","RE","FE"),keep.stat=c("n","rsq"),
          keep=c("ed","bl","hi","exp","mar","un"))
```



For the _random_ effects model, the summary method gives information about the variance of the error. 

```{r}
summary(reg.re)
```

Fixed effects might be extracted easily using `fixef`.  By setting the type argument, the fixed effects may be returned in levels ("level"), as deviations from the first value of the index ("dfirst"), or as deviations from the overall mean ("dmean"). If the argument vcov was specified, the standard errors (stored as attribute "se" in the return value) are the respective robust standard errors.

```{r}
fixef_reg.fe <-plm::fixef(reg.fe)
head(summary(fixef_reg.fe))
```

The summary methods prints the effects (in deviations from the overall intercept), their standard errors and the test of equality to the overall intercept. 

In case of a two-ways effect model, an additional argument `effect` is required to extract fixed effects.

```{r}
reg.fe2w <- (plm(lwage~I(exper^2)+married+union+yr, 
                                      data=wagepan.p, model="within") )
```

```{r}
summary(reg.fe2w)
```


```{r}
fixef_reg.fe2w <- plm::fixef(reg.fe2w)
head(summary(fixef_reg.fe2w, effect = "time"))
```

## Tests on panel data models


### F and LM test

```{r}
# Test whether the fixed effects are significant. H0: FE are irrelevant
pFtest(reg.fe, reg.po)
```

We can reject the null hypothesis. The Fixed effects are significant.

```{r}
# Lagrange Multiplier Test - (Breusch-Pagan) for balanced panels
# Tests whether pooled OLS (H0) or Random effects should be used
plmtest(reg.po, effect="individual", type="bp")
plmtest(reg.po, effect="time", type="bp")
```


### Hausman test

The RE estimator needs stronger assumptions to be consistent than the FE estimator. On the other hand, it is more efficient if in these assumptions hold and we can include time constant regressors. A widely used test of this additional assumption is the Hausman test. 


```{r}
phtest(reg.fe, reg.re)
```

With a p-value of 0.003, the null hypthosis of the RE model being consistent is rejected. 
Robust Hausman test

The presence of heteroskedasdicity can matter for the outcome of the Hausman test. 

```{r}
data("RDSpillovers", package = "pder")
pehs <- pdata.frame(RDSpillovers, index = c("id", "year"))
ehsfm <- lny ~ lnl + lnk + lnrd # just the random effects model formula to the vcov
```

Regression based Hausman test

```{r}
phtest(ehsfm, pehs, method = "aux") 
```

Regression based Hausman test with robust estimator for the covariance. 

```{r}
phtest(ehsfm, pehs, method = "aux", vcov = vcovHC) 
```

The robust version of the Hausman test does not reject the random effects hypothesis anymore.The robust version of the Hausman test does not reject the random effects hypothesis anymore.  


### Serial correlation

 - Breusch-Godfrey/Durbin Watson test for serial correlation in panel models

```{r}
data("RiceFarms", package = "splm")
pdim(RiceFarms, index = "id")
```


```{r}
fm <- log(goutput) ~ log(seed) + log(totlabor) + log(size)
rice.re <- plm(fm, Rice, model='random')
```



```{r}
pbgtest(rice.re, order = 2)
pdwtest(rice.re, order = 2)
```


 - Wald Tests for Serial correlation using *within* and First-difference Estimators 

This test is applicable to an FE panel model and in particular to "short" panels with small $T$ and large $N$. 

```{r}
data("EmplUK", package = "plm")
pwartest(log(emp) ~ log(wage) + log(capital), data = EmplUK)
```

We strongly reject the null of no serial correlation. The evidence of serial correlation is so strong that we should think whether the residuals are stationary at all. A first difference specification might be preferable. 


```{r}
pwfdtest(log(emp) ~ log(wage) + log(capital), data = EmplUK)
```

We can't reject the null hypothesis, indicating that a first difference specification removes the residual serial correlation. 


However, results are not always clear cut. 


```{r}
W.fd <- matrix(ncol = 2, nrow =2)
H0 <- c("fd", "fe")
dimnames(W.fd) <- list(c("test", "p-value"), H0)
for(i in H0) {
    mytest <- pwfdtest(fm, Rice, h0 = i)
    W.fd[1, i] <- mytest$statistic
    W.fd[2, i] <- mytest$p.value
    }
round(W.fd, 6)
```

Both specifications reject the null. Whichever estimate you run, you will have serially correlated errors. In that case it is advisable to use an autocorrelation-robust covariance matrix. 

### Cross-sectional dependence

 - Pesaran CD test for cross-sectional dependence in panels

Cross-sectional dependence can arise if individuals respond to common shocks or if spatial diffusion processes are present ("shock" affecting individuals on a measure of distance). 

```{r}
data("HousePricesUS", package = "pder")
php <- pdata.frame(HousePricesUS)
```


```{r, warning=FALSE}
cbind("rho"   = pcdtest(diff(log(php$price)), test = "rho")$statistic,
      "|rho|" = pcdtest(diff(log(php$price)), test = "absrho")$statistic)
```


The overall averages $\hat\rho$ and $\hat\rho_{ABS}$ are quite large in magnitude and very close to each other, indicating substantial positive correlation. 

We can analyse whether this spatial dependence is geographically uniform or not. 

```{r}
regions.names <- c("New Engl", "Mideast", "Southeast", "Great Lks",
                   "Plains", "Southwest", "Rocky Mnt", "Far West")
corr.table.hp <- cortab(diff(log(php$price)), grouping = php$region, # cortab is from plm package
                        groupnames = regions.names)
colnames(corr.table.hp) <- substr(rownames(corr.table.hp), 1, 5)
round(corr.table.hp, 2)
```


There seems to be a strong dependence with neighbouring regions. 



## General Feasible Generalized Least Squares (FGLS) Models

The function `pggls` estimates general FGLS models with either fixed or "random" effects^[The "random" effect is better termed "general FGSLS" models, as in fact it does not have a proper random effects structure.].

```{r}
data("Produc", package = "plm")
```


```{r}
reg.fgls <- pggls(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, data = Produc, model = "pooling") # Note: pooling stands for "random" fgls
summary(reg.fgls)
```

Whereas the general FGLS estimator is based in the first step on an OLS model, the fixed effects _pggls_ is based on the estimation of a within model in the first step; the rest is the same. 

```{r, eval=F}
reg.fgls.fe<- pggls(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, data = Produc, model = "within")
summary(reg.fgls.fe)
```

## Dummy Variable Regression and Correlated Random Effects

We can get the FE parameter estimates in two other ways than the within transformation. The dummy variable equation uses OLS on the original variables instead of the transformed ones. But it adds $n-1$ dummy variables (or $n$ dummies and removes the constant), one for each cross-sectional unit $i = 1, \dots, n$. The simplest (although not the most computationally efficient) way to implement this in R is to use the cross-sectional index as another factor variable. The third way is the correlated random effects (CRE) approach. Instead of assuming that the individual affects $a_i$ are independent on the regressors $x_{itj}$, it is assumed that they only depend on the averages over time $\bar{x}_{ij} = \frac{1}{T_{i}} \sum^{2}_{i=1}T_{i}$. 

```{r}
data(wagepan, package='wooldridge')

# Generate pdata.frame:
wagepan.p <- pdata.frame(wagepan, index=c("nr","year") )
```

```{r}
# Estimate FE parameter in 3 different ways:
wagepan.p$yr<-factor(wagepan.p$year)
reg.fe <-(plm(lwage~married+union+yr*educ,data=wagepan.p, model="within"))
reg.dum<-( lm(lwage~married+union+yr*educ+factor(nr), data=wagepan.p))
reg.re <-(plm(lwage~married+union+yr*educ,data=wagepan.p, model="random"))
reg.cre<-(plm(lwage~married+union+yr*educ+Between(married)+Between(union)
                                         ,data=wagepan.p, model="random"))
```


```{r}
stargazer(reg.fe,reg.dum,reg.cre,reg.re,type="text",model.names=FALSE,
          keep=c("married","union",":educ"),keep.stat=c("n","rsq"),
          column.labels=c("Within","Dummies","CRE","RE"))
```


Given that we estimated the CRE model, it is easy to the null hypothesis that the RE estimator is consistent. 

```{r}
# RE test as an F test on the "Between" coefficients 
linearHypothesis(reg.cre, matchCoefs(reg.cre,"Between"))
```

Another advantage of the CRE approach is that we can add time-constant regressors to the model. Since we cannot control for average values of $\bar{x}_{ij}$ for these variables, they have to be uncorrelated with $a_i$ for consistent estimation of _their_ coefficients. For the other coefficients of the time-varying variables, we still don't need these additional RE assumptions. 

```{r}
data(wagepan, package='wooldridge')

# Generate pdata.frame:
wagepan.p <- pdata.frame(wagepan, index=c("nr","year") )
```

```{r}
# Estimate CRE parameters
wagepan.p$yr<-factor(wagepan.p$year)
summary(plm(lwage~married+union+educ+black+hisp+Between(married)+
                         Between(union), data=wagepan.p, model="random"))
```

## Robust (Clustered) Standard Errors

Under the RE assumptions, OLS is inefficient but consistent. Instead of using RE, we could use OLS but would have to adjust the standard errors for the fact the composite error term $v_{it} = \alpha_{i}+ u_{it}$ is correlated over time because of the constant individual effect $\alpha_{i}$. In fact, the variance-covariance matrix could be more complex than the RE assumptions with i.i.d. $u_{it}$ implies. These errors terms could be serially correlated and/ or heteroscedastic. This would invalidate the standard errors not only of OLS but also of FD, FE, RE and CRE. 

Especially in large cross-sectional data, formulas exist for the variance-covariance matrix for panel data that are robust with respect to heteroskedasticity and _arbitrary_ correlations of the error term within a cross-sectional unit (or "cluster"). 

Different versions of clustered variance-covariance matrix can be computed with the command `vcovHC` from the `plm` package^[Note that `vcovHC` from the `plm` package is different from the `vcovHC` function of the `sandwich` package.]. The `fixest` package also handles multiway clustered errors. 

The next chunk estimates a familiar FD regression but also reports the regression table with clustered standard errors and respective $t$ statistics in addition to the non-adjusted standard errors. 

```{r}
data(crime4, package='wooldridge')

# Generate pdata.frame:
crime4.p <- pdata.frame(crime4, index=c("county","year") )
```

```{r}
# Estimate FD model:
reg <- ( plm(log(crmrte)~d83+d84+d85+d86+d87+lprbarr+lprbconv+ 
                   lprbpris+lavgsen+lpolpc,data=crime4.p, model="fd") )
```


```{r}
# Regression table with standard SE
coeftest(reg)
```

```{r}
# Regression table with "clustered" SE (default type HC0):
coeftest(reg, vcovHC)
```

```{r}
# Regression table with "clustered" SE (small-sample correction)
# This is the default version used by Stata and reported by Wooldridge:
coeftest(reg, vcovHC(reg, type="sss"))
```


### Panel Data with Instrumental Variables

Consider a model estimating the demand (measured in number of packs) for cigarettes

$$
log(Q_{it} = \beta_0 + \beta_1log(P_{it})+\beta_2log(income_{it})+ e_{it})
$$
  
  As an IV for the price we use

- $SalesTax_{it}$: the proportion of taxes on cigarettes arising from the general sales tax. 
  - Relevant as it is included in the after-tax price
  - Exogenous(independent) since the sales tax does not influence demand directly, but indirectly through the price.
- $CigTax_{it}$: CigTaxit

```{r}
data("CigarettesSW")
Cigdata <- CigarettesSW %>% 
  mutate( rincome = (income / population) / cpi) %>% 
  mutate( rprice  = price / cpi ) %>% 
  mutate( salestax = (taxs - tax) / cpi ) %>% 
  mutate( cigtax = tax/cpi ) 
```

```{r}
# OLS
result_1 <- felm( log(packs) ~ log(rprice) + log(rincome)  | 0 | 0 | state, data = Cigdata )
```

```{r}
# State FE
result_2 <- felm( log(packs) ~ log(rprice) + log(rincome)  | state | 0 | state, data = Cigdata )
```

```{r}
# IV without FE
result_3 <- felm( log(packs) ~ log(rincome)  | 0 | (log(rprice) ~  salestax + cigtax) | state, data = Cigdata )
```

```{r}
# IV with FE 
result_4 <- felm( log(packs) ~ log(rincome)  | state | (log(rprice) ~  salestax + cigtax) | state, data = Cigdata )
```

```{r}
stargazer::stargazer(result_1, result_2, result_3, result_4, type = "text")
```

## Unbalanced panels 

For unbalanced panels, the number of observations for each individual is now individual specific ($T_{n}$). Compared to the balanced panel case, three complications appear: 

1. The covariance matrix of the errors cannot be written anymore as a linear combination of idempotent and mutually orthogonal matrices. 
2. For models with just one fixed effect, the *within* transformation still consists of removing the individual mean from the variable. On the contrary, for models with multiple fixed effects, the *within* transformation is not obtained by performing the difference with the individual and time means (or means of another FE), as in the balanced panel case, but requires more tedious matrix algebra. 
3. To estimate the components of the variance, the quadratic forms of the residuals is calculated as in the balanced panel case, but there is no obvious choice of the denominator. 
The *Tilerseries* dataset contains data on the weekly production of cement floor tiles for 25 small-scale tileries in 1982-1983. The data are observed for 66 weeks in total and aggregated on periods of three weeks. 

```{r}
data("Tileries", package = "pder")
head(Tileries, 3)
pdim(Tileries)
```
The number of observations for each firm ranges from 12 to 22.  

We estimate a Cobb-Douglas production function where the production is measured as output and the quantity of inputs is measured as labour and machines. 

```{r}
Tileries <- pdata.frame(Tileries)
plm.within <- plm(log(output) ~ log(labor) + log(machine), Tileries)
y <- log(Tileries$output)
x1 <- log(Tileries$labor)
x2 <- log(Tileries$machine)
lm.within <- lm(I(y - Between(y)) ~ I(x1 - Between(x1)) + I(x2 - Between(x2)) - 1)
lm.lsdv <- lm(log(output) ~ log(labor) + log(machine) + factor(id), Tileries)
```

First, use a a linear model with OLS on the *within* transformation. 


Estimate a one-way random effects model

```{r}
tile.r <- plm(log(output) ~ log(labor) + log(machine), Tileries, model = "random")
```

```{r}
summary(tile.r)
```


The transformation parameter $\tau$ in the RE model is individual specific. More precisely, it depends on the number of available observations for every individual. This parameter ranges from around 0.489 to 0.598. 


```{r}
modelsummary(list("OLS within" = lm.within,"OLS Dummy" = lm.lsdv, "RE" = tile.r))
```

Estimate a two-way fixed effects model 

```{r}
plm.within <- plm(log(output) ~ log(labor) + log(machine),
                  Tileries, effect = "twoways")
```

We can show that for the unbalanced panel, we can not manually obtain the same coefficients for a linear models estimated with OLS, where the individual and time means have been removed. 

```{r}
lm.lsdv <- lm(log(output) ~ log(labor) + log(machine) +
                  factor(id) + factor(week), Tileries)
y <- log(Tileries$output)
x1 <- log(Tileries$labor)
x2 <- log(Tileries$machine)
y <- y - Between(y, "individual") - Between(y, "time") + mean(y)
x1 <- x1 - Between(x1, "individual") - Between(x1, "time") + mean(x1)
x2 <- x2 - Between(x2, "individual") - Between(x2, "time") + mean(x2)
lm.within <- lm(y ~ x1 + x2 - 1)
```


```{r}
modelsummary(list("OLS within" = lm.within,"OLS Dummy" = lm.lsdv, "Toway FE" = plm.within))
```

Note that we obtain the same coefficients for the LSDV and FE model but not for the individual and time mean corrected OLS model. 

Finally we estimate a two-way RE model with different estimators. 

```{r}
wh <- plm(log(output) ~ log(labor) + log(machine), Tileries,
          model = "random", random.method = "walhus",
          effect = "twoways")
am <- update(wh, random.method = "amemiya")
sa <- update(wh, random.method = "swar")

```

```{r}
ercomp(sa)
```

The shares of the individual in the total error variance are between 0.4934 and 0.6019 and for the and the time effects between 0.1961 and 0.370 when the Swamy-Arora estimator is used. 


```{r}
re.models <- list(walhus = wh, amemiya = am, swar = sa)
```

Coefficients across different RE using the three methods of estimation.

```{r}
sapply(re.models, coef)
```

The differences in the estimated coefficients are as expected not vary large. 

Extract the total error variance from the three methods of estimation.

```{r}
sapply(re.models, function(x) sqrt(ercomp(x)$sigma2))
```


## Unrestricted Generalized Least Squares

The GLS estimator is a matrix-weighted average of the *between* and the *within* estimator. The combination of both estimators is more efficient than either of these alone. 

The "random effect" equivalent, general GLS, is estimated by specifying the *model* argument as `pooling`

```{r}
data("EmplUK", package = "plm")
pdim(EmplUK)
```


```{r}
gglsmod <- pggls(log(emp) ~ log(wage) + log(capital),
                 data = EmplUK, model = "pooling")
summary(gglsmod)
```

Group covariance matrix of errors 

```{r}
round(gglsmod$sigma,3)
```

The correlations between the pairs of residuals (in time) for the same individual do do not die in out with the distance in time, resembling very much the random effects structure. 


Fixed effects GLS (FEGLS)

```{r}
feglsmod <- pggls(log(emp) ~ log(wage) + log(capital), data = EmplUK,
                  model = "within")
summary(feglsmod)
```

Run a Hausman test

```{r}
phtest(feglsmod, gglsmod)
```

The Hausman test clearly rejects the Null Hypthosis. We favour the FEGLS specification. 

If we assume (or analyse) that the residuals of FEGLS regression are not stationary, we can also estimate the model in first differences. 

First difference GLS (FDGLS)


```{r}
fdglsmod <- pggls(log(emp) ~ log(wage) + log(capital), data = EmplUK,
                  model = "fd")
summary(fdglsmod)
```




## Maximum Liklihood Estimator 

An alternative to OLS estimator is the maximum likelihood estimator. Contrary to the GLS estimator, the parameters are not estimated sequentially but simultaniously. 

The maximum likelihood estimator is available in the **pglm** package. We have to specify the distribution of the errors of the model by setting the *family* argument. 

```{r}
data("RiceFarms", package = "splm")
Rice <- pdata.frame(RiceFarms, index = "id")
```



```{r}
rice.ml <- pglm(log(goutput) ~ log(seed) + log(totlabor) + log(size),
                data = Rice, family = gaussian)
```


```{r}
summary(rice.ml)
```

The two parameters `sd.idios` and `sd.id` are the estimated standard deviations of the idiosyncratic and of the individual parts of the *error*. 


## Dynamic Panel Data Models

In the context of panel data, we usually must deal with unobserved heterogeneity by applying the within (demeaning) transformation, as in one-way fixed effects models, or by taking first differences if the second dimension of the panel is a proper time series.

The ability of first differencing to remove unobserved heterogeneity also underlies the family of estimators that have been developed for dynamic panel data (DPD) models. These models contain one or more lagged dependent variables, allowing for the modelling of a partial adjustment mechanism.

The econometric community distinguishes roughly between two types of dynamic panel data models: 

 - **Micro-panels** where $N>>T$
 - **Macro-panels** where $N \leq T$

### Micro panel 

Dynamic panel data models are estimated using the GMM estimator suggested by  @Arellano1991, utilizing lagged endogenous regressors after a first-differences transformation. 

Through the whole chapter we will use a dataset capturing the relationship between Democracy and Income from a study by Acemoglu, Johnson, Robinson, and Yared (2008). The data is spread across two different datasets, containing first observations for 211 countries for a period of 1950 to 2000 and data from 1850 to 2000 for 25 countries. 

```{r}
data("DemocracyIncome", package = "pder")
data("DemocracyIncome25", package = "pder")
```

The provided dataset shows the 5 year average for a given observation. I extract the first 4 characters from the year column to obtain the year starting point for a given observation.

```{r}
dat1 <- DemocracyIncome %>%
  mutate(year = stringr::str_extract(year, "^.{4}")
)
  
```

Plot some of the data

```{r}
set.seed(1)
di2000 <- subset(dat1, year == 2000,
                 select = c("democracy", "income", "country"))
di2000 <- na.omit(di2000)
di2000$country <- as.character(di2000$country)
di2000$country[- c(2,5, 23, 16, 17, 22, 71,  125, 37, 43, 44, 79, 98, 105, 50, 120,  81, 129, 57, 58,99)] <- NA
```


```{r}
ggplot(di2000, aes(income, democracy, label = country)) + 
    geom_point(size = 0.4) + 
    geom_text(aes(y= democracy + sample(0.03 * c(-1, 1), 
                                        nrow(di2000), replace = TRUE)),
                  size = 2) +
    theme(legend.text = element_text(size = 6), 
          legend.title= element_text(size = 8),
          axis.title = element_text(size = 8),
          axis.text = element_text(size = 6))

```

```{r}
pdim(DemocracyIncome)
head(DemocracyIncome, 4)
```

Estimate an OLS model with *within* time fixed effects. 

```{r}
mco <- plm(democracy ~ lag(democracy) + lag(income), 
           DemocracyIncome, index = c("country", "year"), 
           model = "within", effect = "time",
           subset = sample == 1)
```

```{r}
coef(summary(mco))
```

The coefficients for democracy show a high persistence. Lagged income also shows a high significantly positive influence on the democracy index.  However, we know the OLS estimator suffers from a positive bias. 

Next estimate a two-way fixed effects model. 

```{r}
within <- update(mco, effect = "twoways")
coef(summary(within))
```

With respect to the OLS model, the autoregressive coefficient is smaller. After introducing country fixed effects, the coefficient of income is close to 0 and not significant anymore. 

Now we estimate the model with the Anderson and Hsiao (1982) estimator. For that one must specify that both the regressand and regressors are differenced and that the lagged endogenous variable in differences is instrumented wit the endogenous in levels lagged two periods. Acemoglu et al. (2008) use the second lag of per capita income as a second instrument. 

```{r}
ahsiao <- plm(diff(democracy) ~ lag(diff(democracy)) + 
              lag(diff(income)) + year - 1  | 
              lag(democracy, 2) + lag(income, 2) + year - 1, 
              DemocracyIncome, index = c("country", "year"),
              model = "pooling", subset = sample == 1)
coef(summary(ahsiao))[1:2, ]
```

Anderson and Hsiao (1982)'s model being consistent, one expects the estimated autoregressive coefficient to be comprised between that of the *within* model (biased downwards) and that of the OLS model (biased upwards). 

### GMM Estimation of the Differenced Model 

The IV approach presented in the previous section is inefficient for two reasons: 

1. It does not account for the correlation induced into the errors by first-differencing
2. There are further valid instruments available

The **GMM** estimation of a panel data model is performed through the `plm::pgmm()` command. 
```{r}
diff1 <- pgmm(democracy ~ lag(democracy) + lag(income) | # regulars regressors
              lag(democracy, 2:99)| #  GMM instruments
              lag(income, 2), # "normal" instruments
              DemocracyIncome, index=c("country", "year"), 
              model="onestep", effect="twoways",  # Estimation procedure
              subset = sample == 1)
coef(summary(diff1))
```


The two-step model is obtained by setting the `model` argument to `twosteps`
```{r}
diff2 <- update(diff1, model = "twosteps")
coef(summary(diff2))
```

All available lags have been used and the number of instruments is sizeable. The results are near those of the Anderson and Hsiao (1982) model.  


### GMM Estimator in Differences and Levels 

The main drawback of the difference **GMM** estimator is that lagged variables of the dependent variable are often very weakly correlated with its lagged first difference. To solve this weak instruments problem, one can add moment conditions on the model in levels. 

System GMM

```{r}
sys2 <- pgmm(democracy ~ lag(democracy) + lag(income) |  # regressors 
             lag(democracy, 2:99)|  # GMM instruments
              lag(income, 2), # "normal" instruments
             DemocracyIncome, index = c("country", "year"), 
             model = "twosteps", effect = "twoways",
             transformation = "ld") # System GMM level and differences
coef(summary(sys2))
```

The autoregressive coefficient obtained with the difference and the system models are close. The income coefficient is now significantly  positive and much larger than previously. 

### Robust Estimation of the Coefficients' Covariance

The two-steps estimator, albeit being robust to the presence of heteroskedasticity and/ or autocorrelation, depends still on the residuals of the one-step model. The estimator is therefore still biased and requires a robust estimator adjustment for the variance. 

The function `vcov` computes the "classical" inconsistent version of the variance.

```{r}
sqrt(diag(vcov(diff2)))[1:2]
```

The function `vcovHC` computes the the robust version

```{r}
sqrt(diag(vcovHC(diff2)))[1:2]
```

One can see that in this example the classical variance formula seems to be biased downwards. The robust standard errors are clearly superior. 


### Overidentifcation test 

Similar to a general IV approach we can test for overidentification using Sargan-Hansen test 

```{r}
plm::sargan(diff2)
```

For the difference model the hypothesis of moments's validity is not rejected. 

```{r}
plm::sargan(sys2)
```

Also for the system GMM model the hypothesis of moment's validity is not rejected. 


### Error Serial Correlation Test 

The GMM is not consistent unless the moment conditions are verified, which, in particular, implies that the innovations are serially correlated. 

```{r}
plm::mtest(diff2, order = 2)
```

The hypothesis of no serial correlation is not rejected. 


### Macro panel 

Macro panel models are an advanced topic and are therefore not covered in this course. 


