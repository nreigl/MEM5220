---
title: "MEM5220 - Microeconometrics Self-Evaluation 1"
subtitle: "Taltech - DEF"
author: "YOUR NAME HERE"
date: "`r format(Sys.time(), '%d %B, %Y')`"
latex_engine: xelatex
output:
  html_document:
    df_print: paged
    number_sections: true
  pdf_document: 
    number_sections: true
fontsize: 12pt
---

```{r setup, include=F}
# ignore this fiddly bit
knitr::opts_chunk$set(comment=NA)
library(knitr)
```

---

# Preface {-}

This first R econometrics self-evaluation assignment is focused on data cleaning, data manipulation, plotting and estimating and interpreting simple linear regression models.  
Some packages I have used to solve the exercises: 

```{r, message=FALSE}
library(tidyverse)
library(modelsummary)
library(broom)
library(lmtest)
library(sandwich)
library(car)
library(skimr)
library(patchwork)
library(ggpubr)
```

You can use **any** additional packages for answering the questions. 



**Note:**

 - This assignment has to be solved in this R Markdown document and you should be able to "knit" the document without errors. 
 - Fill out your name in "yaml" - block on top of this document
 - Use the R markdown syntax: 
    - Write your code in code chunks
    - Write your explanations including the equations in markdown syntax
 - If you have an error in your code use `#` to comment the line out where the error occurs but do not delete the code itself. I want to see your coding errors so I can give  feedback!

For more information on using R Markdown for class exercises see https://ntaback.github.io/UofT_STA130/Rmarkdownforclassreports.html

---


You will be working with the dataset `Caschool` from the **Ecdat** package, the dataset `Wage1` from the **wooldrige** package. In the last two exercises, you will be working with a simulated data.  

Try to answer by questions by including a code chunk and then a written answer. The answer to question 1.1 serves as a template. Please proceed with the rest of the questions in a similar way.

You can use any plotting package but the figures should have a research project style quality (eg. axis labels, figure legend, figure title and if necessary figures notes). 

---
# Caschool exercises 

## Question: 

Load the dataset `Caschool` from the **Ecdat** package. 

The Caschooldataset contains the average test scores of 420 elementary schools in California along with some additional information.

```{r, message=FALSE}
# install.packages("Ecdat")
library("Ecdat")
data("Caschool", package = "Ecdat")
```


## Question:  

What are the dimensions of the `Caschool` dataset?

**A**: 

```{r}
dim(Caschool)
```
The dataset has 420 rows and 17 columns. 

## Question:  

Does the `Caschool` dataset contain missing observations?

**A**: 

```{r}
sum(is.na(Caschool))
```

There are no missing observations in the dataset. 


## Question: 

Display the structure of the `Caschool` dataset. Which variable are encoded as factors? 

**A**: 


```{r selfeval_1_solutions-4}
str(Caschool)
```


County, district and grspan are encoded as factors.



## Question: 

Provide a summary statistic of the data. 

**A**: 


```{r selfeval_1_solutions-5}
summary(Caschool)
```

Other option is to use for example the skimr package: 

```{r}
library(skimr)
skim(Caschool)
```



## Question: 

What are the names of the variables in the dataset?

**A**: 


```{r selfeval_1_solutions-6}
names(Caschool)
```




## Question: 

How many unique observations are available in the variable "county"

**A**: 


```{r}
unique(Caschool$county)
```


## Question: 

Summarize the mean number of students grouped by county. 

**A**: 


```{r}
mean_countCaschool <- Caschool %>%
  group_by(county) %>%
  summarise(mean_count = mean(enrltot)) %>%
  arrange(desc(mean_count))
mean_countCaschool
```



## Question: 

Calculate the log of average income from of the Caschool dataset. Call the variable **logavginc** and add this variable to the dataset. Then, plot a histogram of the average income vs. a histogram of log average income. What do you observe? 


**A**: 

```{r selfeval_1_solutions-9}
Caschool$logavginc <- log(Caschool$avginc)
```



```{r selfeval_1_solutions-11}
library(patchwork)
library(ggpubr)
p1 <- ggplot(Caschool, aes(avginc)) +
  geom_histogram(show.legend = FALSE) +
  labs(title="Average income",
        x ="Avg. income")+
  theme_pubr()
p2 <- ggplot(Caschool, aes(logavginc)) +
  geom_histogram(show.legend = FALSE) +
  labs(title="Average income",
        x ="Log Avg. income")+
  theme_pubr()
```


```{r}
patchwork <- (p1 + p2)
patchwork 
```


Average income is clearly leftward-skewed. The log of averge income looks more like a normal distribution.


## Question: 

We want to create now a subset of counties that have the ten highest district average income and that have the ten lowest district average income. Call this subset *Caschool_lowhighincome*. 

**Hint**: One way is the create two subsets (eg. Cascholl_highincome and Caschool_lowincome and the use the `rbind()` function to bind them together.). 

**A**:


```{r selfeval_1_solutions-12}
Caschool_highincome <- Caschool %>%
arrange(desc(avginc)) %>%
head(10)

Caschool_lowincome <- Caschool %>%
arrange((avginc)) %>%
head(10)

Caschool_lowhighincome <- rbind(Caschool_highincome,Caschool_highincome)
```


## Question: 

Let us test wether a high student/teacher ratio will be associated with higher-than-average test scores for the school? Create a scatter plot for the full dataset (*Caschool*) for the variables **testscr** and **str**. 

**A**: 

```{r selfeval_1_solutions-14}
ggplot(mapping = aes(x = str, y = testscr), data = Caschool) + # base plot
  geom_point() + # add points
  scale_y_continuous(name = "Average Test Score") +
  scale_x_continuous(name = "Student/Teacher Ratio") +
  labs(title="Testscores vs Student/Teacher Ratio")+
  theme_pubr()
```


## Question: 

Suppose a policymaker is interested in the following linear model:

\begin{equation}
\label{eq:1.1}
testscr = \beta_0 +  \beta_1  str + u
\end{equation}

Where $testscr$ is the average test score for a given school and $str$ is the Student/Teacher Ratio (i.e. the average number of students per teacher). 

Estimate the specified linear model. Is the estimated relationship between a schools Student/Teacher Ratio and its average test results positive or negative?

**A**: 

```{r selfeval_1_solutions-15}
fit_single <- lm(formula = testscr ~ str, data = Caschool)
summary(fit_single)
```





## Question:

Now, plot the regression line for the model we have just estimated. 

**A**: 

```{r selfeval_1_solutions-17}
ggplot(mapping = aes(x = str, y = testscr), data = Caschool) + # base plot
  geom_point() + # add points
  geom_smooth(method = "lm", size=1, color="red") + # add regression line
  scale_y_continuous(name = "Average Test Score") +
  scale_x_continuous(name = "Student/Teacher Ratio") +
  labs(title="Testscores vs Student/Teacher Ratio")+
  theme_pubr()
```


## Question:

Let us extend our example of student test scores  by adding familiesâ€™ average income to our previous model:



\begin{equation}
\label{eq:1.2}
testscr = \beta_0 +  \beta_1  str +   \beta_2  avginc + u
\end{equation}

**A**:



```{r selfeval_1_solutions-18}
fit_multivariate <- lm(formula = "testscr ~ str + avginc", data = Caschool)
summary(fit_multivariate)
```

Adding the explanatory variable "avginc" to the model, the estimated coefficient of the student/ teacher ratio becomes first smaller compared to the previous model and second insignificant at conventional levels.


## Question: 

Assume know that "str" depends also on the value of yet another regressor, "avginc". Estimate the following model. Compare the sign of the estimate of $\beta_2$ and $\beta_3$. Interpret the results. 


\begin{equation}
\label{eq:1.3}
testscr = \beta_0 +  \beta_1  str +   \beta_2  avginc + \beta_3 (str \times avginc)  + u
\end{equation}

**A**: 

```{r selfeval_1_solutions-19}
fit_inter = lm(formula = testscr ~ str + avginc + str*avginc, data = Caschool)
summary(fit_inter)
```



We observe also that the estimate of $\beta_2$ changes signs and becomes negative, while the interaction effect $\beta_3$ is positive.

This means that an increase in str reduces average student scores (more students per teacher make it harder to teach effectively); that an increase in average district income in isolation actually reduces scores; and that the interaction of both increases scores (more students per teacher are actually a good thing for student performance in richer areas).


## Question: 

You have fitted 3 specifications for the Caschool example. Report the regression results of equation \ref{eq:1.1}, \ref{eq:1.2} and \ref{eq:1.3}, in a formatted table regression output table. Discuss the model fit and model selection. 



**A**: 

```{r}
library(modelsummary)
modelsummary(list(fit_single, fit_multivariate, fit_inter),stars = TRUE )
```

The adjusted $R^2$ is highest for the model 3, the model that includes an interaction term. AIC and BIC, two widely used information criteria, would also select model 3, relative to each of the other models (The relatively quality of the model is maximized when the information criterion is minimized).

# Wage1 excercises

Wage data: These are data from the 1976 Current Population Survey. Source of the data is Wooldrige. Familiarize yourself with the dataset if necessary. 

```{r, message=FALSE}
# install.packages("wooldridge")
library("wooldridge") 
data("wage1", package = "wooldridge")
```

## Question

First, estimate the following model and test again for heteroscedasticity.

\begin{equation}
\label{eq:2.1}
 wage = \beta_0 +  \beta_1  female +  \beta_3 educ + \beta_4 exper + u 
\end{equation}

**A**: 

```{r selfeval_1_solutions-27}
lm3_wage1 <- lm(wage~female+educ+exper, data=wage1)
summary(lm3_wage1)
lm3_bptest <- bptest(lm3_wage1)
lm3_bptest
```

The test statistic of the BP-test is `r lm3_bptest$statistic[[1]]` and the corresponding p-value is smaller than `r lm3_bptest$p.value[[1]] `, so we can reject homoscedasticity for all reasonable significance levels.

```{r selfeval_1_solutions-28}
# coeftest(lm3_wage1, vcov=hccm)
cov3 <- hccm(lm3_wage1, type="hc3") # hc3 is the standard method
lm3_robust <- coeftest(lm3_wage1, vcovHC)
lm3_robust
```




## Question: 

Now, estimate the following model: 

\begin{equation}
\label{eq:2.2}
 log(wage) = \beta_0 +  \beta_1 (married \times female) +  \beta_3 educ + \beta_4 exper + beta_5 exper^2 + \beta_6 tenure + \beta_7 tenure^2 + u 
\end{equation}

  1. What is the reference group in this model? 
  2. Ceteris paribus, how much more wage do single males make relative to the reference group?
  3. Ceteris paribus, how much more wage do single females make relative to the reference group?
  4. Ceteris paribus, how much less do married females make than single females?
  5. Do the results make sense economically. What socio-economic factors could explain the results? 

**A**: 


```{r selfeval_1_solutions-23}
lm2_wage1 <- lm(log(wage)~married*female+educ+exper+I(exper^2)+tenure+I(tenure^2), data=wage1)
summary(lm2_wage1)
```

```{r selfeval_1_solutions-25}
library(scales) #  percent
df_lm2_wage1 <- tidy(lm2_wage1)
# Singe male
marriedmale <- df_lm2_wage1 %>%
  filter(term == "married") %>%
  dplyr::select(estimate) %>%
  pull() # pull out the single coefficient value of the dataframe
# Single female
singlefemale <- df_lm2_wage1 %>%
  filter(term == "female") %>%
  dplyr::select(estimate) %>%
  pull() # pull out the single coefficient value of the dataframe
marriedfemale <- df_lm2_wage1 %>%
  filter(term == "married:female") %>%
  dplyr::select(estimate) %>%
  pull() # pull out the single coefficient value of the dataframe
married<- df_lm2_wage1 %>%
  filter(term == "married") %>% #
  dplyr::select(estimate) %>%
  pull() # pull out the single coefficient value of the dataframe
```

```{r}
lm2_robust <- coeftest(lm2_wage1, vcovHC)
lm2_robust
```


1. Reference group: *single* and *male*
2. Cp. married males make `r percent(marriedmale)` (` percent(marriedmale)`) more than single males.
3. Cp. a single female makes `r percent(singlefemale)` (`percent(singlefemale)`) less than the reference group.
3. Married females make `r percent(abs(marriedfemale) - abs(married))` (`percent(abs(marriedfemale) - abs(married))`) less than single females.
4. There seems to be a marriage premium[^1] for men but for women the marriage premium is negative.

[^1]: There is clearly a correlation between men having children and men getting higher salaries, and the reverse for women. However, this may reflect the fact that women are more likely to withdraw from work to take care of children (regardless of whether they'd prefer to), and men may double down on work.



## Question: 

Test for heteroscedasticity test in the estimated regression of the wage1 dataset. Do we reject homoscedasticity for all reasonable significance levels? Adjust for heteroscedasticity by using refined White heteroscedasticity-robust SE. 

**A**: 


```{r selfeval_1_solutions-26}
bptest(lm2_wage1)
```

We do not reject the null hypothesis at conventional significance levels. 




## Question

Create a regression table showing the results from equation \ref{eq:2.1} and \ref{eq:2.2}. Show a specification where the SE have not been adjusted for heteroscedasticity and another specification where the SE have been adjusted for heteroscedasticity.

**A**: 


```{r}
modelsummary(list(lm3_wage1, coeftest(lm3_wage1, vcovHC), lm2_wage1, coeftest(lm2_wage1, vcovHC)), stars = TRUE, notes = "Model 2 and Mode 4 have been estimated with robust standard errors.")
```



# Collinearity exercises

This exercise focuses on the **collineartiy** problem. 
  

## Question: 


Run the following commands in R: 

```{r}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100)/10
y <- 2 +2*x1 + 0.3 *x2 +rnorm(100)
```

The last line corresponds to creating a linear model in which $y$ is a function of $x_1$ and $x_2$. Write out the form of the linear model. What are the regression coefficients?

**A**: 

$y = 2 +2x_1 + 0.3x_2 + \epsilon$

$\beta_0 = 2$, $\beta_1 = 2$, $\beta_3 = 0.3$



## Question: 

What is the correlation between $x_1$ and $x_2$? Create a scatterplot displaying the relationship between the variables. 

**A**:



```{r selfeval_1_solutions-30}
cor(x1, x2)
```


```{r selfeval_1_solutions-32}
d <-  data.frame(x1,x2)
ggplot(d, aes(x1, x2)) +
  geom_point(shape = 16, size = 3, show.legend = FALSE) +
  theme_pubr()
```


## Question: 

Using this data, fit a least squares regression to predict $y$ using $x_1$ and $x_2$. Describe the results obtained. What are $\hat{\beta_0}$, $\hat{\beta_1}$ and $\hat{\beta_2}$? How do these relate to the true $\beta_0$, $\beta_1$ and $\beta_2$? Can you reject the null hypothesis $H_0: \beta_1 =0$? How about the null hypothesis $H_0: \beta_2 =0$? 

**A**: 



```{r selfeval_1_solutions-33}
lm.fit = lm(y~x1+x2)
summary(lm.fit)
```

The regression coefficients are close to the true coefficients, although with high standard error. We can reject the null hypothesis for $\beta_1$ because its p-value is below 5%. We cannot reject the null hypothesis for $\beta_2$ because its p-value is much above the 5% typical cutoff, over 60%.


## Question: 

Now fit least squares regression to predict $y$ using only $x_1$. Comment on your results. Can you reject the null hypothesis $H_0: \beta_1=0$? 

**A**: 


```{r selfeval_1_solutions-34}
lm.fit = lm(y~x1)
summary(lm.fit)
```

Yes, we can reject the null hypothesis for the regression coefficient given the p-value for its t-statistic is near zero.



## Question: 


Now fit least squares regression to predict $y$ using only $x_2$. Comment on your results. Can you reject the null hypothesis $H_0: \beta_2=0$? 

**A**: 

```{r selfeval_1_solutions-35}
lm.fit = lm(y~x2)
summary(lm.fit)
```

Yes, we can reject the null hypothesis for the regression coefficient given the p-value for its t-statistic is near zero.


## Question: 

Do the results from the previous questions contradict each other? Explain your answer. 


**A**:



No, because $x_1$ and $x_2$ have collinearity, it is hard to distinguish their effects when regressed upon together. When they are regressed upon separately, the linear relationship between y and each predictor is indicated more clearly.

# Simulation exercises

## Question:

The probability that a baby is girl or boy is approximately 48.8% or 51.2%, respectively, and these do not much very much across the world. Suppose that 400 babies are born in a hospital in a given year. How many will be girls? 

Set a seed (eg. `set.seed(123)`) to make the result reproducible! 

**A**:

```{r}
set.seed(123)
n_girls <- rbinom(1, 400, 0.488)
n_girls
```



## Question: 

Simulate the process 1000 times and plot the distribution. Indicate the mean in the distribution plot. 


**A**:


```{r}
n_sims <- 1000
n_girls <- rbinom(n_sims, 400, 0.488)
```
```{r}
n_girls <- as.data.frame(n_girls)
ggplot(n_girls, aes(n_girls)) +
  geom_histogram(show.legend = FALSE) +
  scale_x_continuous(breaks = seq(min(n_girls), 300, 5), lim = c(min(n_girls), max(n_girls))) + 
  geom_vline(aes(xintercept = mean(n_girls)),col='red',size=2)+
  labs(title="Histogramm of 1000 simulated values for the number of girls born in a hospital from 400 births.",
        x ="Number of girls")+
  theme_pubr()

```


## Question: 


In the previous exercise we simulated a discrete probability model. Now, we will simulate a mixed discrete/ continuous model. 

In the United States 52% of the adults are women and 48% are men. The heights of the men are approximately normally distributed wit mean 69.1 inches and standard deviation 2.9. Women have a mean height of 63.7 inches and a standard deviation of 2.7. 

Generate the height of one randomly chosen adult (random adult means that this can either be a man or a women). Don't forget to set a seed. How tall is that person? What gender does that random person probably have? 

**A**:


```{r}
set.seed(123)
N <- 10
male <- rbinom(1,1,0.48)
height <- ifelse(male==1, rnorm(N, 69.1, 2.9), rnorm(1, 63.7, 2.7))
avg_height <- mean(height)
```





## Question: 

Now, simulate the distribution of the average height by generating 1000 draws. Plot the distribution of the average height of those 10 adults. 

**A**:


```{r}
n_sims < 1000
avg_height <- rep(NA, n_sims)
for (s in 1:n_sims) {
  N <- 10
male <- rbinom(1,1,0.48)
height <- ifelse(male==1, rnorm(N, 69.1, 2.9), rnorm(1, 63.7, 2.7))
avg_height[s] <- mean(height)
}
```



```{r}
avg_height <- as.data.frame(avg_height)
ggplot(avg_height, aes(avg_height)) +
  geom_histogram(show.legend = FALSE) +
  labs(title="Histogramm of the distribution of average height of 10 adults in the United States",
        x ="Average height")+
  theme_pubr()
```

## Question: 

Finally, instead of estimating the average height of 10 people, simulate the same model and extract the maximum height of 10 people. Plot the distribution. 

**A**:

```{r}
max_height <- rep(NA, n_sims)
for (s in 1:n_sims) {
  N <- 10
male <- rbinom(1,1,0.48)
height <- ifelse(male==1, rnorm(N, 69.1, 2.9), rnorm(1, 63.7, 2.7))
max_height[s] <- max(height)
avg_height[s] <- mean(height)
}
```

```{r}
max_height <- as.data.frame(max_height)
ggplot(max_height, aes(max_height)) +
  geom_histogram(show.legend = FALSE) +
  labs(title="Histogramm of the distribution of maximum height of 10 adults in the United States",
        x ="Maximum height")+
  theme_pubr()
```

