---
output:
  pdf_document: default
  html_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, autodep = TRUE)
# Set plotting to bw plot default, but with transparent background elements.  
# Note transparency requires the panel.background, plot.background, and device background all be set!
library(ggplot2) 
theme_set(theme_bw(base_size=12))
theme_update(panel.background = element_rect(fill = "transparent", colour = NA),
             plot.background = element_rect(fill = "transparent", colour = NA))
knitr::opts_chunk$set(dev.args=list(bg="transparent"))
```


# Linear Regression {#linearregression}


To load the dataset and necessary functions: 


```{r, echo=TRUE, message=FALSE, results='hide'}
# This function 1. checks if the packages are installed. 2. It installs the packages if they were not in the list of installed packages. 3. It loads the packages into the workspace
# devtools::install_github("ccolonescu/PoEdata")
PACKAGES<-c("PoEdata", # R data sets for "Principles of Econometrics" by Hill, Griffiths, an d Lim, 4e, Wiley. https://github.com/ccolonescu/PoEdata
            "wooldridge",  # Wooldrige Datasets
            "tidyverse",  # for data manipulation and ggplots
            "broom",  # Tidy regression output
            "ggpubr",  # Multiple ggplots on a page. Note that, the installation of ggpubr will automatically install the gridExtra and the cowplot package; so you don’t need to re-install them. 
            "ggfortify", # Simple ggplot recipe for lm objects) 
            "plot3D",  #  3D graphs
            "car", # Companion to applied regression
            "knitr", # knit functions
            # "kableExtra", # extended knit functions for objects exported from other packages
            "huxtable", #  Regression tables, broom compatible
            "mice",  # multiple imputation
            "VIM", # visualizing missing data
            "stargazer", # Regression tables
            "AER", #  Functions, data sets, examples, demos, and vignettes for the book Christian Kleiber and Achim Zeileis (2008)
            "MASS",  #  Functions and datasets to support Venables and Ripley,  "Modern Applied Statistics with S"
            "mvtnorm", # Multivariate Normal and t Distributions 
            "summarytools", # Report regression summary tables
            "scales", # scale helper functions such as percent 
            "OutliersO3", # Outlier comparison method
            "robustbase", # Basic robust statistics
            "quantreg", # Quantile regression
            "modelr", # model simulation/ boostraping the modern way
            "magrittr") #  pipes
inst<-match(PACKAGES, .packages(all=TRUE))
need<-which(is.na(inst))
if (length(need)>0) install.packages(PACKAGES[need])
lapply(PACKAGES, require, character.only=T)
```


## Simple Linear Regression

We start off with a simple OLS Regression. We will work with multiple data sources: 

- Data from @wooldridge2015introductory : Introductory Econometrics: A Modern Approach. 
- R data sets for "Principles of Econometrics" by @hill2008principles
- Build in examples such as the `airquality` dataset

Classic examples of quantities modeled with simple linear regression:


* College GPA ∼ SAT scores $\beta > 0$
* Change in GDP ∼ change in unemployment $\beta < 0$
* House price ∼ number of bedrooms $\beta > 0$
* Species heart weight ∼ species body weight  $\beta > 0$
* Fatalities per year ∼ speed limit  $\beta < 0$


Notice that these simple linear regressions are simplifications of
more complex relationships between the variables in question.

In this exercise we use the dataset *ceosal1*. Let us analyse the dataset first

```{r}
data("ceosal1")
help("ceosal1")
?ceosal1
```

As we see from the R documentation the *ceosal1* dataset contain of a random sample of data reported in the May 6, 1991 issue of Businessweek. 


To get a first look at the data you can use the `View()` function inside R Studio. 

```{r, eval=FALSE}
View(ceosal1) # For the compilation we omit the full View()
```

```{r, echo=FALSE}
# We will learn in later chapters how to print nice tables
knitr::kable(
  head(ceosal1[,1:8], 10), booktabs = TRUE, 
  caption = "A table of the first eight columns and ten rows of the ceosal1 data."
)
```


We could also take a look at the variable names, the dimension of the data frame, and some sample observations with `str()`.

```{r}
str(ceosal1)
```


As we have seen before in the general R tutorial, there are a number of additional functions to access some of this information directly.


```{r}
dim(ceosal1)
```


```{r}
nrow(ceosal1)
```


```{r}
ncol(ceosal1)
```


```{r}
summary(ceosal1)
```


The interesting task here is to determine how far a high the CEO salary is, for a given return on equity. 


---

**Your turn**


What sign would be expect of $\beta$ (the slope)?


A: Without seeing the data **my** prior is that $\beta > 0$. 

---

**Note**

A simple linear model as assumes that the mean of each $y_{i}$ conditioned on $x_{i}$
is a linear function of $x_{i}$. But notice that simple linear regressions are simplifications of more complex relationships between the variables in question @dalpiaz2016.

--- 


```{r ceosal1, fig.cap="Relationship between ROE and Salary", }
# Use ggplot style
ggplot(ceosal1, aes(x = roe, y = salary)) +
  geom_point()
```


Consider a simple regression model

$salary = \beta_0 + \beta_1roe + u$

In the general form the linear regression model can be written as: 

\begin{equation}
y = \beta_{0} + \beta_{1}x + u
(\#eq:simplelinearregressionmodel)
\end{equation}



We are concerned with the population parameter $\beta_{0}$ and $\beta_{1}$. The ordinary least squares (OLS) estimators are: 

\begin{equation}
\hat{\beta}_{0} = \bar{y} - \hat{\beta}_{1}\bar{x}
(\#eq:populationparameterBeta0)
\end{equation}


The ordinary least squares (OLS) estimators are

\begin{equation}
\hat{\beta}_{1} = \frac{Cov(x,y)}{Var(x)}
(\#eq:populationparameterBeta1)
\end{equation}

Ingredients for the OLS formulas


```{r}
attach(ceosal1)
cov(roe, salary)
```

```{r}
var(roe)
```


```{r}
mean(salary)
```

Manual calculation of the OLS coefficients

```{r}
b1hat <- cov(roe,salary)/var(roe)
```

```{r}
b0hat <- mean(salary) - b1hat * mean(roe)
```

Or use the `lm()` function

```{r}
lm(salary ~ roe, data=ceosal1)
lm1_ceosal1 <- lm(salary ~ roe, data=ceosal1) 
summary(lm1_ceosal1)
```


Plot the linear regression fit the *base* r way. 

```{r fig1, fig.cap='OLS regression base Rstyle', out.width='80%', fig.asp=.75, fig.align='center'}
plot(salary~ roe, data = ceosal1,
     xlab = "Return on equity",
     ylab = "Salary",
     main = "Salary vs return on equity",
     pch  = 20,
     cex  = 2,
     col  = "grey")
abline(lm1_ceosal1, lwd = 3, col = "darkorange")
```


Or use ggplot 


```{r fig2, fig.cap='OLS regression ggplot2 style', out.width='80%', fig.asp=.75, fig.align='center'}
ggplot(ceosal1, aes(x = roe, y = salary)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")
```

Determine the names of the elements of the list using the `names()` command.

```{r}
names(lm1_ceosal1)
```

Extract one element, for example the residuals from the list object

```{r}
head(lm1_ceosal1$residuals) # head() just prints out the first 6 residual values
```


Another way to access stored information in *lm1_ceosal1* are the `coef()`, `resid()`, and `fitted()` functions. These return the coefficients, residuals, and fitted values, respectively.


```{r}
coef(lm1_ceosal1)
```

The function `summary()` is useful in many situations. We see that when it is called on our model, it returns a good deal of information.

```{r}
summary(lm1_ceosal1)
```


The `summary()` command also returns a list, and we can again use `names()` to learn what about the elements of this list.


```{r}
names(summary(lm1_ceosal1))
```

So, for example, if we wanted to directly access the value of $R^2$, instead of copy and pasting it out of the printed statement from `summary()`, we could do so.

```{r}
summary(lm1_ceosal1)$r.squared
```

---

**Your turn**


Recall that the explained sum of squares (SSE) is 

\begin{equation}
SSE = \sum_{i=1}^{n}(\hat{y}_{i} - \bar{y})^2 = (n-1) \times Var(\hat{y})
(\#eq:SSE)
\end{equation}


and  the residual sum of squares (SSR) is 

\begin{equation}
R^2 = \frac{Var(\hat{y})}{Var(y)} = 1 - \frac{Var(\hat{u})}{Var(y)} 
(\#eq:SSR)
\end{equation}

One can see that the correlation between observed and fitted values is a square root of $R^2$. 

Calculate $R^2$ manually: 

```{r}
var(fitted(lm1_ceosal1))/var(ceosal1$salary)
```


```{r}
1 - var(residuals(lm1_ceosal1))/var(ceosal1$salary)
```

----

Another useful function is the `predict()` function.


```{r}
set.seed(123)
# unique(ceosal1$roe)
roe_sample <-sample(ceosal1$roe, 1)
roe_sample
```

Let’s make a prediction for salary when the return on equity is `r roe_sample`. 

```{r}
b0hat_sample <- mean(salary) - b1hat * roe_sample 
```

We are not restricted to observed values of the explanatory variable. Instead we can supply also our own predictor values

```{r}
predict(lm1_ceosal1, newdata = data.frame(roe = 30))
```

The above code reads “predict the salary when the return on equity is 30 using the  *lm1_ceosal1* model.”

---

<details><summary>**Overthinking**</summary>
<p>


### Regression through the Origin and Regression on a Constant

Regression without intercept (through origin)

```{r}
lm2 <- lm(salary ~  0 + roe, data = ceosal1)
```


Regression without slope

```{r}
lm3 <- lm(salary ~ 1, data = ceosal1)
```


```{r fig3, fig.cap='Regression through the Origin and on a Constant', out.width='80%', fig.asp=.75, fig.align='center'}
plot(salary~ roe, data = ceosal1,
     xlab = "Return on equity",
     ylab = "Salary",
     main = "Salary vs return on equity",
     pch  = 20,
     cex  = 2,
     col  = "grey")
abline(lm1_ceosal1, lwd = 3, lty = 1, col = "darkorange")
abline(lm2,lwd = 3,  lty = 2,   col = "darkblue")
abline(lm3, lwd = 3,  lty = 3,   col = "black")
legend("topleft", 
       c("full", 
         "through origin", 
         "constant only"), 
       lwd =2, 
       lty = 1:3)
```


In models without the intercept the $R^2$ loses its interpretatation.  The reason is that the $R^2$ is the ratio of explained variance to total variance **only** if the intercept is included. 


---

</p>
</details>



**Overthinking**

### Simulating SLR



##### Expected Values, Variance, and Standard Errors

The **Gauss–Markov theorem** tells us that when estimating the parameters of the simple linear regression model $\beta_{0}$ and $\beta_{1}$, the $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ which we derived are the best linear unbiased estimates, or BLUE for short. (The actual conditions for the Gauss–Markov theorem are more relaxed than the SLR model.)

In short those assumptions are: 

- SLR.1 Linear population regression function   $y = \beta_0  + \beta_{1} \times x + u$
- SLR.2 Random sampling of x and y from the population   
- SLR.3 Variation in the sample values:   $x_{1}, \dots , x_{n}$ 
- SLR.4 Zero conditional mean: $\mathbf{E}(u|x) = 0$
- SLR.5 Homeskedasticity:   $Var(u|x) = \sigma^2$


Recall that under **SLR.1 - SLR.4** the OLS parameter estimators are unbiased. Under **SLR.1 - SLR.4** the OLS parameter estimators have a specific sampling variance. 


Simulating a model is an important concept. In practice you will almost never have a true model, and you will use data to attempt to recover information about the unknown true model. With simulation, we decide the true model and simulate data from it. Then, we apply a method to the data, in this case least squares. Now, since we know the true model, we can assess how well it did.

Simulation also helps to grasp the concepts of estimators, estimates, unbiasedness, the sampling variance of the estimators, and the consequences of violated assumptions. 

Sample size

```{r}
n <- 200
```

True parameters

```{r}
b0<- 1
b1 <- 0.5
sigma <- 2 # standard deviation of the error term u 
x1 <- 5
```

Determine the distribution of the independent variable

```{r}
yhat1 <- b0 + b1 * x1 #  Note that we do not include the error term 
```

Plot a Gaussian distribution of the dependent variable based on the parameters


```{r}
curve(dnorm(x, mean = yhat1, sd = sigma), -5, 15, col = "blue")
abline(v = yhat1, col = "blue", lty = 2)
legend("topright", legend = c("f(y|x = 5)"), lty = 1, col = c("blue"))
```

This represent the theoretical (true) probability distribution of $y$, given $x$

We can calculate the variance of $b_{1}$ and plot the corresponding density function. 

\begin{equation}
var(b_2) = \frac{\sigma^2}{\sum{}{}(x_1 - \bar{x})^2}
(\#eq:variancebeta)
\end{equation}

Assume that $x_{2}$ represents a second possible predictor of $y$


```{r}
x2 <- 18

x <- c(rep(x1, n/2), rep(x2, n/2))
xbar <- mean(x)

sumxbar <- sum((x-xbar)^2)
varb <- (sigma^2)/sumxbar
sdb <-sqrt(varb)
leftlim <- b1-3*sdb
rightlim <- b1+3*sdb
```



```{r fig4, fig.cap='The theoretical (true) probability density function of b1', out.width='80%', fig.asp=.75, fig.align='center'}
curve(dnorm(x, mean = b1, sd = sdb), leftlim, rightlim)
abline(v = b1, col = "blue", lty = 2)
```

Draw sample of size $n$

```{r}
x <- rnorm(n, 4, sigma)
# Another way is to assume that the values for x are fixed and know
# x= seq(from = 0, to = 10, length.out = n)
```


```{r}
u <- rnorm(n, 0, sigma)
```


```{r}
y <- b0 + b1 * x + u
```

Estimate parameter by OLS

```{r}
olsreg <- lm(y ~x )
```


```{r}
simulation.df <- data.frame(x,y)
population.df <- data.frame(b0, b1)
```


```{r fig5, fig.cap='Simulated Sample and OLS Regression Line', out.width='80%', fig.asp=.75, fig.align='center'}
plot(simulation.df, 
     xlab = "x",
     ylab = "y",
     # main = "Simulate least squares regression",
     pch  = 20,
     cex  = 2,
     col  = "grey")
abline(olsreg, lwd = 3, lty = 1, col = "darkorange")
abline(b0, b1,  lwd = 3,  lty = 2,   col = "darkblue")
legend("topleft", 
       c("OLS regression function", 
         "Population regression function"), 
       lwd =2, 
       lty = 1:2)
```


```{r fig6, fig.cap='Simulated Sample and OLS Regression Line (gpplot Style)', out.width='80%', fig.asp=.75, fig.align='center'}
lable1 <- "OLS regression function"
ggplot(simulation.df, aes(x = x,  y = y)) +
  geom_point() +
  geom_abline(aes(intercept=b0,slope=b1,colour="Population regression function"), linetype ="dashed", show.legend  = TRUE)+
  stat_smooth(aes(colour ="OLS regression function"), method = "lm",se=FALSE, show.legend =TRUE)+
  labs(colour = "Regression functions" 
       # , title = "Simulate least squares regression"
  )
```


Since the expected values and variances of our estimators are defined over separate random samples from the same population, it makes sense to  repeat our simulation exercise over many simulated samples. 

```{r, cache=TRUE}
# Set the random seed
set.seed(1234567)

# set sample size and number of simulations
n<-1000; r<-10000

# set true parameters: betas and sd of u
b0<-1.0; b1<-0.5; sigma<-2

# initialize b0hat and b1hat to store results later:
b0hat <- numeric(r)
b1hat <- numeric(r)

# Draw a sample of x, fixed over replications:
x <- rnorm(n,4,1)

# repeat r times:
for(j in 1:r) {
  # Draw a sample of y:
  u <- rnorm(n,0,sigma)
  y <- b0 + b1*x + u
  
  # estimate parameters by OLS and store them in the vectors
  bhat <- coefficients( lm(y~x) )
  b0hat[j] <- bhat["(Intercept)"]
  b1hat[j] <- bhat["x"]
}

```

```{r}
# MC estimate of the expected values:
mean(b0hat)
mean(b1hat)

# MC estimate of the variances:
var(b0hat)
var(b1hat)

```

```{r fig7, fig.cap='Population and Simulated OLS Regression Lines', out.width='80%', fig.asp=.75, fig.align='center'}
# Initialize empty plot
plot( NULL, xlim=c(0,8), ylim=c(0,6), xlab="x", ylab="y")
# add OLS regression lines
for (j in 1:10) abline(b0hat[j],b1hat[j],col="gray")
# add population regression line
abline(b0,b1,lwd=2)
# add legend
legend("topleft",c("Population","OLS regressions"),
       lwd=c(2,1),col=c("black","gray"))

```


Even though the loop solution is transparent, let us take a look at a different, more *modern* approach.  

```{r, cache=TRUE}
# define a function the returns the alpha -- its point estimate, standard error, etc. -- from the OLS
x <- rnorm(n,4,1) # NOTE 1: Although a normal distribution is usually defined by its mean and variance, 'rnorm()' requires the standard deviation as input for the second moment.
# NOTE 2: We use the same values for x in all samples since we draw them outside of the loop. 

iteration <- function() {
  u <- rnorm(n,0,sigma)
  y <- b0 + b1*x + u
  
  lm(y~x) %>% 
    broom::tidy() # %>% 
  # filter(term == 'x') # One could only extract the slope
}

# 1000 iterations of the above simulation
MC_coef<- map_df(1:1000, ~iteration()) 
str(MC_coef)
```

Instead of plotting simulated and true parameter regression lines we can take a look at the kernel density of the simulated parameter estimates

Figure \@ref(fig:fig8) shows the simulated distribution of $\beta_{0}$ and $\beta_{1}$ the theoretical one. 

```{r}
# plot the results
str(MC_coef)
MC_coef<- MC_coef %>%
  mutate(OLScoeff =  ifelse(term == "x", "b1hat", "b0hat")) %>%  # rename the x to b1hat and (Intercept) to b0hat and create a new column 
  mutate(Simulated = ifelse(term == "x", "b1", "b0")) #  %>% 
```

```{r fig8, fig.cap='Histogram b0 and b1 and true parameter', out.width='80%', fig.asp=.75, fig.align='center'}
ggplot(data= MC_coef, aes(estimate)) + 
  geom_histogram() + 
  geom_vline(data = filter(MC_coef, OLScoeff == "b0hat"), aes(xintercept=b0), colour="pink") +
  geom_vline(data = filter(MC_coef, OLScoeff == "b1hat"), aes(xintercept=b1), colour="darkgreen") + 
  geom_text(data=MC_coef[3,], mapping=aes(x=estimate, y=8, label=paste("True parameter: ", MC_coef[3,7])), colour = "pink") +
  geom_text(data=MC_coef[4,], mapping=aes(x=estimate, y=8, label=paste("True parameter: ", MC_coef[4,7])), colour = "darkgreen") +
  facet_wrap( ~ OLScoeff, scales = "free")   +
  labs(
    title = "Histogram Monte Carlo Simulations and True population parameters") +
  theme_bw()

```


```{r}
b1_sim <- MC_coef %>% 
  filter(Simulated == "b1")

mean(b1_sim$estimate)
var(b1_sim$estimate) == (sd(b1_sim$estimate))^2
all.equal(var(b1_sim$estimate) , (sd(b1_sim$estimate))^2) # Floating point arithmetic!
```

```{r fig9, fig.cap='Kernel Density Monte Carlo Simulations vs. True population parameters of b1', out.width='80%', fig.asp=.75, fig.align='center'}
ggplot(data= b1_sim, aes(estimate)) +  
  geom_density(aes(fill = Simulated), alpha = 0.2) + # computes and draws the kernel density, which is the smoothed version of the histogram
  # stat_function(fun = dnorm, args = list(mean = mean(b1_sim$estimate), sd = sd(b1_sim$estimate)), aes(colour = "true")) +
  stat_function(fun = dnorm, args = list(mean = 0.5, sd = sd(b1_sim$estimate)), aes(colour = "Population")) +
  # labs(
  #  title = "Kernel Density Monte Carlo Simulations vs. True population parameters"
  # ) +
  scale_color_discrete(name="")
```



##### Violation of SLR.4

To implement a violation of **SLR.4** (zero conditional mean) consider a case where in the population $u$ is not mean independent of $x$, for example 

$$
\mathbf{E}(u|x) = \frac{x-4}{5}
$$


```{r, cache=TRUE}
# Set the random seed
set.seed(1234567)

# set sample size and number of simulations
n<-1000; r<-10000

# set true parameters: betas and sd of u
b0<-1; b1<-0.5; su<-2

# initialize b0hat and b1hat to store results later:
b0hat <- numeric(r)
b1hat <- numeric(r)

# Draw a sample of x, fixed over replications:
x <- rnorm(n,4,1)

# repeat r times:
for(j in 1:r) {
  # Draw a sample of y:
  u <- rnorm(n, (x-4)/5, su) # this is where manipulate the assumption of zero conditional mean
  y <- b0 + b1*x + u
  
  # estimate parameters by OLS and store them in the vectors
  bhat <- coefficients( lm(y~x) )
  b0hat[j] <- bhat["(Intercept)"]
  b1hat[j] <- bhat["x"]
}
```

OLS coefficients

```{r}
# MC estimate of the expected values:
mean(b0hat)
mean(b1hat)

# MC estimate of the variances:
var(b0hat)
var(b1hat)
```


The average estimates are far from the population parameters $\beta_0=1$ and $\beta_1 = 0.5$!


##### Violation of SLR.5

Homoskedasticity is not required for unbiasedness but for it is a requirement for the theorem of sampling variance. Consider the following heteroskedastic behavior of $u$ given $x$. 

```{r, cache=TRUE}
# Set the random seed
set.seed(1234567)

# set sample size and number of simulations
n<-1000; r<-10000

# set true parameters: betas and sd of u
b0<-1; b1<-0.5; su<-2

# initialize b0hat and b1hat to store results later:
b0hat <- numeric(r)
b1hat <- numeric(r)

# Draw a sample of x, fixed over replications:
x <- rnorm(n,4,1)

# repeat r times:
for(j in 1:r) {
  # Draw a sample of y:
  varu <- 4/exp(4.5) * exp(x)
  u <- rnorm(n, 0, sqrt(varu) )
  y <- b0 + b1*x + u
  
  # estimate parameters by OLS and store them in the vectors
  lm_heterosced <- lm(y~x)
  
  bhat <- coefficients( lm(y~x) )
  b0hat[j] <- bhat["(Intercept)"]
  b1hat[j] <- bhat["x"]
}
```


```{r}
summary(lm_heterosced) # just the last sample of the MC-simulation
```

Plot the residual against the regressor suspected of creating heteroskedasticity, or more generally, the fitted values of the regression.

```{r}
res <- residuals(lm_heterosced)
yhat <- fitted(lm_heterosced)
```

```{r fig10, fig.cap='Heteroskedasticity in the simulated data', out.width='80%', fig.asp=.75, fig.align='center'}
par(mfrow = c(1,2))
plot(x, res, ylab = "residuals")
plot(yhat, res, xlab = "fitted values", ylab = "residuals")
```


```{r}
# MC estimate of the expected values:
mean(b0hat)
mean(b1hat)

# MC estimate of the variances:
var(b0hat)
var(b1hat)
```

Unbiasedness is provided but sampling variance is incorrect (compared to the results provided above). 

---


### Nonlinearities

Sometimes the scatter plot diagram or some theoretical considerations suggest a non-linear relationship. The most popular non-linear transformation involve logarithms of the dependent or independent variables and polynomial functions.

We will use a new dataset,  *wage1*, for this section. A detailed exploratory analysis of the dataset is left to the reader. 

```{r}
data("wage1")
```

#### Predictor variable transformation

A common variance stabilizing transformation (VST) is necessary when we see increasing variance in a fitted versus residuals plot.  

To use the *log* of an independent variable is to make its distribution closer to the normal distribution. 

```{r}
# wage1$logwage <- log(wage1$wage) # one could also create a new variable 

p1_wagehisto <- ggplot(wage1)  +
  geom_histogram(aes(x = wage), fill = "red", alpha = 0.6)

p2_wagehisto <- ggplot(wage1)  +
  geom_histogram(aes(x = wage),  fill = "blue", alpha = 0.6) +
  scale_x_continuous(trans='log2', "Log Wage")  # instead of creating a new variable with simply define that the x-scale undergoes a logarithmic transformation

```


```{r fig11, fig.cap='Histogram of wage and log(wage)', out.width='80%', fig.asp=.75, fig.align='center'}
ggarrange(p1_wagehisto, p2_wagehisto,  
          labels = c("A", "B"),
          ncol = 2, nrow = 1)
```



A model with a log transformed response: 


\begin{equation}
log(Y_{i}) = \beta_{0} + \beta_{1} \times x_{i} + \epsilon_{i}
\end{equation}


```{r}
lm_wage <- lm(wage ~ educ, data = wage1)
lm_wage1 <- lm(log(wage) ~ educ, data =  wage1)
summary(lm_wage)
summary(lm_wage1)
```


Plotting Diagnostics for Linear Models

```{r fig12, fig.cap='Regression diagnostics plot base R - Linear Relationship', out.width='80%', fig.asp=.75, fig.align='center'}
plot(lm_wage)
```



```{r fig13, fig.cap='Regression diagnostics autoplot(ggplot) - Linear Relationship', out.width='80%', fig.asp=.75, fig.align='center'}
autoplot(lm_wage, which = 1:6, colour = 'dodgerblue3',
         smooth.colour = 'red', smooth.linetype = 'dashed',
         ad.colour = 'blue',
         label = FALSE,
         label.size = 3, label.n = 5, label.colour = 'blue',
         ncol = 3) +
  theme_bw()
```


```{r fig14, fig.cap='Regression diagnostics - Non-Linear Relationship', out.width='80%', fig.asp=.75, fig.align='center'}
autoplot(lm_wage1, which = 1:6, colour = 'dodgerblue3',
         smooth.colour = 'red', smooth.linetype = 'dashed',
         ad.colour = 'blue',
         label = FALSE,
         label.size = 3, label.n = 5, label.colour = 'blue',
         ncol = 3) +
  theme_bw()
```



```{r}
p1_nonlinearities <- ggplot(wage1, aes(x = educ, y = wage )) +
  geom_point()   + 
  scale_y_continuous(trans='log2', "Log Wage") + 
  stat_smooth(aes(fill="Linear Model"),size=1,method = "lm" ,span =0.3, se=F) + 
  guides(fill = guide_legend("Model Type")) + 
  theme_bw()
p1_nonlinearities
```

Note that if we re-scale the model from a log scale back to the original scale of the data, we now have 

\begin{equation}
Y_{i} = exp(\beta_{0} + \beta_{1} \times x_{i})  \times exp(\epsilon_{i})
\end{equation}

which has errors entering in a multiplicative fashion.  

```{r}
log.model.df <- data.frame(x = wage1$educ,
                           y = exp(fitted(lm_wage1))) # This is essentially exp(b0_wage1 + b1_wage1 * wage1$educ) 
```

```{r}
p2_nonlinearities <- ggplot(wage1, aes(x = educ, y = wage))  +
  geom_point()   +
  geom_line(data = log.model.df, aes(x, y, color = "Log Model"), size = 1, linetype = 2)  +
  guides(color = guide_legend("Model Type")) + 
  theme_bw()
```



```{r fig15, fig.cap='Wages by Education - Different transformations', out.width='80%', fig.asp=.75, fig.align='center'}
ggarrange(p1_nonlinearities, p2_nonlinearities,  
          labels = c("A", "B"),
          ncol = 2, nrow = 1)
```

A: Plotting the data on the transformed log scale and adding the fitted line, the relationship again appears linear, and the variation about the fitted line looks more constant. 

B: By plotting the data on the original scale, and adding the fitted regression, we see an exponential relationship. However, this is still a *linear* model, since the new transformed response, $log(Y_{i}$, is still a *linear* combination of the predictors. In other words, only $\beta$ needs to be linear, not the $x$ values. 


**Quadratic Model**


\begin{equation}
Y_{i} = \beta_{0} + \beta_{1} \times x_{i}  \beta_{2} \times x^2_{i} + \epsilon_{i}
\end{equation}


New dataset from Wooldrige: Collected from the real estate pages of the Boston Globe during 1990. These are homes that sold in the Boston, MA area.

```{r}
data("hprice1", package = "wooldridge")
```

In R, independent variables involving mathematical operators can be included in regression equation with the function `I()`

We estimate the following regression: 

\begin{equation}
hprice ~ \beta_{0} +   \beta_{1} sqrft + u
\end{equation}

```{r}
lm_hprice <- lm(price ~ sqrft, data  = hprice1)
```

and a regression model that includes a squared term 

\begin{equation}
  hprice ~ \beta_{0} +   \beta_{1} sqrft + \beta_{2} sqrft^2 + u  
\end{equation}


```{r}
lm_hprice1 <- lm(price ~ sqrft + I(sqrft^2), data  = hprice1)
```


Alternatively use the `poly()` function. Be careful of the additional argument `raw`. 

```{r}
lm_hprice2 <- lm(price ~ poly(sqrft, degree = 2),  data  = hprice1) 
lm_hprice3 <- lm(price ~ poly(sqrft, degree = 2, raw = TRUE),  data  = hprice1) # if true, use raw and not orthogonal polynomials.
```

```{r}
unname(coef(lm_hprice1))
unname(coef(lm_hprice2))
unname(coef(lm_hprice3))

all.equal(unname(coef(lm_hprice1)), unname(coef(lm_hprice2)))
all.equal(unname(coef(lm_hprice1)), unname(coef(lm_hprice3)))

all.equal(fitted(lm_hprice1), fitted(lm_hprice2))
all.equal(fitted(lm_hprice1), fitted(lm_hprice3))
```

With the function `all.equal()` we can test if all elements in two vectors are the "nearly" the same. "Nearly" refers to the case when *tolerance* values are exceeded. 

Why are those values not the same depending on the `poly()` function argument `raw = True`? In the case of  `raw = True` R  uses raw and not orthogonal polynomials.

This can have importan implications in the case when multiple polynimoals are used in the regression. The linear regressar $sqrft$ is uncorrelated linearly with the squared explanatory variable $sqrft^2$. However, if we add a cubic term $sqrft^3$, multicollinearty between $sqrft^2$ and $sqrft^3$ might become and issues of the polynomials are *NOT* orthogonalized.

We can also extract the standard error of our estimated coefficents manually. 

Starting with the variance-covariance matrix of the regression model

```{r}
vcov_lm_hprice2 <- vcov(lm_hprice2)
```

Extracting only the diagonal element of the matrix and taking the square root we obtain the standard errors as reported in `summary()` function call. 

```{r}
sqrt(diag(vcov_lm_hprice2))
```


```{r}
summary(lm_hprice2)
```


## Multiple Linear Regression

---

**Note**


A **(general) linear model** is similar to the simple variant, but with a multivariate $x \epsilon \!R^{\rho}$ and a mean given by a hyperplane in place of a single line.

* General principles are the same as the simple case
* Math is more difficult because we need to use matrices
* Interpretation is more difficult because the $\beta_{j}$ are effects conditional on the other variables

Many would retain the same signs as the simple linear regression, but the magnitudes would be smaller. In some cases, it is possible for
the relationship to flip directions when a second (highly correlated) variable is added @dalpiaz2016.   

--- 

\begin{equation}
y = \beta_{0} + \beta_{1}x_{1} +  \beta_{2}x_{2} + \dots + \beta_{k}x_{k} + u   
(\#eq:multipleregression)
\end{equation}

The next example from Wooldrige relates the college GPA ("cloGPA") to the high school GPA ("hsGPA") and achievement test score ("ACT") for a sample of 141 students.

```{r}
data("gpa1", package = "wooldridge")
attach(gpa1)
?gpa1
```

Obtain parameter estimates
```{r}
GPAres <- lm(colGPA ~ hsGPA + ACT, data = gpa1)
summary(GPAres)
coef(GPAres)[[1]]

```

In the multiple linear regression setting, some of the interpretations of the coefficients change slightly. Here, $\hat\beta_{0} =$ `r coef(GPAres)[[1]]` is our estimate for $\beta_{0}$ when all of the predictors are 0. In this example this makes sense but think of the following example: 

---

**Your turn**

Assume the following model: 

```{r}
mpg_model = lm(hp ~ wt + cyl, data = mtcars)
coef(mpg_model)
```

How do you interpret the intercept coefficient estimate? 


A: Here, $\hat\beta_{0} =$ `r coef(mpg_model)[[1]]` is our estimate for $\beta_{0}$, the mean gross horsepower for a car that weights 0 pounds and has 0 cylinders. We see our estimate here is negative, which is a physical impossibility. However, this isn’t unexpected, as we shouldn’t expect our model to be accurate for cars which weight 0 pounds and have no cylinders to propel the engine. 

---




```{r fig16, fig.cap='College GPA  High School GPA + Achievment test score', out.width='80%', fig.asp=.75, fig.align='center'}
with (gpa1, {
  # find min-max seq for grid construction
  min_hsGPA <- min(gpa1$hsGPA)
  max_hsGPA <- max(gpa1$hsGPA)
  min_ACT <- min(gpa1$ACT)
  max_ACT <- max(gpa1$ACT)
  
  
  # linear regression
  fit <- lm(colGPA ~ hsGPA + ACT)
  
  # predict values on regular xy grid
  hsGPA.pred <- seq(min_hsGPA, max_hsGPA, length.out = 30)
  ACT.pred <- seq(min_ACT, max_ACT, length.out = 30)
  xy <- expand.grid(hsGPA = hsGPA.pred, 
                    ACT = ACT.pred)
  
  colGPA.pred <- matrix (nrow = 30, ncol = 30, 
                         data = predict(fit, newdata = data.frame(xy), 
                                        interval = "prediction"))
  
  # fitted points for droplines to surface
  fitpoints <- predict(fit) 
  
  scatter3D(z = colGPA, x = hsGPA, y = ACT, pch = 18, cex = 2, 
            theta = 20, phi = 20, ticktype = "detailed",
            xlab = "hsGPA", ylab = "ACT", zlab = "colGPA",  
            surf = list(x = hsGPA.pred, y = ACT.pred, z = colGPA.pred,  
                        facets = NA, fit = fitpoints),
            main = "colGPA")
  
})
```


The data points ($x_{i1}$,$x_{i2}$,$y_{i}$) now exist in 3-dimensional space, so instead of fitting a line to the data, we will fit a plane.

### Ceteris Paribus Interpretation and Omitted Variable bias


Consider a regression with two explanatory variables


\begin{equation}
\hat{y} = \hat{\beta}_{0} + \hat{\beta}_{1}x_{1} +  \hat{\beta}_{2}x_{2}    
(\#eq:lmtwoexplanatory)
\end{equation}


```{r}
# Parameter estimates for full and simple model:
beta.hat <- coef( lm(colGPA ~ ACT+hsGPA, data=gpa1))
beta.hat
```

Now, lets omit one variable in the regression

\begin{equation}
\hat{y} = \hat{\beta}_{0} + \hat{\beta}_{1}x_{1}    
(\#eq:lmomitted)
\end{equation}



```{r}
# Relation between regressors:
delta.tilde <- coef( lm(hsGPA ~ ACT, data=gpa1) )
delta.tilde
```



The parameter $\hat\beta_1$ is the estimated effect of increasing $x_1$ by one unit (and **NOT** keeping $x_2$ fixed). It can be related to $\hat\beta_1$ using the formula

\begin{equation}
\hat{y} = \hat{\beta}_{0} + \hat{\beta}_{1} x_{1} +  \hat{\beta}_{2}  \tilde\delta_{1}
(\#eq:lmommited)
\end{equation}


where $\tilde\delta_{1}$ is the slope parameters of the linear regression of $x_2$ on $x_1$

\begin{equation}
\hat{x}_2 = \tilde\delta_{0} + \tilde\delta_{1} x_{1}
(\#eq:lmomitted2)
\end{equation}




```{r}

# Omitted variables formula for beta1.tilde:
beta.hat["ACT"] + beta.hat["hsGPA"]*delta.tilde["ACT"]

# Actual regression with hsGPA omitted:
summary(lm(colGPA ~ ACT, data=gpa1))

```

In this example, the indirect effect is actually stronger than the direct effect. *ACT* predicts *colGPA* mainly because it is related to *hsGPA* which in turn is strongly related to *colGPA*. 


### Standard errors, Multicollinearity and VIF

Multicollinearity means that two or more regressors in a multiple regression model are strongly correlated. If the correlation between two or more regressors is perfect, that is, one regressor can be written as a linear combination of the other(s), we have perfect multicollinearity. While strong multicollinearity in general is unpleasant as it causes the variance of the OLS estimator to be large (we will discuss this in more detail later), the presence of perfect multicollinearity makes it impossible to solve for the OLS estimator, i.e., the model cannot be estimated in the first place.

#### Perfect multicollinearity

We will work first with the *CAschools* data from the **AER** package to simulate an example of perfect multicollinearity

```{r}
data("CASchools", package = "AER")
?CASchools
```

```{r}
# define the fraction of English learners        
CASchools$FracEL <- CASchools$english / 100

# check the correlation between CASchools$FracEL and CASchools$english 
cor(CASchools$FracEL, CASchools$english)

# estimate the model
mult.mod <- lm(read ~ students + english + FracEL, data = CASchools) 

# obtain a summary of the model
summary(mult.mod)  
```

The row *FracEL* in the coefficients section of the output consists of NA entries since FracEL was excluded from the model.

Another example of perfect multicollinearity is known as the dummy variable trap. This may occur when multiple dummy variables are used as regressors. A common case for this is when dummies are used to sort the data into mutually exclusive categories. For example, suppose we have spatial information that indicates whether a school is located in the North, West, South or East of California.


```{r}
# set seed for reproducibility
set.seed(1)

# generate artificial data on location
CASchools$direction <- sample(c("West", "North", "South", "East"), 
                              420, 
                              replace = T)

# estimate the model
mult.mod <- lm(read ~ students + english + direction, data = CASchools)

# obtain a model summary
summary(mult.mod)   
```

Notice that R solves the problem on its own by generating and including the dummies directionNorth, directionSouth and directionWest but omitting directionEast. Of course, the omission of every other dummy instead would achieve the same. Another solution would be to exclude the constant and to include all dummies instead.


A last example considers the case where a perfect linear relationship arises from redundant regressors. Suppose we have a regressor  
Spanish speakers, *spanish*,  , the percentage of English speakers in the school where

\begin{equation}
spanish = 100 - english  
\end{equation}

and both  *spanish*  and *english*  are included in a regression model. 

```{r}
# Percentage of english speakers 
CASchools$spanish <- 100 - CASchools$english

# estimate the model
mult.mod <- lm(read ~ students + english + spanish, data = CASchools)

# obtain a model summary
summary(mult.mod)                                                 
```

Once more, `lm()` refuses to estimate the full model using OLS and excludes spanish.


#### Imperfect multicollinearity


As opposed to perfect multicollinearity, imperfect multicollinearity is — to a certain extent — less of a problem. In fact, imperfect multicollinearity is the reason why we are interested in estimating multiple regression models in the first place: the OLS estimator allows us to isolate influences of correlated regressors on the dependent variable. If it was not for these dependencies, there would not be a reason to resort to a multiple regression approach and we could simply work with a single-regressor model. However, this is rarely the case in applications. We already know that ignoring dependencies among regressors which influence the outcome variable has an adverse effect on estimation results.

Simulation study: imperfect multicollinearity


```{r}
# set number of observations
n <- 50

# initialize vectors of coefficients
coefs1 <- cbind("hat_beta_1" = numeric(10000), "hat_beta_2" = numeric(10000))
coefs2 <- coefs1

# set seed
set.seed(1)

# loop sampling and estimation
for (i in 1:1000) {
  
  # for cov(X_1,X_2) = 0.25
  X <- rmvnorm(n, c(50, 100), sigma = cbind(c(10, 2.5), c(2.5, 10))) # function from the mvtnorm package
  u <- rnorm(n, sd = 5)
  Y <- 5 + 2.5 * X[, 1] + 3 * X[, 2] + u
  coefs1[i, ] <- lm(Y ~ X[, 1] + X[, 2])$coefficients[-1]
  
  # for cov(X_1,X_2) = 0.85
  X <- rmvnorm(n, c(50, 100), sigma = cbind(c(10, 8.5), c(8.5, 10)))
  Y <- 5 + 2.5 * X[, 1] + 3 * X[, 2] + u
  imperf_multicol <- lm(Y ~ X[, 1] + X[, 2])
  coefs2[i, ] <- lm(Y ~ X[, 1] + X[, 2])$coefficients[-1]
  
}

# obtain variance estimates
diag(var(coefs1))
```

```{r}
diag(var(coefs2))
```

We are interested in the variances which are the diagonal elements. We see that due to the high collinearity, the variances of $\hat{\beta}_1$ and $\hat{\beta}_2$ and have increased, meaning it is more difficult to precisely estimate the true coefficients.



The variance inflation factor,  *VIF*, accounts for (imperfect) multicollinearity.If $x_t$ is highly related to the other regressors, $R^2_j$ and therefore also $VIF_j$ and the variance of $\hat\beta_j$ are large. 


\begin{equation}
\frac{1}{1-R^2_j}
(\#eq:VIF)
\end{equation}


```{r}
GPAres <- lm(colGPA ~ hsGPA + ACT, data = gpa1)
SER<-summary(GPAres)$sigma
```


```{r}
# regressing hsGPA on ACT for calculation of R2 & VIF
( R2.hsGPA  <- summary( lm(hsGPA~ACT, data=gpa1) )$r.squared )
( VIF.hsGPA <- 1/(1-R2.hsGPA) )
```

The **car** package implements the command `vif()` for each regressor


```{r}
vif(GPAres)
vif(imperf_multicol) # from the simulated data
```



### Reporting Regression Results 

As we start moving towards the comparing different regression models this section provides a discussion on how to report regression reports in `R`. Depending on your script (R scripts, R Markdown, bookdown) and what your desired output format is (LaTeX, word, html) the exact approach might differ. There are multiple packages to format regression or table output, most notably **stargazer**^[Stargazer supports ton of options, including theming the LaTex output to journal styles. However, stargazer was written before R Markdown was really a thing, so it has excellent support for HTML and LaTeX output, but that’s it. Including stargazer tables in an R Markdown document is a hassle. **huxtable** on the the contrary plays really nice with **broom** and the **tidyverse**. For more info see Andrew Heiss [blog](https://www.andrewheiss.com/blog/2018/03/08/amelia-broom-huxtable/).], **huxtable**, **Hmisc** and **xtable**. One can also tidy the the regression output as well as tables with **broom** or **summarytool**. The wrapper `knitr::kable()` is a support function that renders the table in an R Markdown in a pretty way. 




#### Table



```{r}
knitr::kable(
  head(gpa1[,1:8], 10), booktabs = TRUE, 
  caption = "A table of the first eight columns and ten rows of the gpa1 data."
)
```



Reporting summary statistics (transposed)
```{r}
descr(gpa1[,1:3], stats = c("mean", "sd", "min", "med", "max"), transpose = TRUE, 
      omit.headings = TRUE, style = "rmarkdown")
```

Including the `knitr::kable()` wrapper

```{r}
knitr::kable(
  descr(gpa1[,1:3], stats = c("mean", "sd", "min", "med", "max"), transpose = TRUE, 
        omit.headings = TRUE, style = "rmarkdown")
)
```



```{r}
model1 <- lm(colGPA ~ hsGPA , data = gpa1)
model2 <- lm(colGPA ~ hsGPA + ACT, data = gpa1)
model3 <- lm(colGPA ~ hsGPA + ACT + age, data = gpa1)
```


```{r, is_latex_output = TRUE, results='asis'}
invisible(stargazer(
  list(model1, 
       model2,
       model3)
  ,keep.stat = c("n", "rsq"), type = "latex", header = FALSE))# to have number of observations and R^2 reported
```

```{r, is_html_output = TRUE, results='asis'}
stargazer(
  list(model1, 
       model2,
       model3)
  ,keep.stat = c("n", "rsq"), type = "html", header = FALSE) # to have number of observations and R^2 reported
```


### Model Formulae

#### Arithmetic operations within a formula

A model relating to birth weight to cigarette smoking of the mother during pregnancy and the family income. 

```{r}
data("bwght")
attach(bwght)
```

```{r}
lm1 <- lm(bwght ~ cigs + faminc, data = bwght)
# Weights in pounds, direct way
lm2 <- lm(I(bwght/16) ~ cigs + faminc, data = bwght)
# Packs of cigarettes
lm3 <- lm(bwght ~ I(cigs/20) + faminc, data = bwght)
```


See table \@ref(tab:regressiontable).

```{r eval=is_html_output()}
huxreg(lm1, lm2, lm3) %>%  
  set_caption('(#tab:regressiontable) Regression table')  # #tab:foo allows to reference to a table directly in a dynamic document. 
```



```{r eval=is_latex_output(), results='asis'}
invisible(stargazer( # invisible supresses additional output such as the package author name when the regression table is compiled
  list(lm1, 
       lm2,
       lm3)
  ,keep.stat = c("n", "rsq"), type = "latex", header = FALSE))# to have number of observations and R^2 reported}
```


Dividing the dependent variable by 16 changes all coefficients by the same factor $\frac{1}{16}$ and dividing the regressor by 20 changes its coefficients by the factor 20. Other statistics like $R^2$ are unaffected. 


#### Standardization: Beta coefficients


The standardized dependent variable $y$ and regressor $x_1$ are


\begin{equation}
z_y=\frac{y-\bar{y}}{sd(y)}
\end{equation}


and 


\begin{equation}
z_{x1}=\frac{x_{1}-\bar{x}_{x1}}{sd(x_{1})}
\end{equation}

They measure by how many *standard deviations* $y$ changes as the respective independent variable increases by *one standard deviation*. 


The model does not include a constant because all averages are removed in the standardization. 


```{r}
data(hprice2)
```

```{r}
lm(scale(price)~0 +  scale(crime) +  scale(rooms) + scale(dist) +  scale(stratio), data = hprice2)
```

#### Logarithms, Quadratics and Polynomials

The model for house prices as in Wooldrige: 

\begin{equation}
log(price) = \beta_0 +  \beta_1 log(nox) + \beta_2 log(dist) + \beta_3 rooms + \beta_4 rooms^{2} + \beta_5 stratio + u 
\end{equation}


```{r}
lm_hprice2 <- lm(log(price)~  log(nox) +  log(dist) + rooms + I(rooms^2) + stratio, data = hprice2)
summary(lm_hprice2)
```

- The quadratic term of rooms significantly positive coefficient $\hat\beta_4$  implying that the semi-elasticity increases with more rooms
- The negative coefficient for rooms indicates that for small number of rooms the price decreases and 
- the positive coefficient for $rooms^2$ implies that for "large" value of rooms the price increases
- The number of rooms implying the smallest price can be found as


\begin{equation}
rooms^{\star} = \frac{-\beta_3}{2\beta_4} \approx 4.4
\end{equation}


```{r}
beta3 <- lm_hprice2$coefficients[[4]]
beta4 <- lm_hprice2$coefficients[[5]]
-beta3 / (2 * beta4)
```




#### Interaction terms


Consider the following model,


\begin{equation}
Y = {\beta}_{0} + {\beta}_{1}x_{1} +  {\beta}_{2}x_{2} + {\beta}_{3}x_{1}x_{2} + u    
(\#eq:interactionterm)
\end{equation}


where $x_1$,  $x_2$, and $Y$ are the same as before, but we have added a new interaction term $x_1x_2$ which multiplies $x_1$ and $x_2$, so we also have an additional $\beta$ parameter $\beta_3$. 

This model essentially creates two slopes and two intercepts, $\beta_2$ being the difference in intercepts and $\beta_3$ being the difference in slopes.

Recall that R reads **x1** times **x2** as $y \sim x_1+x_2+x_1x_2$ and **x1:x2** as $y \sim x_1x_2$.



```{r}
data("attend")
```


```{r}
# Estimate model with interaction effect:
(myres<-lm(stndfnl~atndrte*priGPA+ACT+I(priGPA^2)+I(ACT^2), data=attend))

# Estimate for partial effect at priGPA=2.59:
b <- coef(myres)
b["atndrte"] + 2.59*b["atndrte:priGPA"] 

# Test partial effect for priGPA=2.59:
linearHypothesis(myres,c("atndrte+2.59*atndrte:priGPA"))
```


### MLR Prediction

```{r}
data(gpa2)
```

```{r}
# Regress and report coefficients
reg <- lm(colgpa~sat+hsperc+hsize+I(hsize^2),data=gpa2)
reg

# Generate data set containing the regressor values for predictions
cvalues <- data.frame(sat=1200, hsperc=30, hsize=5)

# Point estimate of prediction
predict(reg, cvalues)

# Point estimate and 95% confidence interval
predict(reg, cvalues, interval = "confidence")

# Define three sets of regressor variables
cvalues <- data.frame(sat=c(1200,900,1400), hsperc=c(30,20,5), 
                      hsize=c(5,3,1))
cvalues
# Point estimates and 99% confidence intervals for these
predict(reg, cvalues, interval = "confidence", level=0.99)
```


#### Prediction intervals

```{r}
# Regress (as before)
reg <- lm(colgpa~sat+hsperc+hsize+I(hsize^2),data=gpa2)

# Define three sets of regressor variables (as before)
cvalues <- data.frame(sat=c(1200,900,1400), hsperc=c(30,20,5), 
                      hsize=c(5,3,1))

# Point estimates and 95% prediction intervals for these
predict(reg, cvalues, interval = "prediction")
```


## MLR Analysis with Qualitative Regressors

### Dummy variabes

```{r}
data(wage1)
```

```{r}

lm1_wage1 <- lm(wage ~ female+educ+exper+tenure, data=wage1)
summary(lm1_wage1)

```

On average a women makes \$ `r -1 * round(coefficients(lm1_wage1)[[2]])` per less than a man with the *same* education, experience, and tenure. 

```{r}
lm2_wage1 <- lm(log(wage)~married*female+educ+exper+I(exper^2)+tenure+I(tenure^2), data=wage1)
summary(lm2_wage1)
```

---

**Your turn**

1. What is the reference group in this model? 
2. Ceteris paribus, how much more wage do single males make relative to the reference group?
3. Ceteris paribus, how much more wage do single females make relative to the reference group?
4. Ceteris paribus, how much less do married females make than single females?
5. Do the results make sense economically. What socio-economic factors could explain the results? 

```{r}
df_lm2_wage1 <- tidy(lm2_wage1)
# Singe male
marriedmale <- df_lm2_wage1 %>%
  filter(term == "married") %>% 
  dplyr::select(estimate) %>% 
  pull() # pull out the single coefficient value of the dataframe
# Single female
singlefemale <- df_lm2_wage1 %>%
  filter(term == "female") %>% 
  dplyr::select(estimate) %>% 
  pull() # pull out the single coefficient value of the dataframe
marriedfemale <- df_lm2_wage1 %>%
  filter(term == "married:female") %>% 
  dplyr::select(estimate) %>% 
  pull() # pull out the single coefficient value of the dataframe
married<- df_lm2_wage1 %>%
  filter(term == "married") %>% #  
  dplyr::select(estimate) %>% 
  pull() # pull out the single coefficient value of the dataframe
```


A: 

1. Reference group: *single* and *male*
2. Cp. married males make `r percent(marriedmale)` (` percent(marriedmale)`) more than single males. 
3. Cp. a single female makes `r percent(singlefemale)` (`percent(singlefemale)`) less than the reference group. 
3. Married females make `r percent(abs(marriedfemale) - abs(married))` (`percent(abs(marriedfemale) - abs(married))`) less than single females. 
4. There seems to be a marriage premium for men but for women the marriage premium is negative. 

---


### Logical variables

```{r}
# replace "female" with logical variable
wage1$female <- as.logical(wage1$female)
table(wage1$female)

# regression with logical variable
lm(wage ~ female+educ+exper+tenure, data=wage1)
```

### Factor variables

As discussed in the R introduction, categorical variables encoded as factors are special *animals* in R. They are immensely useful in a regression when you have a categorical variable with many levels (e.g. “Very Bad”, “Bad”, “Good”, “Very Good”) but can create  a set of subtle issues. Here, we discuss the base R way and the more robust tidyverse way of dealing with factors in the area of regression modelling.    

Factor variables can be directly added to the list of regressors. R is clever enough to implicitly add $g-1$ dummy variables if the factor has $g$ outcomes. 


```{r}
data(CPS1985,package="AER")
str(CPS1985)
# Table of categories and frequencies for two factor variables:
table(CPS1985$gender)
table(CPS1985$occupation)
levels(CPS1985$occupation)
levels(CPS1985$gender)
```


```{r}
# Directly using factor variables in regression formula:
lm(log(wage) ~ education+experience+gender+occupation, data=CPS1985)

# Fragile method (base R)
# Manually redefine the  reference category: 
CPS1985$gender <- relevel(CPS1985$gender,"female")
CPS1985$occupation <- relevel(CPS1985$occupation,"management")

# Rerun regression:
lm(log(wage) ~ education+experience+gender+occupation, data=CPS1985)
```

```{r}
# Robust method (tidyverse)
# Manually redefine the  reference category (back to default): 
CPS1985 <- CPS1985 %>% 
  mutate(gender = fct_relevel(gender, "female")) %>% 
  mutate(occupation = fct_relevel(occupation, "worker"))

lm(log(wage) ~ education+experience+gender+occupation, data=CPS1985)
```

#### Breaking a numeric variable into categories

```{r}
data(lawsch85)
str(lawsch85$rank)
# Define cut points for the rank
cutpts <- c(0,10,25,40,60,100,175)

# Create factor variable containing ranges for the rank
lawsch85$rankcat <- cut(lawsch85$rank, cutpts)

# Display frequencies
table(lawsch85$rankcat)

# Choose reference category
lawsch85$rankcat <- relevel(lawsch85$rankcat,"(100,175]")

# Run regression
(res <- lm(log(salary)~rankcat+LSAT+GPA+log(libvol)+log(cost), data=lawsch85))

# ANOVA table
car::Anova(res)
```

The regression results imply that graduates from the top 100 schools collect a starting salary which is around 70% higher than those of the schools below rank 100. This approximation is inaccurate with these large numbers and the coefficient of 0.7 actually implies a difference of ex(0.7-1) = 1.103 or 101.3%. 



### Interactions and differences in regression functions across groups

Dummy variables and factor variables can be interacted just like any other variable 

- Use the *subset* option of `lm()` to directly define the estimation sample
- The dummy variable *female* is interacted with all other regressor
- The F test for all interaction effects is performed using the function `linearHypothesis()` from the **car** package


```{r}
data(gpa3)

# Model with full interactions with female dummy (only for spring data)
reg<-lm(cumgpa~female*(sat+hsperc+tothrs), data=gpa3, subset=(spring==1))
summary(reg)

# F-Test from package "car". H0: the interaction coefficients are zero
# matchCoefs(...) selects all coeffs with names containing "female"

linearHypothesis(reg, matchCoefs(reg, "female"))
```

As the p-value is much less than 0.05, we reject the null that the interaction with female dummy  is statistically insignificant. 

#### Visualizing coefficients

```{r}
treg <- tidy(reg, conf.int = TRUE)
```

```{r fig17, fig.cap='Coefficient plots', out.width='80%', fig.asp=.75, fig.align='center'}
ggplot(treg, aes(estimate, term, color = term)) +
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high))  +
  geom_vline(xintercept = 0, color = "grey")
```




To recap, the general form in which we specify regression models in R: 

```{r}
## response ~ terms
## 
## y ~ age + sex            # age + sex main effects
## y ~ age + sex + age:sex  # add second-order interaction
## y ~ age*sex              # second-order interaction +
##                          # all main effects
## y ~ (age + sex + pressure)^2
##                          # age+sex+pressure+age:sex+age:pressure...
## y ~ (age + sex + pressure)^2 - sex:pressure
##                          # all main effects and all 2nd order
##                          # interactions except sex:pressure
## y ~ (age + race)*sex     # age+race+sex+age:sex+race:sex
## y ~ treatment*(age*race + age*sex) # no interact. with race,sex
## sqrt(y) ~ sex*sqrt(age) + race
## # functions, with dummy variables generated if
## # race is an R factor (classification) variable
## y ~ sex + poly(age,2)    # poly generates orthogonal polynomials
## race.sex <- interaction(race,sex)
## y ~ age + race.sex       # for when you want dummy variables for
##                          # all combinations of the factors
```



## Heteroskedasticity

The homoskedasticity assumptions SLR.5 and MLR.5 require that the variance of the error term is unrelated to the regressors, i.e.

\begin{equation}
Var(u|x_1, \dots , x_n) = \sigma^2  
\end{equation}

Unbiasedness and consistency do not depend on this assumption, but the sampling distribution does. If homoskedasticity is violated, the standard errors are invalid and all inferences from $t$, $F$, and other tests based on them are unreliable.  

There are various ways of dealing with heteroskedasticity in R. The **car** package provides linear hypothesis. 
For high-dimensional fixed effects the **lfe** package is a good alternative. It also allows to specify clusters as part of the formula. 
A good balance between functionality and ease of use is provided by the **sandwich** package @Zeileis2017.

### Spotting Heteroskedasticity in Scatter Plots


```{r fig18, fig.cap='Heteroskedasticity in the ‘food’ data', out.width='80%', fig.asp=.75, fig.align='center'}
data("food",package="PoEdata")
mod1 <- lm(food_exp~income, data=food)
plot(food$income,food$food_exp, type="p",
     xlab="income", ylab="food expenditure")
abline(mod1)
```

Another useful method to visualize possible heteroskedasticity is to plot the residuals against the regressors suspected of creating heteroskedasticity, or, more generally, against the fitted values of the regression. 

```{r fig19, fig.cap='Residual plots in the ‘food’ model ', out.width='80%', fig.asp=.75, fig.align='center'}
res <- residuals(mod1)
yhat <- fitted(mod1)
plot(food$income,res, xlab="income", ylab="residuals")
plot(yhat,res, xlab="fitted values", ylab="residuals")
```


### Heteroskedasticity Tests

```{r}
data(gpa3, package='wooldridge')


# Estimate model (only for spring data)
reg <- lm(cumgpa~sat+hsperc+tothrs+female+black+white, 
          data=gpa3, subset=(spring==1))


# Breusch-Pagan (BP) Test

bptest(reg)
```


The R function that does this job is `hccm()`, which is part of the car package and yields a heteroskedasticity-robust coefficient covariance matrix. This matrix can then be used with other functions, such as `coeftest()` (instead of summary), `waldtest()` (instead of anova), or `linearHypothesis()` to perform hypothesis testing. The function `hccm()` takes several arguments, among which is the model for which we want the robust standard errors and the type of standard errors we wish to calculate.

```{r}
# Usual SE:
coeftest(reg)

# Refined White heteroscedasticity-robust SE:
coeftest(reg, vcov=hccm)
cov3 <- hccm(reg, type="hc3") # hc3 is the standard method
ref.HC3 <- coeftest(reg, vcov.=cov3)

# Supply other White corrections
cov1 <- hccm(reg, type="hc1")
ref.HC1 <- coeftest(reg, vcov.=cov1)
```




Another way of dealing with heteroskedasticity is to use the `lmrob()` function from the **robustbase** package^[This example has been adapted from the blog post of @Rodrigues2018]. This package is quite interesting, and offers quite a lot of functions for robust linear, and nonlinear, regression models. Running a robust linear regression is just the same as with `lm()`:



```{r}
regrobfit <- lmrob(cumgpa~sat+hsperc+tothrs+female+black+white, 
                   data=gpa3, subset=(spring==1))

summary(regrobfit)
```




This however, gives you different estimates than when fitting a linear regression model. The estimates should be the same, only the standard errors should be different. This is because the estimation method is different, and is also robust to outlines (at least that’s my understanding, I haven’t read the theoretical papers behind the package yet).

Finally, it is also possible to bootstrap the standard errors. For this I will use the `bootstrap()` function from the modelr package:

```{r, cache=TRUE}
resamples <- 100

boot_gpa3 <- gpa3 %>% 
  modelr::bootstrap(resamples)
```



The column strap contains resamples of the original data. I will run my linear regression from before on each of the resamples:

```{r}

boot_lin_reg <- boot_gpa3 %>% 
  mutate(regressions = 
           map(strap, 
               ~lm(cumgpa~sat+hsperc+tothrs+female+black+white, 
                   data= . , subset=(spring==1)))
  )
```


We have added a new column called regressions which contains the linear regressions on each bootstrapped sample. Now, I will create a list of tidied regression results:

```{r}
tidied <- boot_lin_reg %>% 
  mutate(tidy_lm = 
           map(regressions, broom::tidy))
```

```{r}
tidied$tidy_lm[[1]]
```

```{r}
list_mods <- tidied %>% 
  pull(tidy_lm)
```

```{r}
mods_df <- map2_df(list_mods, 
                   seq(1, resamples), 
                   ~mutate(.x, resample = .y))
```


```{r}
head(mods_df, 5)
```


```{r}
r.std.error <- mods_df %>% 
  group_by(term) %>% 
  summarise(r.std.error = sd(estimate))
```


```{r}
reg %>% 
  broom::tidy()   %>% 
  full_join(r.std.error)
```

Using the whole bootstrapping procedure is longer than simply using either one of the first two methods. However, this procedure is very flexible and can thus be adapted to a very large range of situations. 

## Weighted least squares

Weighted Least Squares (WLS) attempts to provide a more efficient alternative to OLS. It is a special version of a feasible generalized least squares (FGLS) estimator. 


```{r}
data("k401k")
```



```{r}
# OLS (only for singles: fsize==1)
lm(nettfa ~ inc + I((age-25)^2) + male + e401k, 
   data=k401ksubs, subset=(fsize==1))

```

Following Wooldrige, we assume that the variance is proportional to the income variable *inc.*. Therefore, the optimal weight is  $\frac{1}{inc}$ which is given as *weight* in the `lm()` call. 

```{r}
# WLS
lm(nettfa ~ inc + I((age-25)^2) + male + e401k, weight=1/inc, 
   data=k401ksubs, subset=(fsize==1))

```

We can also use heteroscedasticity-robust statistics to account for the fact that our variance function might be misspecified. 



```{r}
# WLS
wlsreg <- lm(nettfa ~ inc + I((age-25)^2) + male + e401k, 
             weight=1/inc, data=k401ksubs, subset=(fsize==1))

# non-robust results
coeftest(wlsreg)

# robust results (Refined White SE:)
coeftest(wlsreg,hccm)

coeftest(wlsreg, vcov. = vcovHC)

mySummary <- function(model, VCOV) {
  print(coeftest(model, vcov. = VCOV))
  print(waldtest(model, vcov = VCOV))
}
mySummary(wlsreg, VCOV = vcovHAC)
```

The assumption that the variance is proportional to a regressor is usually  hard to justify. Typically, we do not know the variance function; we have to estimate it. We can estimate the relation between variance and regressors using a linear regression of the log of the squared residuals from an initial OLS regression $log(\hat{u}^{2})$ as the dependent variable. 

Wooldrige suggests two version for the selection of regressors: 

- the regressors $x_1, \dots ,  x_k$ from the original model similar to the BP test
- $\hat{y}$ and $\hat{y}^{2}$ from the original model similar to the White test


```{r}
data("smoke")
# OLS
olsreg<-lm(cigs~log(income)+log(cigpric)+educ+age+I(age^2)+restaurn, 
           data=smoke)
olsreg

# BP test
bptest(olsreg)

# FGLS: estimation of the variance function
logu2 <- log(resid(olsreg)^2)
varreg<-lm(logu2~log(income)+log(cigpric)+educ+age+I(age^2)+restaurn, 
           data=smoke)

# FGLS: WLS
w <- 1/exp(fitted(varreg))
lm(cigs~log(income)+log(cigpric)+educ+age+I(age^2)+restaurn, 
   weight=w ,data=smoke)
```





## Model specification and Parameter Heterogeneity

### Functional Form Misspecifcation 


We have seen many ways to specify the relation between the dependent variable and the regressors. An obvious question is to ask whether or not a given specification is "correct". 

#### RESET

The Regression Equation Specification Error Test (RESET) is a convenient tool to test the null hypothesis that the functional form is adequate. 


We can run the test ourselves or use the boxed routine `resettest()` from the package **lmtest**. 


```{r}
data("hprice1")
# original linear regression
orig <- lm(price ~ lotsize+sqrft+bdrms, data=hprice1)

# regression for RESET test
RESETreg <- lm(price ~ lotsize+sqrft+bdrms+I(fitted(orig)^2)+ 
                 I(fitted(orig)^3), data=hprice1)
RESETreg

# RESET test. H0: all coeffs including "fitted" are=0 
linearHypothesis(RESETreg, matchCoefs(RESETreg,"fitted"))
```


Automatic routine: 

```{r}
# original linear regression
orig <- lm(price ~ lotsize+sqrft+bdrms, data=hprice1)

# RESET test
resettest(orig)
```


Wooldrige (2016, Section 9.2) also discusses tests of non-nested models. We can use the `encomptest()` function from the package **lmtest**. 


Two alternative models for the housing price 


\begin{equation}
price = \beta_0 + \beta_1 lotsize  +\beta_2 sqrft  +\beta_3 bdrms  + u 
\end{equation}

\begin{equation}
price = \beta_0 + \beta_1 log(lotsize)  +\beta_2 log(sqrft)  +\beta_3 log(bdrms)  + u 
\end{equation}




```{r}
# two alternative models
model1 <- lm(price ~     lotsize  +     sqrft  + bdrms, data=hprice1)
model2 <- lm(price ~ log(lotsize) + log(sqrft) + bdrms, data=hprice1)

# Test against comprehensive model
encomptest(model1,model2, data=hprice1)
```

The output shows the "encompassing model " $E$ with all variables. Both models are rejected against this comprehensive model. 


#### Outlying observations 


Dealing with outliers is a tricky business. R offers different packages to test and adjust for outliers. But outliers can be a matter of opinion and not all outlier detection methods give the same results. With the **OutliersO3** package we can compare different outlier detection methods. 

```{r}

data(rdchem)
s3 <- O3prep(rdchem, method=c("HDo", "adjOut", "DDC"))
O3s3 <- O3plotM(s3)
print(O3s3$nOut)
O3s3$gO3
```


An O3 plot of stackloss using the methods HDoutliers,  adjOutlyingness
and DectectDeviatingCells. The darker the cell, the more methods agree. If they all agree, the cell is coloured red and if all but one agree then orange.

We use functions from the **car** package to obtain a table of different measures of leverage and influence for all observations. 

```{r}
# Regression
reg <- lm(rdintens~sales+profmarg, data=rdchem)

# Studentized residuals for all observations:
studres <- rstudent(reg)

# Display extreme values:
min(studres)
max(studres)

# Histogram (and overlayed density plot):
hist(studres, freq=FALSE)
lines(density(studres), lwd=2)
```


#### Missing Data

Missing values in data is a common phenomenon in real world problems. In R, missing data can be represented by different values of the variable. 

- **NA** (not available) indicates that we do not have the information
- **NaN** (not a number) indicates that the value is not defined, for example when we take the log of a negative number


Base R offers many functions to detect missing observations. Sometimes using **mice** and **VIM** package for looking at missing data pattern and imputing missing data is even easier. 

```{r}
data("lawsch85", package = "wooldridge" )

# extract LSAT
lsat <- lawsch85$LSAT

# Create logical indicator for missings
missLSAT <- is.na(lawsch85$LSAT)

# LSAT and indicator for Schools No. 120-129:
rbind(lsat,missLSAT)[,120:129]

# Frequencies of indicator
table(missLSAT)

# Missings for all variables in data frame (counts)
colSums(is.na(lawsch85))

# Indicator for complete cases
compl <- complete.cases(lawsch85)
table(compl)
# MICE package function to display msising values 
head(md.pattern(lawsch85, plot = FALSE)) 
```



```{r fig20, fig.cap='Visualizing missing data', out.width='80%', fig.asp=.75, fig.align='center'}
aggr_plot <- aggr(lawsch85, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(lawsch85), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
```

Regression command like `lm()` have as argument **na.rm=TRUE**! 

```{r}
# Mean of a variable with missings:
mean(lawsch85$LSAT)
mean(lawsch85$LSAT,na.rm=TRUE)

# Regression with missings
summary(lm(log(salary)~LSAT+cost+age, data=lawsch85))
```

R packages provide multiple imputation algorithms. Without going into detail how those algorithms work, we can use for example the **meth="pmm"** argument from the **mice** package to apply a predictive mean matching as imputation method. 

```{r , results = "hide"}
# We use a diffferent dataset to speed up the imputation process
data <- airquality
data[4:10,3] <- rep(NA,7)
data[1:5,4] <- NA
tempData <- mice(data,m=5,maxit=50,meth='pmm',seed=500)
```

```{r}
summary(tempData)
```



## Least absolute Deviations (LAD)  Estimation


As an alternative to OLS, the least absolute deviations (LAD) is less sensitive to outliers. Instead of minimizing the sum of *squared* residuals, it minimizes the sum of the *absolute values* of the residuals. 

In R, general quantile regression (and LAD as the default special case) can easily be implemented with the command `reg()` from the **quantreg** package. 


```{r}
# OLS Regression
ols <- lm(rdintens ~ I(sales/1000) +profmarg, data=rdchem)
# LAD Regression
lad <- rq(rdintens ~ I(sales/1000) +profmarg, data=rdchem)

# regression table
stargazer(ols,lad,  type = "text")
```

Note: LAD inferences are only valid asymptotically, so the results in this example with $n =32$ should be taken with caution. 






