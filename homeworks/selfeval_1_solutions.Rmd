---
title: "MEM5220 - R Econometrics Evaluation"
author: "YOUR NAME HERE"
latex_engine: xelatex
output:
  html_document:
    df_print: paged
    number_sections: true
  pdf_document: default
fontsize: 12pt
---

```{r setup, include=F}
# ignore this fiddly bit
knitr::opts_chunk$set(comment=NA)
```

---

This first R econometrics self-evaluation assignment is focused on data cleaning, data manipulation, plotting and estimating and interpreting simple linear regression models.  You can use **any** additional packages for answering the questions. 

Packages I have used to solve the exercises: 
```{r selfeval_1_solutions-1, message=FALSE}
library(tidyverse)
library(stargazer)
library(huxtable)
library(broom)
library(lmtest)
library(sandwich)
library(car)
library(magrittr)
```


**Note:**

- This assignemtn has to be solved in this R markdown and you should be able to "knit" the document without errors
- Fill out your name in "yaml" - block on top of this document
- Use the R markdown syntax: 
- Write your code in code chunks
- Write your explanations including the equations in markdown syntax
- If you have an error in your code use `#` to comment the line out where the error occurs but do not delete the code itself


----


You will be working with the dataset `Caschool` from the **Ecdat** package, the dataset `Wage1` from **wooldrige** package. In the last step, you will be working with a simulated dataset.  

# Caschool exercises 

## Question: 


Load the dataset `Caschool` from the **Ecdat** package. 

The Caschooldataset contains the average test scores of 420 elementary schools in California along with some additional information.

```{r selfeval_1_solutions-2, message=FALSE}
# install.packages("Ecdat")
library("Ecdat")
data("Caschool", package = "Ecdat")
```



What are the dimensions of the `Caschool` dataset?

**A**: 

```{r selfeval_1_solutions-3}
dim(Caschool)
```

## Question: 

Display the structure of the Caschool dataset. Which variable has values encoded as factors? 

**A**: 

```{r selfeval_1_solutions-4}
str(Caschool)
```


County, district and grspan are encoded as factors. 

## Question: 

Provide a summary statistic of the data

**A**: 

  ```{r selfeval_1_solutions-5}
summary(Caschool)
```



## Question: 

What are the names of the variables in the dataset?

**A**: 


```{r selfeval_1_solutions-6}
names(Caschool)
```


## Question: 

How many unique observations are available in the variable "county"

**A**: 

```{r selfeval_1_solutions-7}
unique(Caschool$county)
```

## Question: 

Summarize the mean number of students grouped by county. 

**A**: 

```{r selfeval_1_solutions-8}
mean_countCaschool <- Caschool %>% 
  group_by(county) %>%
  summarise(mean_count = mean(enrltot)) %>%
  arrange(desc(mean_count))
# options(scipen=999) # remove scientific notation
# head(mean_countCaschool) #  just show the first six counties
```




## Question: 

Calculate the log of average income from of the Caschool dataset. Call the variable **logavginc** and add this variable to the dataset. Then, plot a histogram of the average income vs. a histogram of log average income. What do you observe?

**A**: 


```{r selfeval_1_solutions-9}
Caschool$logavginc <- log(Caschool$avginc)
```


```{r selfeval_1_solutions-10}
par(mfrow=c(1,2))  
hist(Caschool$avginc)
hist(Caschool$logavginc)
```


```{r selfeval_1_solutions-11}
library(gridExtra)
p1 <- ggplot(Caschool, aes(avginc)) +
  geom_histogram(show.legend = FALSE) +
  theme_bw()
p2 <- ggplot(Caschool, aes(logavginc)) +
  geom_histogram(show.legend = FALSE) +
  theme_bw()
grid.arrange(p1, p2,  
             ncol = 2)  
```


Average income is clearly leftward-skewed. The log of averge income looks more like a normal distribution. 


## Question: 

We want to create now a subset of counties that have the ten highest district average income and that have the ten lowest district average income. Call this subset *Caschool_lowhighincome*. 

**Hint**: One way is the create two subsets (eg. Cascholl_highincome and Caschool_lowincome and the use the `rbind()` function to bind them together.). 

**A**:


```{r selfeval_1_solutions-12}
Caschool_highincome <- Caschool %>% 
arrange(desc(avginc)) %>% 
head(10)

Caschool_lowincome <- Caschool %>% 
arrange((avginc)) %>% 
head(10)

Caschool_lowhighincome <- rbind(Caschool_highincome,Caschool_highincome)
```



## Question: 

Let us test wether a high student/teacher ratio will be associated with higher-than-average test scores for the school? Create a scatter plot for the full dataset (*Caschool*) for the variables **testscr** and **str**. You can plot either in base R or use ggplot2. 

**A**: 

Base R-style

```{r selfeval_1_solutions-13}
plot(formula = testscr ~ str,
     data = Caschool,
     xlab = "Student/Teacher Ratio",
     ylab = "Average Test Score", pch = 21, col = 'blue')
```

ggplot2-style

```{r selfeval_1_solutions-14}
ggplot(mapping = aes(x = str, y = testscr), data = Caschool) + # base plot
  geom_point() + # add points
  scale_y_continuous(name = "Average Test Score") + 
  scale_x_continuous(name = "Student/Teacher Ratio") +
  theme_bw() + ggtitle("Testscores vs Student/Teacher Ratio")
```


## Question: 

Suppose a policymaker is interested in the following linear model:

\begin{equation}
testscr = \beta_0 +  \beta_1  str + u
\end{equation}

Where $testscr$ is the average test score for a given school and $str$ is the Student/Teacher Ratio (i.e. the average number of students per teacher). 

Estimate the specified linear model. Is the estimated relationship between a school’s Student/Teacher Ratio and its average test results postitive or negative?

**A**: 

```{r selfeval_1_solutions-15}
fit_single <- lm(formula = testscr ~ str, data = Caschool)
summary(fit_single)
```





## Question:

Now, plot the regression line for the model we have just estimated. Again, you can use either base R or ggplot2-style. 

**A**: 

Base R-style 

```{r selfeval_1_solutions-16}
fit_california <- lm(formula = testscr ~ str, data = Caschool)
plot(formula = testscr ~ str,
     data = Caschool,
     xlab = "Student/Teacher Ratio",
     ylab = "Average Test Score", pch = 21, col = 'blue')# same plot as before
abline(fit_california, col = 'red') # add regression line
```

ggplot2-style

```{r selfeval_1_solutions-17}
ggplot(mapping = aes(x = str, y = testscr), data = Caschool) + # base plot
  geom_point() + # add points
  geom_smooth(method = "lm", size=1, color="red") + # add regression line
  scale_y_continuous(name = "Average Test Score") + 
  scale_x_continuous(name = "Student/Teacher Ratio") + 
  theme_bw() + ggtitle("Testscores vs Student/Teacher Ratio")
```


## Question:

Let us extend our example of student test scores  by adding families’ average income to our previous model:



\begin{equation}
testscr = \beta_0 +  \beta_1  str +   \beta_2  avginc + u
\end{equation}

**A**:


```{r selfeval_1_solutions-18}
fit_multivariate <- lm(formula = "testscr ~ str + avginc", data = Caschool)
summary(fit_multivariate)
```

Addiing the explanatory variable "avginc" to the model, the estimated coefficient of the student/ teacher ratio becomes first smaller compared to the previous model and second insignificant at confentional levels. 

## Question: 

Assume know that "str" depends also on the value of yet another regressor, "avginc". Estimate the following model. Compare the sign of the estimate of $\beta_2$ and $\beta_3$. Interpret the results. 


\begin{equation}
testscr = \beta_0 +  \beta_1  str +   \beta_2  avginc + \beta_3 (str \times avginc)  + u
\end{equation}

**A**: 

```{r selfeval_1_solutions-19}
fit_inter = lm(formula = testscr ~ str + avginc + str*avginc, data = Caschool)
summary(fit_inter)
```



We observe also that the estimate of $\beta_2$ changes signs and becomes negative, while the interaction effect $\beta_3$ is positive. 

This means that an increase in str reduces average student scores (more students per teacher make it harder to teach effectively); that an increase in average district income in isolation actually reduces scores; and that the interaction of both increases scores (more students per teacher are actually a good thing for student performance in richer areas).



## Question: 

In question 10, 12 and 13, you have fitted 3 models. Report the regression results, the number of observations, the Akaike information criterion and the model fit (adj. $R^2$) in a formatted table regression output table. You can use for example the **stargazer** or the **huxtable** package. Which model fits the data best?  

**stargazer** package:

```{r selfeval_1_solutions-20}
library(stargazer)
invisible(stargazer(
list(fit_single, 
fit_multivariate,
fit_inter)
,keep.stat = c("n", "adj.rsq", "aic", "bic"), type = "text", style = "ajps"))# to have number of observations and R^2 reported
```

**huxtable** package:

```{r selfeval_1_solutions-21}
library(huxtable)
huxreg(fit_single, 
fit_multivariate, 
fit_inter, 
statistics = c("nobs", "adj.r.squared", "AIC", "BIC")) 
```

The adjusted $R^2$ is highest for the model 3, the model that includes an interaction term. AIC and BIC, two widely used information criteria, would also select model 3, relative to each of the other models (The relatively quality of the model is maximized when the information criterium is minimized). 

# Wage1 excercises

Wooldridge Source: These are data from the 1976 Current Population Survey. 

```{r selfeval_1_solutions-22, message=FALSE}
# install.packages("wooldridge")
library("wooldridge") 
data("wage1", package = "wooldridge")
```


## Question: 

Estimate the following model: 

\begin{equation}
log(wage) = \beta_0 +  \beta_1 (married \times female) +  \beta_3 educ + \beta_4 exper + beta_5 exper^2 + \beta_6 tenure + \beta_7 tenure^2 + u 
\end{equation}

1. What is the reference group in this model? 
2. Ceteris paribus, how much more wage do single males make relative to the reference group?
3. Ceteris paribus, how much more wage do single females make relative to the reference group?
4. Ceteris paribus, how much less do married females make than single females?
5. Do the results make sense economically. What socio-economic factors could explain the results? 

**A**: 




```{r selfeval_1_solutions-23}
lm2_wage1 <- lm(log(wage)~married*female+educ+exper+I(exper^2)+tenure+I(tenure^2), data=wage1)
summary(lm2_wage1)
```

```{r selfeval_1_solutions-24, message= FALSE}
library(scales) #  quick percent
```


```{r selfeval_1_solutions-25}
df_lm2_wage1 <- tidy(lm2_wage1)
# Singe male
marriedmale <- df_lm2_wage1 %>%
  filter(term == "married") %>% 
  dplyr::select(estimate) %>% 
  pull() # pull out the single coefficient value of the dataframe
# Single female
singlefemale <- df_lm2_wage1 %>%
  filter(term == "female") %>% 
  dplyr::select(estimate) %>% 
  pull() # pull out the single coefficient value of the dataframe
marriedfemale <- df_lm2_wage1 %>%
  filter(term == "married:female") %>% 
  dplyr::select(estimate) %>% 
  pull() # pull out the single coefficient value of the dataframe
married<- df_lm2_wage1 %>%
  filter(term == "married") %>% #  
  dplyr::select(estimate) %>% 
  pull() # pull out the single coefficient value of the dataframe
```




1. Reference group: *single* and *male*
2. Cp. married males make `r percent(marriedmale)` (` percent(marriedmale)`) more than single males. 
3. Cp. a single female makes `r percent(singlefemale)` (`percent(singlefemale)`) less than the reference group. 
3. Married females make `r percent(abs(marriedfemale) - abs(married))` (`percent(abs(marriedfemale) - abs(married))`) less than single females. 
4. There seems to be a marriage premium[^1] for men but for women the marriage premium is negative. 

[^1]: There is clearly a correlation between men having children and men getting higher salaries, and the reverse for women. However, this may reflect the fact that women are more likely to withdraw from work to take care of children (regardless of whether they'd prefer to), and men may double down on work.

## Question: 

Test for heteroskedasdicity test in the estimated regression of the wage1 dataset. Do we reject homoscedasticity for all reasonable signficance levels? Adjust for heteroscedasticity if necessary by using refined White heteroscedasticity-robust SE

```{r selfeval_1_solutions-26}
bptest(lm2_wage1)
```

We do not reject the null hypothesis at conventional signficance levels

## Question


## Question

Now, estimate the following model and test again for heteroscedasticity.

\begin{equation}
wage = \beta_0 +  \beta_1  female +  \beta_3 educ + \beta_4 exper + u 
\end{equation}



Adjust for heteroscedasticity if necessary.

```{r selfeval_1_solutions-27}
lm3_wage1 <- lm(wage~female+educ+exper, data=wage1)
lm3_bptest <- bptest(lm3_wage1)
lm3_bptest
```

The test statistic of the BP-test is `r lm3_bptest$statistic[[1]]` and the corresponding p-value is smaller than `r lm3_bptest$p.value[[1]] `, so we can reject homoscedasticity for all reasonable signficance levels.

```{r selfeval_1_solutions-28}
coeftest(lm3_wage1, vcov=hccm)
cov3 <- hccm(lm3_wage1, type="hc3") # hc3 is the standard method
ref.HC3 <- coeftest(lm3_wage1, vcov.=cov3)
ref.HC3
```



# Collinearity exercises

This exercise focuses on the **collineartiy** problem. 


## Question: 


Perform the following commands in R: 

```{r selfeval_1_solutions-29}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100)/10
y <- 2 +2*x1 + 0.3 *x2 +rnorm(100)
```

The last line corresponds to creating a linear model in which $y$ is a function of $x_1$ and $x_2$. Write out the form of the linear model. What are the regression coefficients?

**A**: 

$y = 2 +2x_1 + 0.3x_2 + \epsilon$

$\beta_0 = 2$, $\beta_1 = 2$, $\beta_3 = 0.3$


## Question: 

What is the correlation between $x_1$ and $x_2$? Create a scatterplot displaying the relationship between the variables. 

**A**:


```{r selfeval_1_solutions-30}
cor(x1, x2)
```


Base R style: 

```{r selfeval_1_solutions-31}
plot(x1, x2)

```

ggplot2 style: 

```{r selfeval_1_solutions-32}
d <-  data.frame(x1,x2)
ggplot(d, aes(x1, x2)) +
  geom_point(shape = 16, size = 3, show.legend = FALSE) +
  theme_minimal() 
```



## Question: 

Using this data, fit a least squares regression to predict $y$ using $x_1$ and $x_2$. Describe the results obtained. What are $\hat{\beta_0}$, $\hat{\beta_1}$ and $\hat{\beta_2}$? How do these relate to the true $\beta_0$, $\beta_1$ and $\beta_2$? Can you reject the null hypothesis $H_0: \beta_1 =0$? How about the null hypothesis $H_0: \beta_2 =0$? 

**A**: 

```{r selfeval_1_solutions-33}
lm.fit = lm(y~x1+x2)
summary(lm.fit)
```

The regression coefficients are close to the true coefficients, although with high standard error. We can reject the null hypothesis for $\beta_1$ because its p-value is below 5%. We cannot reject the null hypothesis for $\beta_2$ because its p-value is much above the 5% typical cutoff, over 60%.


## Question: 

Now fit least squares regression to predict $y$ using only $x_1$. Comment on your results. Can you reject the null hypothesis $H_0: \beta_1=0$? 


**A**:

```{r selfeval_1_solutions-34}
lm.fit = lm(y~x1)
summary(lm.fit)
```

Yes, we can reject the null hypothesis for the regression coefficient given the p-value for its t-statistic is near zero.


## Question: 


Now fit least squares regression to predict $y$ using only $x_2$. Comment on your results. Can you reject the null hypothesis $H_0: \beta_2=0$? 


**A**: 

```{r selfeval_1_solutions-35}
lm.fit = lm(y~x2)
summary(lm.fit)
```

Yes, we can reject the null hypothesis for the regression coefficient given the p-value for its t-statistic is near zero.


## Question: 

Do the results from the previous questions contradict each other? Explain your answer. 


**A**:


No, because $x_1$ and $x_2$ have collinearity, it is hard to distinguish their effects when regressed upon together. When they are regressed upon separately, the linear relationship between y and each predictor is indicated more clearly.


## Question:

Now suppose we obtain one additional observation, which was unfortunately mismeasured. 

```{r selfeval_1_solutions-36}
x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y = c(y,6)
```

Re-fit the linear model using the new data. What effect does this new observation have on the each of the models? In each model, is this observation an outlier? 

**A**: 

```{r selfeval_1_solutions-37}
lm.fit1 = lm(y~x1+x2)
summary(lm.fit1)
```


```{r selfeval_1_solutions-38}
lm.fit2 = lm(y~x1)
summary(lm.fit2)
```

```{r selfeval_1_solutions-39}
lm.fit3 = lm(y~x2)
summary(lm.fit3)
```

In the first model, it shifts $x_1$ to statistically insignificance and shifts $x_2$ to statistiscal significance from the change in p-values between the two linear regressions.


```{r selfeval_1_solutions-40}
library(ggfortify)
```

```{r selfeval_1_solutions-41}
autoplot(lm.fit1)
```



```{r selfeval_1_solutions-42}
autoplot(lm.fit2)
```


```{r selfeval_1_solutions-43}
autoplot(lm.fit3)
```

The additional observation for $x_2$ seems to become a high leverage point.


