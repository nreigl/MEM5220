[
["binarymodels.html", "Chapter 2 Qualitative and LDV Models 2.1 Linear probability models 2.2 Logit and Probit Models: Estimation 2.3 Count data: The Poisson Regression Model 2.4 Censored and Truncated Regression Models", " Chapter 2 Qualitative and LDV Models To load the dataset and necessary functions: # This function 1. checks if the packages are installed. 2. It installs the packages if they were not in the list of installed packages. 3. It loads the packages into the workspace # devtools::install_github(&quot;ccolonescu/PoEdata&quot;) PACKAGES&lt;-c( &quot;tidyverse&quot;, # for data manipulation and ggplots &quot;broom&quot;, # Tidy regression output &quot;Hmisc&quot;, # Harrell Miscellaneous functions &quot;psych&quot;, # Procedures for Psychometric research &quot;car&quot;, # Companion to applied regression &quot;knitr&quot;, # knit functions # &quot;kableExtra&quot;, # extended knit functions for objects exported from other packages &quot;huxtable&quot;, # Regression tables, broom compatible &quot;stargazer&quot;, # Regression tables &quot;AER&quot;, # Functions, data sets, examples, demos, and vignettes for the book Christian Kleiber and Achim Zeileis (2008) &quot;PoEdata&quot;, # R data sets for &quot;Principles of Econometrics&quot; by Hill, Griffiths, and Lim, 4e, Wiley. https://github.com/ccolonescu/PoEdata &quot;wooldridge&quot;, # Wooldrige Datasets &quot;MCMCpack&quot;, # Contains functions to perform Bayesian inference using posterior simulation for a number of ssatistical models. &quot;sampleSelection&quot;, # Two-step and maximum likelihood estimation of Heckman-type sample selection models &quot;scales&quot;, # scale helper functions such as percent &quot;lmtest&quot;, &quot;margins&quot;, # Stata like margin functions &quot;prediction&quot;, # Type stable predictions &quot;nnet&quot;, # Multinomial logit &quot;survival&quot;, # Survival Analysis &quot;sampleSelection&quot;, # Heckman type sample selection &quot;censReg&quot;, # Censored Regression models &quot;magrittr&quot;) # pipes inst&lt;-match(PACKAGES, .packages(all=TRUE)) need&lt;-which(is.na(inst)) if (length(need)&gt;0) install.packages(PACKAGES[need]) lapply(PACKAGES, require, character.only=T) Binary dependent variables are frequently studied in applied economics. Because a dummy variable \\(y\\) can only take values 0 and 1, its (conditional) expected value is equal to the (conditional) probability that \\(y=1\\): \\(E(y|x) = 0 \\times P(y = 0|x) + 1 \\times P(y = 1|x) = P(y=1|x)\\) An important class of models specifies the success probability as \\(P(y = 1 | x) = G(\\beta_0 + \\beta_1 + \\dots \\beta_k x_k) = G(\\boldsymbol{x} \\boldsymbol{\\beta})\\) The following table is taken from Dalpiaz (2016) and summarizes three examples of a generalized linear model: Linear Regression Poisson Regression Logistic Regression \\(Y \\mid {\\bf X} = {\\bf x}\\) \\(N(\\mu({\\bf x}), \\sigma^2)\\) \\(\\text{Pois}(\\lambda({\\bf x}))\\) \\(\\text{Bern}(p({\\bf x}))\\) Distribution Name Normal Poisson Bernoulli (Binomial) \\(\\text{E}[Y \\mid {\\bf X} = {\\bf x}]\\) \\(\\mu({\\bf x})\\) \\(\\lambda({\\bf x})\\) \\(p({\\bf x})\\) Support Real: \\((-\\infty, \\infty)\\) Integer: \\(0, 1, 2, \\ldots\\) Integer: \\(0, 1\\) Usage Numeric Data Count (Integer) Data Binary (Class ) Data Link Name Identity Log Logit Link Function \\(\\eta({\\bf x}) = \\mu({\\bf x})\\) \\(\\eta({\\bf x}) = \\log(\\lambda({\\bf x}))\\) \\(\\eta({\\bf x}) = \\log \\left(\\frac{p({\\bf x})}{1 - p({\\bf x})} \\right)\\) Mean Function \\(\\mu({\\bf x}) = \\eta({\\bf x})\\) \\(\\lambda({\\bf x}) = e^{\\eta({\\bf x})}\\) \\(p({\\bf x}) = \\frac{e^{\\eta({\\bf x})}}{1 + e^{\\eta({\\bf x})}} = \\frac{1}{1 + e^{-\\eta({\\bf x})}}\\) Like ordinary linear regression, we will seek to “fit” the model by estimating the \\(\\beta\\) parameters. To do so, we will use the method of maximum likelihood. Note that a Bernoulli distribution is a specific case of a binomial distribution where the \\(n\\) parameter of a binomial is \\(1\\). Binomial regression is also possible, but we’ll focus on the much more popular Bernoulli case. So, in general, GLMs relate the mean of the response to a linear combination of the predictors, \\(\\eta({\\bf x})\\), through the use of a link function, \\(g()\\). That is, \\[ \\eta({\\bf x}) = g\\left(\\text{E}[Y \\mid {\\bf X} = {\\bf x}]\\right). \\] The mean is then \\[ \\text{E}[Y \\mid {\\bf X} = {\\bf x}] = g^{-1}(\\eta({\\bf x})). \\] 2.1 Linear probability models If a dummy variable is used as the dependent variable \\(y\\), we can still use OLS to estimate its relation to the regressors \\(x\\). data(&quot;mroz&quot;, package = &quot;wooldridge&quot;) Let us use the describe() function from the Hmisc package to use a different way to describe the datatset. describe(mroz) vars n mean sd median trimmed mad min max range skew kurtosis se 1 753 0.568&nbsp;&nbsp;&nbsp; 0.496&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.585&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -0.276&nbsp; -1.93&nbsp;&nbsp; 0.0181&nbsp; 2 753 741&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 871&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 288&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 634&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 427&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.95e+03 4.95e+03 0.921&nbsp; 0.185&nbsp; 31.8&nbsp;&nbsp;&nbsp;&nbsp; 3 753 0.238&nbsp;&nbsp;&nbsp; 0.524&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.119&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3&nbsp;&nbsp;&nbsp; 5.23&nbsp;&nbsp; 0.0191&nbsp; 4 753 1.35&nbsp;&nbsp;&nbsp;&nbsp; 1.32&nbsp;&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.21&nbsp;&nbsp;&nbsp;&nbsp; 1.48&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.906&nbsp; 0.8&nbsp;&nbsp;&nbsp; 0.0481&nbsp; 5 753 42.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8.07&nbsp;&nbsp;&nbsp;&nbsp; 43&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 42.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 10.4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 30&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 60&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 30&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.151&nbsp; -1.02&nbsp;&nbsp; 0.294&nbsp;&nbsp; 6 753 12.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.28&nbsp;&nbsp;&nbsp;&nbsp; 12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 17&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.021&nbsp; 0.734&nbsp; 0.0831&nbsp; 7 428 4.18&nbsp;&nbsp;&nbsp;&nbsp; 3.31&nbsp;&nbsp;&nbsp;&nbsp; 3.48&nbsp;&nbsp;&nbsp;&nbsp; 3.67&nbsp;&nbsp;&nbsp;&nbsp; 1.94&nbsp;&nbsp;&nbsp;&nbsp; 0.128&nbsp;&nbsp; 25&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 24.9&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.07&nbsp;&nbsp; 13.7&nbsp;&nbsp;&nbsp; 0.16&nbsp;&nbsp;&nbsp; 8 753 1.85&nbsp;&nbsp;&nbsp;&nbsp; 2.42&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.44&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 9.98&nbsp;&nbsp;&nbsp;&nbsp; 9.98&nbsp;&nbsp;&nbsp;&nbsp; 1.17&nbsp;&nbsp; 0.724&nbsp; 0.0882&nbsp; 9 753 2.27e+03 596&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.16e+03 2.24e+03 421&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 175&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.01e+03 4.84e+03 0.565&nbsp; 1.84&nbsp;&nbsp; 21.7&nbsp;&nbsp;&nbsp;&nbsp; 10 753 45.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8.06&nbsp;&nbsp;&nbsp;&nbsp; 46&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 45.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 10.4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 30&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 60&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 30&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -0.0643 -1.01&nbsp;&nbsp; 0.294&nbsp;&nbsp; 11 753 12.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.02&nbsp;&nbsp;&nbsp;&nbsp; 12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12.6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.97&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 17&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 14&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -0.276&nbsp; -0.343&nbsp; 0.11&nbsp;&nbsp;&nbsp; 12 753 7.48&nbsp;&nbsp;&nbsp;&nbsp; 4.23&nbsp;&nbsp;&nbsp;&nbsp; 6.98&nbsp;&nbsp;&nbsp;&nbsp; 7.04&nbsp;&nbsp;&nbsp;&nbsp; 3.25&nbsp;&nbsp;&nbsp;&nbsp; 0.412&nbsp;&nbsp; 40.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 40.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.26&nbsp;&nbsp; 11.5&nbsp;&nbsp;&nbsp; 0.154&nbsp;&nbsp; 13 753 2.31e+04 1.22e+04 2.09e+04 2.17e+04 9.08e+03 1.5e+03 9.6e+04&nbsp; 9.45e+04 1.9&nbsp;&nbsp;&nbsp; 6.46&nbsp;&nbsp; 444&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 14 753 0.679&nbsp;&nbsp;&nbsp; 0.0835&nbsp;&nbsp; 0.692&nbsp;&nbsp;&nbsp; 0.687&nbsp;&nbsp;&nbsp; 0.0445&nbsp;&nbsp; 0.442&nbsp;&nbsp; 0.942&nbsp;&nbsp;&nbsp; 0.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -0.64&nbsp;&nbsp; 1.38&nbsp;&nbsp; 0.00304 15 753 9.25&nbsp;&nbsp;&nbsp;&nbsp; 3.37&nbsp;&nbsp;&nbsp;&nbsp; 10&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 9.24&nbsp;&nbsp;&nbsp;&nbsp; 4.45&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 17&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 17&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -0.118&nbsp; -0.104&nbsp; 0.123&nbsp;&nbsp; 16 753 8.81&nbsp;&nbsp;&nbsp;&nbsp; 3.57&nbsp;&nbsp;&nbsp;&nbsp; 7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8.76&nbsp;&nbsp;&nbsp;&nbsp; 4.45&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 17&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 17&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.197&nbsp; -0.0463 0.13&nbsp;&nbsp;&nbsp; 17 753 8.62&nbsp;&nbsp;&nbsp;&nbsp; 3.11&nbsp;&nbsp;&nbsp;&nbsp; 7.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8.57&nbsp;&nbsp;&nbsp;&nbsp; 3.71&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 14&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 11&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.195&nbsp; -0.649&nbsp; 0.114&nbsp;&nbsp; 18 753 0.643&nbsp;&nbsp;&nbsp; 0.48&nbsp;&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.678&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -0.595&nbsp; -1.65&nbsp;&nbsp; 0.0175&nbsp; 19 753 10.6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8.07&nbsp;&nbsp;&nbsp;&nbsp; 9&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 9.76&nbsp;&nbsp;&nbsp;&nbsp; 7.41&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 45&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 45&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.959&nbsp; 0.692&nbsp; 0.294&nbsp;&nbsp; 20 753 20.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 11.6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 17.7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 18.6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8.24&nbsp;&nbsp;&nbsp;&nbsp; -0.0291&nbsp; 96&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 96&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.21&nbsp;&nbsp; 8.35&nbsp;&nbsp; 0.424&nbsp;&nbsp; 21 428 1.19&nbsp;&nbsp;&nbsp;&nbsp; 0.723&nbsp;&nbsp;&nbsp; 1.25&nbsp;&nbsp;&nbsp;&nbsp; 1.22&nbsp;&nbsp;&nbsp;&nbsp; 0.586&nbsp;&nbsp;&nbsp; -2.05&nbsp;&nbsp;&nbsp; 3.22&nbsp;&nbsp;&nbsp;&nbsp; 5.27&nbsp;&nbsp;&nbsp;&nbsp; -0.683&nbsp; 2.53&nbsp;&nbsp; 0.035&nbsp;&nbsp; 22 753 178&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 250&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 81&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 124&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 107&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.02e+03 2.02e+03 2.58&nbsp;&nbsp; 8.69&nbsp;&nbsp; 9.1&nbsp;&nbsp;&nbsp;&nbsp; The function rcorr() can be useful to check visually for correlation. rcorr(as.matrix(mroz)) ## inlf hours kidslt6 kidsge6 age educ wage repwage hushrs ## inlf 1.00 0.74 -0.21 0.00 -0.08 0.19 NaN 0.63 -0.07 ## hours 0.74 1.00 -0.22 -0.09 -0.03 0.11 -0.10 0.61 -0.06 ## kidslt6 -0.21 -0.22 1.00 0.08 -0.43 0.11 0.03 -0.13 0.02 ## kidsge6 0.00 -0.09 0.08 1.00 -0.39 -0.06 -0.08 -0.07 0.10 ## age -0.08 -0.03 -0.43 -0.39 1.00 -0.12 0.03 -0.06 -0.08 ## educ 0.19 0.11 0.11 -0.06 -0.12 1.00 0.34 0.27 0.08 ## wage NaN -0.10 0.03 -0.08 0.03 0.34 1.00 0.42 -0.03 ## repwage 0.63 0.61 -0.13 -0.07 -0.06 0.27 0.42 1.00 -0.07 ## hushrs -0.07 -0.06 0.02 0.10 -0.08 0.08 -0.03 -0.07 1.00 ## husage -0.07 -0.03 -0.44 -0.35 0.89 -0.13 0.03 -0.06 -0.10 ## huseduc 0.05 -0.01 0.13 0.01 -0.16 0.61 0.17 0.11 0.11 ## huswage -0.07 -0.10 0.03 -0.03 0.03 0.28 0.22 0.02 -0.24 ## faminc 0.10 0.15 -0.03 -0.02 0.05 0.36 0.30 0.21 0.13 ## mtr -0.14 -0.19 0.06 0.15 -0.06 -0.41 -0.31 -0.24 -0.14 ## motheduc 0.09 0.06 0.11 0.03 -0.23 0.44 0.06 0.09 0.05 ## fatheduc 0.06 0.01 0.10 -0.03 -0.16 0.44 0.11 0.10 0.05 ## unem -0.03 -0.06 -0.01 0.01 0.08 0.07 0.03 0.01 -0.16 ## city -0.01 -0.02 -0.04 -0.03 0.10 0.16 0.12 0.09 -0.11 ## exper 0.34 0.40 -0.19 -0.30 0.33 0.07 0.05 0.34 -0.10 ## nwifeinc -0.12 -0.12 0.04 0.02 0.06 0.28 0.14 -0.06 0.16 ## lwage NaN -0.02 -0.02 -0.12 0.05 0.34 0.83 0.51 0.00 ## expersq 0.26 0.34 -0.18 -0.30 0.38 0.02 0.04 0.28 -0.07 ## husage huseduc huswage faminc mtr motheduc fatheduc unem city ## inlf -0.07 0.05 -0.07 0.10 -0.14 0.09 0.06 -0.03 -0.01 ## hours -0.03 -0.01 -0.10 0.15 -0.19 0.06 0.01 -0.06 -0.02 ## kidslt6 -0.44 0.13 0.03 -0.03 0.06 0.11 0.10 -0.01 -0.04 ## kidsge6 -0.35 0.01 -0.03 -0.02 0.15 0.03 -0.03 0.01 -0.03 ## age 0.89 -0.16 0.03 0.05 -0.06 -0.23 -0.16 0.08 0.10 ## educ -0.13 0.61 0.28 0.36 -0.41 0.44 0.44 0.07 0.16 ## wage 0.03 0.17 0.22 0.30 -0.31 0.06 0.11 0.03 0.12 ## repwage -0.06 0.11 0.02 0.21 -0.24 0.09 0.10 0.01 0.09 ## hushrs -0.10 0.11 -0.24 0.13 -0.14 0.05 0.05 -0.16 -0.11 ## husage 1.00 -0.20 0.02 0.04 -0.04 -0.23 -0.14 0.05 0.07 ## huseduc -0.20 1.00 0.39 0.38 -0.44 0.32 0.37 0.06 0.23 ## huswage 0.02 0.39 1.00 0.73 -0.72 0.13 0.19 0.16 0.32 ## faminc 0.04 0.38 0.73 1.00 -0.88 0.16 0.21 0.06 0.25 ## mtr -0.04 -0.44 -0.72 -0.88 1.00 -0.19 -0.25 -0.06 -0.26 ## motheduc -0.23 0.32 0.13 0.16 -0.19 1.00 0.57 0.02 0.07 ## fatheduc -0.14 0.37 0.19 0.21 -0.25 0.57 1.00 0.06 0.15 ## unem 0.05 0.06 0.16 0.06 -0.06 0.02 0.06 1.00 0.18 ## city 0.07 0.23 0.32 0.25 -0.26 0.07 0.15 0.18 1.00 ## exper 0.27 -0.04 -0.10 -0.03 -0.03 -0.08 -0.08 0.00 0.01 ## nwifeinc 0.05 0.36 0.76 0.94 -0.81 0.13 0.19 0.07 0.24 ## lwage 0.04 0.16 0.17 0.35 -0.37 0.05 0.08 0.04 0.10 ## expersq 0.31 -0.07 -0.12 -0.04 -0.02 -0.10 -0.10 -0.02 -0.01 ## exper nwifeinc lwage expersq ## inlf 0.34 -0.12 NaN 0.26 ## hours 0.40 -0.12 -0.02 0.34 ## kidslt6 -0.19 0.04 -0.02 -0.18 ## kidsge6 -0.30 0.02 -0.12 -0.30 ## age 0.33 0.06 0.05 0.38 ## educ 0.07 0.28 0.34 0.02 ## wage 0.05 0.14 0.83 0.04 ## repwage 0.34 -0.06 0.51 0.28 ## hushrs -0.10 0.16 0.00 -0.07 ## husage 0.27 0.05 0.04 0.31 ## huseduc -0.04 0.36 0.16 -0.07 ## huswage -0.10 0.76 0.17 -0.12 ## faminc -0.03 0.94 0.35 -0.04 ## mtr -0.03 -0.81 -0.37 -0.02 ## motheduc -0.08 0.13 0.05 -0.10 ## fatheduc -0.08 0.19 0.08 -0.10 ## unem 0.00 0.07 0.04 -0.02 ## city 0.01 0.24 0.10 -0.01 ## exper 1.00 -0.17 0.17 0.94 ## nwifeinc -0.17 1.00 0.14 -0.17 ## lwage 0.17 0.14 1.00 0.13 ## expersq 0.94 -0.17 0.13 1.00 ## ## n ## inlf hours kidslt6 kidsge6 age educ wage repwage hushrs husage ## inlf 753 753 753 753 753 753 428 753 753 753 ## hours 753 753 753 753 753 753 428 753 753 753 ## kidslt6 753 753 753 753 753 753 428 753 753 753 ## kidsge6 753 753 753 753 753 753 428 753 753 753 ## age 753 753 753 753 753 753 428 753 753 753 ## educ 753 753 753 753 753 753 428 753 753 753 ## wage 428 428 428 428 428 428 428 428 428 428 ## repwage 753 753 753 753 753 753 428 753 753 753 ## hushrs 753 753 753 753 753 753 428 753 753 753 ## husage 753 753 753 753 753 753 428 753 753 753 ## huseduc 753 753 753 753 753 753 428 753 753 753 ## huswage 753 753 753 753 753 753 428 753 753 753 ## faminc 753 753 753 753 753 753 428 753 753 753 ## mtr 753 753 753 753 753 753 428 753 753 753 ## motheduc 753 753 753 753 753 753 428 753 753 753 ## fatheduc 753 753 753 753 753 753 428 753 753 753 ## unem 753 753 753 753 753 753 428 753 753 753 ## city 753 753 753 753 753 753 428 753 753 753 ## exper 753 753 753 753 753 753 428 753 753 753 ## nwifeinc 753 753 753 753 753 753 428 753 753 753 ## lwage 428 428 428 428 428 428 428 428 428 428 ## expersq 753 753 753 753 753 753 428 753 753 753 ## huseduc huswage faminc mtr motheduc fatheduc unem city exper ## inlf 753 753 753 753 753 753 753 753 753 ## hours 753 753 753 753 753 753 753 753 753 ## kidslt6 753 753 753 753 753 753 753 753 753 ## kidsge6 753 753 753 753 753 753 753 753 753 ## age 753 753 753 753 753 753 753 753 753 ## educ 753 753 753 753 753 753 753 753 753 ## wage 428 428 428 428 428 428 428 428 428 ## repwage 753 753 753 753 753 753 753 753 753 ## hushrs 753 753 753 753 753 753 753 753 753 ## husage 753 753 753 753 753 753 753 753 753 ## huseduc 753 753 753 753 753 753 753 753 753 ## huswage 753 753 753 753 753 753 753 753 753 ## faminc 753 753 753 753 753 753 753 753 753 ## mtr 753 753 753 753 753 753 753 753 753 ## motheduc 753 753 753 753 753 753 753 753 753 ## fatheduc 753 753 753 753 753 753 753 753 753 ## unem 753 753 753 753 753 753 753 753 753 ## city 753 753 753 753 753 753 753 753 753 ## exper 753 753 753 753 753 753 753 753 753 ## nwifeinc 753 753 753 753 753 753 753 753 753 ## lwage 428 428 428 428 428 428 428 428 428 ## expersq 753 753 753 753 753 753 753 753 753 ## nwifeinc lwage expersq ## inlf 753 428 753 ## hours 753 428 753 ## kidslt6 753 428 753 ## kidsge6 753 428 753 ## age 753 428 753 ## educ 753 428 753 ## wage 428 428 428 ## repwage 753 428 753 ## hushrs 753 428 753 ## husage 753 428 753 ## huseduc 753 428 753 ## huswage 753 428 753 ## faminc 753 428 753 ## mtr 753 428 753 ## motheduc 753 428 753 ## fatheduc 753 428 753 ## unem 753 428 753 ## city 753 428 753 ## exper 753 428 753 ## nwifeinc 753 428 753 ## lwage 428 428 428 ## expersq 753 428 753 ## ## P ## inlf hours kidslt6 kidsge6 age educ wage repwage hushrs ## inlf 0.0000 0.0000 0.9470 0.0272 0.0000 0.0000 0.0738 ## hours 0.0000 0.0000 0.0128 0.3642 0.0036 0.0435 0.0000 0.1224 ## kidslt6 0.0000 0.0000 0.0209 0.0000 0.0028 0.5168 0.0002 0.5057 ## kidsge6 0.9470 0.0128 0.0209 0.0000 0.1063 0.1017 0.0596 0.0063 ## age 0.0272 0.3642 0.0000 0.0000 0.0009 0.5306 0.1098 0.0206 ## educ 0.0000 0.0036 0.0028 0.1063 0.0009 0.0000 0.0000 0.0304 ## wage 0.0435 0.5168 0.1017 0.5306 0.0000 0.0000 0.5061 ## repwage 0.0000 0.0000 0.0002 0.0596 0.1098 0.0000 0.0000 0.0521 ## hushrs 0.0738 0.1224 0.5057 0.0063 0.0206 0.0304 0.5061 0.0521 ## husage 0.0458 0.3943 0.0000 0.0000 0.0000 0.0002 0.5966 0.1288 0.0088 ## huseduc 0.2082 0.7915 0.0002 0.7960 0.0000 0.0000 0.0006 0.0033 0.0030 ## huswage 0.0567 0.0068 0.3749 0.4156 0.4592 0.0000 0.0000 0.5974 0.0000 ## faminc 0.0066 0.0000 0.4465 0.5930 0.1505 0.0000 0.0000 0.0000 0.0004 ## mtr 0.0000 0.0000 0.1010 0.0000 0.1037 0.0000 0.0000 0.0000 0.0002 ## motheduc 0.0130 0.1126 0.0031 0.3749 0.0000 0.0000 0.2387 0.0188 0.1436 ## fatheduc 0.1135 0.7080 0.0083 0.4622 0.0000 0.0000 0.0258 0.0048 0.1676 ## unem 0.4311 0.0983 0.8042 0.6956 0.0345 0.0478 0.5054 0.8026 0.0000 ## city 0.8658 0.6568 0.2426 0.3577 0.0081 0.0000 0.0135 0.0164 0.0020 ## exper 0.0000 0.0000 0.0000 0.0000 0.0000 0.0692 0.2563 0.0000 0.0064 ## nwifeinc 0.0012 0.0006 0.2951 0.4974 0.1078 0.0000 0.0032 0.1258 0.0000 ## lwage 0.7315 0.7026 0.0130 0.2569 0.0000 0.0000 0.0000 0.9378 ## expersq 0.0000 0.0000 0.0000 0.0000 0.0000 0.5084 0.3814 0.0000 0.0406 ## husage huseduc huswage faminc mtr motheduc fatheduc unem ## inlf 0.0458 0.2082 0.0567 0.0066 0.0000 0.0130 0.1135 0.4311 ## hours 0.3943 0.7915 0.0068 0.0000 0.0000 0.1126 0.7080 0.0983 ## kidslt6 0.0000 0.0002 0.3749 0.4465 0.1010 0.0031 0.0083 0.8042 ## kidsge6 0.0000 0.7960 0.4156 0.5930 0.0000 0.3749 0.4622 0.6956 ## age 0.0000 0.0000 0.4592 0.1505 0.1037 0.0000 0.0000 0.0345 ## educ 0.0002 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0478 ## wage 0.5966 0.0006 0.0000 0.0000 0.0000 0.2387 0.0258 0.5054 ## repwage 0.1288 0.0033 0.5974 0.0000 0.0000 0.0188 0.0048 0.8026 ## hushrs 0.0088 0.0030 0.0000 0.0004 0.0002 0.1436 0.1676 0.0000 ## husage 0.0000 0.5897 0.2670 0.2214 0.0000 0.0002 0.1455 ## huseduc 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.1315 ## huswage 0.5897 0.0000 0.0000 0.0000 0.0005 0.0000 0.0000 ## faminc 0.2670 0.0000 0.0000 0.0000 0.0000 0.0000 0.0916 ## mtr 0.2214 0.0000 0.0000 0.0000 0.0000 0.0000 0.1138 ## motheduc 0.0000 0.0000 0.0005 0.0000 0.0000 0.0000 0.6141 ## fatheduc 0.0002 0.0000 0.0000 0.0000 0.0000 0.0000 0.1085 ## unem 0.1455 0.1315 0.0000 0.0916 0.1138 0.6141 0.1085 ## city 0.0636 0.0000 0.0000 0.0000 0.0000 0.0623 0.0000 0.0000 ## exper 0.0000 0.3198 0.0045 0.4478 0.3725 0.0241 0.0306 0.9033 ## nwifeinc 0.2091 0.0000 0.0000 0.0000 0.0000 0.0004 0.0000 0.0398 ## lwage 0.4211 0.0010 0.0005 0.0000 0.0000 0.3305 0.1086 0.3933 ## expersq 0.0000 0.0413 0.0011 0.2458 0.6564 0.0060 0.0068 0.5108 ## city exper nwifeinc lwage expersq ## inlf 0.8658 0.0000 0.0012 0.0000 ## hours 0.6568 0.0000 0.0006 0.7315 0.0000 ## kidslt6 0.2426 0.0000 0.2951 0.7026 0.0000 ## kidsge6 0.3577 0.0000 0.4974 0.0130 0.0000 ## age 0.0081 0.0000 0.1078 0.2569 0.0000 ## educ 0.0000 0.0692 0.0000 0.0000 0.5084 ## wage 0.0135 0.2563 0.0032 0.0000 0.3814 ## repwage 0.0164 0.0000 0.1258 0.0000 0.0000 ## hushrs 0.0020 0.0064 0.0000 0.9378 0.0406 ## husage 0.0636 0.0000 0.2091 0.4211 0.0000 ## huseduc 0.0000 0.3198 0.0000 0.0010 0.0413 ## huswage 0.0000 0.0045 0.0000 0.0005 0.0011 ## faminc 0.0000 0.4478 0.0000 0.0000 0.2458 ## mtr 0.0000 0.3725 0.0000 0.0000 0.6564 ## motheduc 0.0623 0.0241 0.0004 0.3305 0.0060 ## fatheduc 0.0000 0.0306 0.0000 0.1086 0.0068 ## unem 0.0000 0.9033 0.0398 0.3933 0.5108 ## city 0.7583 0.0000 0.0446 0.7565 ## exper 0.7583 0.0000 0.0004 0.0000 ## nwifeinc 0.0000 0.0000 0.0034 0.0000 ## lwage 0.0446 0.0004 0.0034 0.0090 ## expersq 0.7565 0.0000 0.0000 0.0090 Estimate linear probability model linprob &lt;- lm(inlf~nwifeinc+educ+exper+I(exper^2)+age+kidslt6+kidsge6,data=mroz) # Regression table with heteroscedasticity-robust SE and t tests: coeftest(linprob,vcov=hccm) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.58551922 0.15358032 3.8125 0.000149 *** ## nwifeinc -0.00340517 0.00155826 -2.1852 0.029182 * ## educ 0.03799530 0.00733982 5.1766 2.909e-07 *** ## exper 0.03949239 0.00598359 6.6001 7.800e-11 *** ## I(exper^2) -0.00059631 0.00019895 -2.9973 0.002814 ** ## age -0.01609081 0.00241459 -6.6640 5.183e-11 *** ## kidslt6 -0.26181047 0.03215160 -8.1430 1.621e-15 *** ## kidsge6 0.01301223 0.01366031 0.9526 0.341123 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The estimated coefficient educ can be interpreted as: an additional year of schooling increases the probability that a woman is in the labor force ceteris paribus by 3.80%. One problem with linear probability models is that \\(P(y=1|x)\\) is specified as a linear function of the regressors. By construction, there are more or less realistic combinations of regressor values that yield \\(\\hat{y} &lt; 0\\) or \\(\\hat{y} &gt; 1\\). Predictions for two “extreme” women: xpred &lt;- list(nwifeinc=c(100,0),educ=c(5,17),exper=c(0,30), age=c(20,52),kidslt6=c(2,0),kidsge6=c(0,0)) predict(linprob,xpred) ## 1 2 ## -0.4104582 1.0428084 2.2 Logit and Probit Models: Estimation For binary response models, the most widely used specification for G are the probit model with \\(G(z) = \\phi(z)\\), the standard cdf and the logit model with \\(G(z) = \\Lambda(z) = \\frac{exp(z)}{1+ exp(z)}\\), the cdf of the logistic distribution. In R many generalized linear models can be estimated by the glm() command. It accepts the additional option family = binomial (link = logit) for the logit model or family = binomial (link = probit) for the probit model # Estimate logit model logitres&lt;-glm(inlf~nwifeinc+educ+exper+I(exper^2)+age+kidslt6+kidsge6, family=binomial(link=logit),data=mroz) # Summary of results: summary(logitres) ## ## Call: ## glm(formula = inlf ~ nwifeinc + educ + exper + I(exper^2) + age + ## kidslt6 + kidsge6, family = binomial(link = logit), data = mroz) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1770 -0.9063 0.4473 0.8561 2.4032 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.425452 0.860365 0.495 0.62095 ## nwifeinc -0.021345 0.008421 -2.535 0.01126 * ## educ 0.221170 0.043439 5.091 3.55e-07 *** ## exper 0.205870 0.032057 6.422 1.34e-10 *** ## I(exper^2) -0.003154 0.001016 -3.104 0.00191 ** ## age -0.088024 0.014573 -6.040 1.54e-09 *** ## kidslt6 -1.443354 0.203583 -7.090 1.34e-12 *** ## kidsge6 0.060112 0.074789 0.804 0.42154 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1029.75 on 752 degrees of freedom ## Residual deviance: 803.53 on 745 degrees of freedom ## AIC: 819.53 ## ## Number of Fisher Scoring iterations: 4 # Log likelihood value: logLik(logitres) ## &#39;log Lik.&#39; -401.7652 (df=8) # McFadden&#39;s pseudo R2: 1 - logitres$deviance/logitres$null.deviance ## [1] 0.2196814 We can also extract the fitted values from the specified model and compare them the observed predicated binomial explained variable. fit &lt;- data.frame(response = mroz$inlf, predicted = round(fitted(logitres), 0)) xtabs(~ predicted + response, data = fit) ## response ## predicted 0 1 ## 0 207 81 ## 1 118 347 If you want to interpret the estimated effects as relative odds ratios (plus their confidence intervals): exp(logitres$coefficients) ## (Intercept) nwifeinc educ exper I(exper^2) age ## 1.5302825 0.9788810 1.2475360 1.2285929 0.9968509 0.9157386 ## kidslt6 kidsge6 ## 0.2361344 1.0619557 exp(confint(logitres)) # Lets convert the confidence intervals the same way ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 0.2829827 8.2907776 ## nwifeinc 0.9625447 0.9949193 ## educ 1.1473738 1.3607465 ## exper 1.1540225 1.3092803 ## I(exper^2) 0.9948707 0.9988906 ## age 0.8894633 0.9418229 ## kidslt6 0.1567171 0.3485513 ## kidsge6 0.9175642 1.2306237 resultsTable &lt;- exp(cbind(OddsRatios = coef(logitres), confint(logitres))) ## Waiting for profiling to be done... round(resultsTable, digits = 5) ## OddsRatios 2.5 % 97.5 % ## (Intercept) 1.53028 0.28298 8.29078 ## nwifeinc 0.97888 0.96254 0.99492 ## educ 1.24754 1.14737 1.36075 ## exper 1.22859 1.15402 1.30928 ## I(exper^2) 0.99685 0.99487 0.99889 ## age 0.91574 0.88946 0.94182 ## kidslt6 0.23613 0.15672 0.34855 ## kidsge6 1.06196 0.91756 1.23062 This gives you \\(e^\\beta\\), the multiplicative change in the odds ratio for \\(y=1\\) if the covariate associated with \\(\\beta\\) increases by 1. 2.2.1 Multinomial Logit A relatively common R function that fits multinomial logit models is multinom() from package nnet. Let us use the dataset nels_small for an example of how multinom works. The variable grades in this dataset is an index, with best grades represented by lower values of grade. We try to explain the choice of a secondary institution (psechoice) only by the high school grade. The variable pschoice can take one of three values: psechoice = 1 no college, psechoice = 2 two year college psechoice = 3 four year college data(&quot;nels_small&quot;, package=&quot;PoEdata&quot;) nels.multinom &lt;- multinom(psechoice~grades, data=nels_small) ## # weights: 9 (4 variable) ## initial value 1098.612289 ## iter 10 value 875.313116 ## final value 875.313099 ## converged summary(nels.multinom) ## Call: ## multinom(formula = psechoice ~ grades, data = nels_small) ## ## Coefficients: ## (Intercept) grades ## 2 2.505273 -0.3086404 ## 3 5.770170 -0.7062468 ## ## Std. Errors: ## (Intercept) grades ## 2 0.4183944 0.05228532 ## 3 0.4043290 0.05292638 ## ## Residual Deviance: 1750.626 ## AIC: 1758.626 medGrades &lt;- median(nels_small$grades) fifthPercentileGrades &lt;- quantile(nels_small$grades, .05) newdat &lt;- data.frame(grades=c(medGrades, fifthPercentileGrades)) pred &lt;- predict(nels.multinom, newdat, &quot;probs&quot;) pred ## 1 2 3 ## 0.18101808 0.28557312 0.5334088 ## 5% 0.01781764 0.09662199 0.8855604 2.2.2 The Conditional Logit Model In the multinomial logit model all individuals faced the same external conditions and each individual’s choice is only determined by an individual’s circumstances or preferences. The conditional logit model allows for individuals to face individual-specific external conditions, such as the price of a product. Suppose we want to study the effect of price on an individual’s decision about choosing one of three brands of soft drinks: pepsi sevenup coke R offers several alternatives that allow fitting conditional logit models, one of which is the function MCMCmnl() from the package MCMCpack (others are, for instance, clogit() in the survival package and mclogit() in the mclogit package). The following code is adapted from Adkins (2014). data(&quot;cola&quot;, package=&quot;PoEdata&quot;) N &lt;- nrow(cola) N3 &lt;- N/3 price1 &lt;- cola$price[seq(1,N,by=3)] price2 &lt;- cola$price[seq(2,N,by=3)] price3 &lt;- cola$price[seq(3,N,by=3)] bchoice &lt;- rep(&quot;1&quot;, N3) for (j in 1:N3){ if(cola$choice[3*j-1]==1) bchoice[j] &lt;- &quot;2&quot; if(cola$choice[3*j]==1) bchoice[j] &lt;- &quot;3&quot; } cola.clogit &lt;- MCMCmnl(bchoice ~ choicevar(price1, &quot;b2&quot;, &quot;1&quot;)+ choicevar(price2, &quot;b2&quot;, &quot;2&quot;)+ choicevar(price3, &quot;b2&quot;, &quot;3&quot;), baseline=&quot;3&quot;, mcmc.method=&quot;IndMH&quot;) ## Calculating MLEs and large sample var-cov matrix. ## This may take a moment... ## Inverting Hessian to get large sample var-cov matrix. The output from function multinom gives coefficient estimates for each level of the response variable psechoice, except for the first level, which is the benchmark. We can calculate the probability that individual \\(i\\) chooses pepsi and sevenup for some given values of the prices that individual \\(i\\). Coke is not shown as it is the baseline choice. sclogit &lt;- summary(cola.clogit) tabMCMC &lt;- as.data.frame(sclogit$statistics)[,1:2] row.names(tabMCMC)&lt;- c(&quot;b2&quot;,&quot;b11&quot;,&quot;b12&quot;) kable(tabMCMC, digits=4, align=&quot;c&quot;, caption=&quot;Conditional logit estimates for the &#39;cola&#39; problem&quot;) Table 2.1: Conditional logit estimates for the ‘cola’ problem Mean SD b2 -2.2991 0.1382 b11 0.2839 0.0610 b12 0.1037 0.0621 We can also estimate the probabilities of individual \\(i\\) choosing Pepsi or Sevenup for a given price. pPepsi &lt;- 1 pSevenup &lt;- 1.25 pCoke &lt;- 1.10 b13 &lt;- 0 b2 &lt;- tabMCMC$Mean[1] b11 &lt;- tabMCMC$Mean[2] b12 &lt;- tabMCMC$Mean[3] # The probability that individual i chooses Pepsi: PiPepsi &lt;- exp(b11+b2*pPepsi)/ (exp(b11+b2*pPepsi)+exp(b12+b2*pSevenup)+ exp(b13+b2*pCoke)) # The probability that individual i chooses Sevenup: PiSevenup &lt;- exp(b12+b2*pSevenup)/ (exp(b11+b2*pPepsi)+exp(b12+b2*pSevenup)+ exp(b13+b2*pCoke)) # The probability that individual i chooses Coke: PiCoke &lt;- 1-PiPepsi-PiSevenup The calculatred probabilities are: \\(p_{i,pepsi}=0.483\\) \\(p_{i,sevenup}=0.227\\) \\(p_{i,coke}=0.289\\) The three probabilities are different for different individuals because different individuals face different prices; in a more complex model other regressors may be included, some of which may reflect individual characteristics. 2.2.3 Ordered Choice Models The order of choices in these models is meaningful, unlike the multinomial and conditional logit model we have studied so far. The following example explains the choice of higher education, when the choice variable is psechoice and the only regressor is grades; the dataset, nels_small , is already known to us. The R package MCMCpack is again used here, with its function MCMCoprobit(). nels.oprobit &lt;- MCMCoprobit(psechoice ~ grades, data=nels_small, mcmc=10000) sOprobit &lt;- summary(nels.oprobit) tabOprobit &lt;- sOprobit$statistics[, 1:2] kable(tabOprobit, digits=4, align=&quot;c&quot;, caption=&quot;Ordered probit estimates for the &#39;nels&#39; problem&quot;) Table 2.2: Ordered probit estimates for the ‘nels’ problem Mean SD (Intercept) 2.9542 0.1478 grades -0.3074 0.0193 gamma2 0.8616 0.0487 The results from MCMCoprobit can be translated into the textbook notations as follows: \\(\\mu_1\\) =− (Intercept) \\(\\beta\\) = grades \\(\\mu_2\\) = gamma2 − (Intercept) The probabilities for each choice can be calculated as in the next code fragment: mu1 &lt;- -tabOprobit[1] b &lt;- tabOprobit[2] mu2 &lt;- tabOprobit[3]-tabOprobit[1] xGrade &lt;- c(mean(nels_small$grades), quantile(nels_small$grades, 0.05)) # Probabilities: prob1 &lt;- pnorm(mu1-b*xGrade) prob2 &lt;- pnorm(mu2-b*xGrade)-pnorm(mu1-b*xGrade) prob3 &lt;- 1-pnorm(mu2-b*xGrade) # Marginal effects: Dp1DGrades &lt;- -pnorm(mu1-b*xGrade)*b Dp2DGrades &lt;- (pnorm(mu1-b*xGrade)-pnorm(mu2-b*xGrade))*b Dp3DGrades &lt;- pnorm(mu2-b*xGrade)*b For instance, the marginal effect of grades on the probability of attending a four-year college for a student with average grade and for a student in the top 5 percent are, respectively, -0.143 and -0.031. 2.2.4 Probit model x &lt;- seq(-3,3, .2) plot(x, pnorm(x), type=&quot;l&quot;, xlab=&quot;b1+b2x&quot;, ylab=&quot;P[y=1]&quot;) Figure 2.1: The shape of the probit function is the standard normal distribution plot(x, dnorm(x), type=&quot;l&quot;) Figure 2.1: The shape of the probit function is the standard normal distribution detach(&quot;package:PoEdata&quot;, unload=TRUE) data(&quot;mroz&quot;, package=&#39;wooldridge&#39;) attach(mroz) # Estimate probit model probitres&lt;-glm(inlf~nwifeinc+educ+exper+I(exper^2)+age+kidslt6+kidsge6, family=binomial(link=probit),data=mroz) # Summary of results: summary(probitres) ## ## Call: ## glm(formula = inlf ~ nwifeinc + educ + exper + I(exper^2) + age + ## kidslt6 + kidsge6, family = binomial(link = probit), data = mroz) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2156 -0.9151 0.4315 0.8653 2.4553 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.2700736 0.5080782 0.532 0.59503 ## nwifeinc -0.0120236 0.0049392 -2.434 0.01492 * ## educ 0.1309040 0.0253987 5.154 2.55e-07 *** ## exper 0.1233472 0.0187587 6.575 4.85e-11 *** ## I(exper^2) -0.0018871 0.0005999 -3.145 0.00166 ** ## age -0.0528524 0.0084624 -6.246 4.22e-10 *** ## kidslt6 -0.8683247 0.1183773 -7.335 2.21e-13 *** ## kidsge6 0.0360056 0.0440303 0.818 0.41350 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1029.7 on 752 degrees of freedom ## Residual deviance: 802.6 on 745 degrees of freedom ## AIC: 818.6 ## ## Number of Fisher Scoring iterations: 4 # Log likelihood value: logLik(probitres) ## &#39;log Lik.&#39; -401.3022 (df=8) # McFadden&#39;s pseudo R2: 1 - probitres$deviance/probitres$null.deviance ## [1] 0.2205805 Take a boostrap sample for the 95% confidence interval for each parameter. data(&quot;mroz&quot;, package=&#39;wooldridge&#39;) boot_probitres &lt;- mroz %&gt;% bootstrap(500) %&gt;% do(tidy(glm(inlf~nwifeinc+educ+exper+I(exper^2)+age+kidslt6+kidsge6, family=binomial(link=probit),.))) boot_probitres %&gt;% group_by(term) %&gt;% dplyr::summarise(low = quantile(estimate, .025), high = quantile(estimate, .0975)) term low high (Intercept) 0.27&nbsp;&nbsp;&nbsp; 0.27&nbsp;&nbsp;&nbsp; age -0.0529&nbsp; -0.0529&nbsp; educ 0.131&nbsp;&nbsp; 0.131&nbsp;&nbsp; exper 0.123&nbsp;&nbsp; 0.123&nbsp;&nbsp; I(exper^2) -0.00189 -0.00189 kidsge6 0.036&nbsp;&nbsp; 0.036&nbsp;&nbsp; kidslt6 -0.868&nbsp;&nbsp; -0.868&nbsp;&nbsp; nwifeinc -0.012&nbsp;&nbsp; -0.012&nbsp;&nbsp; 2.2.5 Inference We can implement the test for overall significance for the probit model using both manual and automatic calculations. # Test of overall significance: # Manual calculation of the LR test statistic: probitres$null.deviance - probitres$deviance ## [1] 227.142 # Automatic calculations including p-values,...: lrtest(probitres) #Df LogLik Df Chisq Pr(&gt;Chisq) 8 -401 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 -515 -7 227 2.01e-45 # Test of H0: experience and age are irrelevant restr &lt;- glm(inlf~nwifeinc+educ+ kidslt6+kidsge6, family=binomial(link=logit),data=mroz) lrtest(restr,probitres) #Df LogLik Df Chisq Pr(&gt;Chisq) 5 -465 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8 -401 3 127 2.12e-27 2.2.6 Predictions The command predict() can calculate predicted values for the estimation sample or arbitrary sets of regressor values. We can calculate # predictions for two &quot;extreme&quot; women: xpred &lt;- list(nwifeinc=c(100,0),educ=c(5,17),exper=c(0,30), age=c(20,52),kidslt6=c(2,0),kidsge6=c(0,0)) # Predictions from linear probability, probit and logit model: predict(linprob, xpred,type = &quot;response&quot;) ## 1 2 ## -0.4104582 1.0428084 predict(logitres, xpred,type = &quot;response&quot;) ## 1 2 ## 0.005218002 0.950049117 predict(probitres,xpred,type = &quot;response&quot;) ## 1 2 ## 0.001065043 0.959869044 # Simulated data set.seed(12345) y &lt;- rbinom(100, 1, 0.5) x &lt;- rnorm(100) + 2 * y # Estimation linpr.res &lt;- lm(y~x) logit.res &lt;-glm(y ~x, family = binomial(link = logit)) probit.res &lt;-glm(y ~x, family = binomial(link = probit)) # Prediction from a regular grid of x values xp &lt;- seq(from= min(x), to=max(x), length=50) linpr.p &lt;- predict(linpr.res, list(x = xp), type = &quot;response&quot;) logit.p &lt;- predict(logit.res, list(x = xp), type = &quot;response&quot;) probit.p &lt;- predict(probit.res, list(x = xp), type = &quot;response&quot;) plot(x,y) lines(xp, linpr.p, lwd=2, lty = 1) lines(xp, logit.p, lwd=2, lty = 2) lines(xp, probit.p, lwd=1, lty = 1) legend(&quot;topleft&quot;, c(&quot;linear prob.&quot;, &quot;logit&quot;, &quot;probit&quot;), lwd = c(2,2,1), lty = c(1,2,1)) Figure 2.2: Predictions from binary response models (simulated data) 2.2.7 Marginal (partial) effects Several packages provide estimates of marginal effects for different types of models. Among these are car, alr3, mfx, erer, among others. Unfortunately, none of these packages implement marginal effects correctly (i.e., correctly account for interrelated variables such as interaction terms (e.g., a:b) or power terms (e.g., I(a^2)) and the packages all implement quite different interfaces for different types of models. The margins and prediction packages are a combined effort to calculate marginal effects that include complex terms and provide a uniform interface for doing those calculations. To know how much a variable influences the labour force participation, one has to use margins() command: effects_logit_participation &lt;- margins(logitres) summary(effects_logit_participation) factor AME SE z p lower upper age -0.0157&nbsp; 0.00238 -6.6&nbsp;&nbsp; 4.04e-11 -0.0204&nbsp; -0.0111&nbsp;&nbsp; educ 0.0395&nbsp; 0.00729 5.41&nbsp; 6.15e-08 0.0252&nbsp; 0.0538&nbsp;&nbsp; exper 0.0254&nbsp; 0.00224 11.4&nbsp;&nbsp; 5.99e-30 0.021&nbsp;&nbsp; 0.0298&nbsp;&nbsp; kidsge6 0.0107&nbsp; 0.0133&nbsp; 0.805 0.421&nbsp;&nbsp;&nbsp; -0.0154&nbsp; 0.0369&nbsp;&nbsp; kidslt6 -0.258&nbsp;&nbsp; 0.0319&nbsp; -8.07&nbsp; 7.05e-16 -0.32&nbsp;&nbsp;&nbsp; -0.195&nbsp;&nbsp;&nbsp; nwifeinc -0.00381 0.00148 -2.57&nbsp; 0.0101&nbsp;&nbsp; -0.00672 -0.000906 plot(effects_logit_participation) If one desires subgroup effects, simply pass a subset of data to the data argument: summary(margins(logitres, data = subset(mroz, kidslt6 == 0))) # no kids &lt; 6 years factor AME SE z p lower upper age -0.0156&nbsp; 0.00234 -6.64&nbsp; 3.04e-11 -0.0202&nbsp; -0.011&nbsp;&nbsp;&nbsp; educ 0.0391&nbsp; 0.00726 5.39&nbsp; 7.13e-08 0.0249&nbsp; 0.0534&nbsp;&nbsp; exper 0.0246&nbsp; 0.00212 11.6&nbsp;&nbsp; 4.23e-31 0.0204&nbsp; 0.0287&nbsp;&nbsp; kidsge6 0.0106&nbsp; 0.0132&nbsp; 0.807 0.42&nbsp;&nbsp;&nbsp;&nbsp; -0.0152&nbsp; 0.0365&nbsp;&nbsp; kidslt6 -0.255&nbsp;&nbsp; 0.033&nbsp;&nbsp; -7.73&nbsp; 1.09e-14 -0.32&nbsp;&nbsp;&nbsp; -0.191&nbsp;&nbsp;&nbsp; nwifeinc -0.00378 0.00147 -2.57&nbsp; 0.0102&nbsp;&nbsp; -0.00666 -0.000894 ggplot(data = summary(effects_logit_participation)) + geom_point(aes(factor, AME)) + geom_errorbar(aes(x = factor, ymin = lower, ymax = upper)) + geom_hline(yintercept = 0, color=&quot;lightgrey&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45)) Figure 2.3: Logit effect plot You can also extract the marginal effects of a single variable, with dydx: head(dydx(mroz, logitres, &quot;educ&quot;)) dydx_educ 0.0464 0.0416 0.0463 0.0384 0.0538 0.0335 The function cplot() provides the commonly needed visual summaries of predictions or average marginal effects conditional on a covariate. cplot(logitres, x = &quot;educ&quot;, se.type = &quot;shade&quot;) ## xvals yvals upper lower ## 1 5.0 0.2549509 0.3787666 0.1311353 ## 2 5.5 0.2765192 0.3988913 0.1541471 ## 3 6.0 0.2991794 0.4190899 0.1792689 ## 4 6.5 0.3228678 0.4392936 0.2064421 ## 5 7.0 0.3475019 0.4594481 0.2355556 ## 6 7.5 0.3729801 0.4795186 0.2664417 ## 7 8.0 0.3991836 0.4994946 0.2988726 ## 8 8.5 0.4259774 0.5193953 0.3325594 ## 9 9.0 0.4532129 0.5392750 0.3671508 ## 10 9.5 0.4807315 0.5592299 0.4022331 ## 11 10.0 0.5083674 0.5794045 0.4373304 ## 12 10.5 0.5359524 0.5999963 0.4719084 ## 13 11.0 0.5633190 0.6212490 0.5053891 ## 14 11.5 0.5903055 0.6434188 0.5371922 ## 15 12.0 0.6167588 0.6666929 0.5668248 ## 16 12.5 0.6425384 0.6910722 0.5940047 ## 17 13.0 0.6675187 0.7162931 0.6187444 ## 18 13.5 0.6915913 0.7418704 0.6413122 ## 19 14.0 0.7146659 0.7672368 0.6620950 ## 20 14.5 0.7366715 0.7918749 0.6814681 Figure 2.4: Marginal effects for logit model 2.3 Count data: The Poisson Regression Model Instead of just 0/1-coded binary data, count data can take any non-negative integer \\(0, 1, 2, \\dots\\). If they take very large numbers (like the number of students in a school), they can be approximated reasonably well as contiouns variables in a linear models and estimated using OLS. If the numbers are relativly small, this approximation might not work well. The Poisson regression model is the most basic and convenient model explicitly model explicitly designed for count data. Poisson regression models can be estimated in R via the glm() function with the specification family= poisson Estimating the model with quasipoisson is to adjust for potential vioalations of the Poisson distribution. data(crime1, package=&#39;wooldridge&#39;) # Estimate linear model lm.res &lt;- lm(narr86~pcnv+avgsen+tottime+ptime86+qemp86+inc86+ black+hispan+born60, data=crime1) # Estimate Poisson model Poisson.res &lt;- glm(narr86~pcnv+avgsen+tottime+ptime86+qemp86+inc86+ black+hispan+born60, data=crime1, family=poisson) # Quasi-Poisson model QPoisson.res&lt;- glm(narr86~pcnv+avgsen+tottime+ptime86+qemp86+inc86+ black+hispan+born60, data=crime1, family=quasipoisson) stargazer(lm.res,Poisson.res,QPoisson.res,type=&quot;text&quot;,keep.stat=&quot;n&quot;) ## ## ================================================== ## Dependent variable: ## ------------------------------------- ## narr86 ## OLS Poisson glm: quasipoisson ## link = log ## (1) (2) (3) ## -------------------------------------------------- ## pcnv -0.132*** -0.402*** -0.402*** ## (0.040) (0.085) (0.105) ## ## avgsen -0.011 -0.024 -0.024 ## (0.012) (0.020) (0.025) ## ## tottime 0.012 0.024* 0.024 ## (0.009) (0.015) (0.018) ## ## ptime86 -0.041*** -0.099*** -0.099*** ## (0.009) (0.021) (0.025) ## ## qemp86 -0.051*** -0.038 -0.038 ## (0.014) (0.029) (0.036) ## ## inc86 -0.001*** -0.008*** -0.008*** ## (0.0003) (0.001) (0.001) ## ## black 0.327*** 0.661*** 0.661*** ## (0.045) (0.074) (0.091) ## ## hispan 0.194*** 0.500*** 0.500*** ## (0.040) (0.074) (0.091) ## ## born60 -0.022 -0.051 -0.051 ## (0.033) (0.064) (0.079) ## ## Constant 0.577*** -0.600*** -0.600*** ## (0.038) (0.067) (0.083) ## ## -------------------------------------------------- ## Observations 2,725 2,725 2,725 ## ================================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 By construction, the parameter estiamtes are the same but the standard errors are larger for the QMLE. 2.3.1 Corner Solution Response: The Tobit Model Corner solutions describe situations where the variable of interest is continous but restricted in range. Typically, it cannot be negative. The package censReg offers the command censReg() for estimating a Tobit model. We have already estimated labor supply mdoeld for the women in the dataset mroz, ignoring the fact that the hours worked is necessarily non-negative. # Estimate Tobit model using censReg: TobitRes &lt;- censReg(hours~nwifeinc+educ+exper+I(exper^2)+ age+kidslt6+kidsge6, data=mroz ) summary(TobitRes) ## ## Call: ## censReg(formula = hours ~ nwifeinc + educ + exper + I(exper^2) + ## age + kidslt6 + kidsge6, data = mroz) ## ## Observations: ## Total Left-censored Uncensored Right-censored ## 753 325 428 0 ## ## Coefficients: ## Estimate Std. error t value Pr(&gt; t) ## (Intercept) 965.30528 446.43613 2.162 0.030599 * ## nwifeinc -8.81424 4.45910 -1.977 0.048077 * ## educ 80.64561 21.58324 3.736 0.000187 *** ## exper 131.56430 17.27939 7.614 2.66e-14 *** ## I(exper^2) -1.86416 0.53766 -3.467 0.000526 *** ## age -54.40501 7.41850 -7.334 2.24e-13 *** ## kidslt6 -894.02174 111.87803 -7.991 1.34e-15 *** ## kidsge6 -16.21800 38.64139 -0.420 0.674701 ## logSigma 7.02289 0.03706 189.514 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Newton-Raphson maximisation, 7 iterations ## Return code 1: gradient close to zero ## Log-likelihood: -3819.095 on 9 Df # Partial Effects at the average x: margEff(TobitRes) ## nwifeinc educ exper I(exper^2) age kidslt6 ## -5.326442 48.734094 79.504232 -1.126509 -32.876918 -540.256831 ## kidsge6 ## -9.800526 Another alternative for estimating Tobit models is the command survreg from the package survival. It is less straightforward to use but more flexible. # Estimate Tobit model using survreg: res &lt;- survreg(Surv(hours, hours&gt;0, type=&quot;left&quot;) ~ nwifeinc+educ+exper+ I(exper^2)+age+kidslt6+kidsge6, data=mroz, dist=&quot;gaussian&quot;) summary(res) ## ## Call: ## survreg(formula = Surv(hours, hours &gt; 0, type = &quot;left&quot;) ~ nwifeinc + ## educ + exper + I(exper^2) + age + kidslt6 + kidsge6, data = mroz, ## dist = &quot;gaussian&quot;) ## Value Std. Error z p ## (Intercept) 965.3053 446.4361 2.16 0.03060 ## nwifeinc -8.8142 4.4591 -1.98 0.04808 ## educ 80.6456 21.5832 3.74 0.00019 ## exper 131.5643 17.2794 7.61 2.7e-14 ## I(exper^2) -1.8642 0.5377 -3.47 0.00053 ## age -54.4050 7.4185 -7.33 2.2e-13 ## kidslt6 -894.0217 111.8780 -7.99 1.3e-15 ## kidsge6 -16.2180 38.6414 -0.42 0.67470 ## Log(scale) 7.0229 0.0371 189.51 &lt; 2e-16 ## ## Scale= 1122 ## ## Gaussian distribution ## Loglik(model)= -3819.1 Loglik(intercept only)= -3954.9 ## Chisq= 271.59 on 7 degrees of freedom, p= 7e-55 ## Number of Newton-Raphson Iterations: 4 ## n= 753 2.4 Censored and Truncated Regression Models Censored regression models are closely related to Tobit models. In the basic Tobit model we observe \\(y = y^*\\) in the “uncensored” cases with \\(y^* &gt; 0\\) and we only know an upper bound for \\(y^* \\le 0\\) if we observe $y 0 = $. In this example we are are interested in criminal prognosis of individuals released from prison to reoffend. data(recid, package=&#39;wooldridge&#39;) # Define Dummy for UNcensored observations recid$uncensored &lt;- recid$cens==0 # Estimate censored regression model: res&lt;-survreg(Surv(log(durat),uncensored, type=&quot;right&quot;) ~ workprg+priors+ tserved+felon+alcohol+drugs+black+married+educ+age, data=recid, dist=&quot;gaussian&quot;) # Output: summary(res) ## ## Call: ## survreg(formula = Surv(log(durat), uncensored, type = &quot;right&quot;) ~ ## workprg + priors + tserved + felon + alcohol + drugs + black + ## married + educ + age, data = recid, dist = &quot;gaussian&quot;) ## Value Std. Error z p ## (Intercept) 4.099386 0.347535 11.80 &lt; 2e-16 ## workprg -0.062572 0.120037 -0.52 0.6022 ## priors -0.137253 0.021459 -6.40 1.6e-10 ## tserved -0.019331 0.002978 -6.49 8.5e-11 ## felon 0.443995 0.145087 3.06 0.0022 ## alcohol -0.634909 0.144217 -4.40 1.1e-05 ## drugs -0.298160 0.132736 -2.25 0.0247 ## black -0.542718 0.117443 -4.62 3.8e-06 ## married 0.340684 0.139843 2.44 0.0148 ## educ 0.022920 0.025397 0.90 0.3668 ## age 0.003910 0.000606 6.45 1.1e-10 ## Log(scale) 0.593586 0.034412 17.25 &lt; 2e-16 ## ## Scale= 1.81 ## ## Gaussian distribution ## Loglik(model)= -1597.1 Loglik(intercept only)= -1680.4 ## Chisq= 166.74 on 10 degrees of freedom, p= 1.3e-30 ## Number of Newton-Raphson Iterations: 4 ## n= 1445 2.4.1 The Heckman, or Sample Selection Model The models are useful when the sample selection is not random, but whehter an individual is in the sample depends on individual characteristics. For example, when studying wage determination for married women, some women are not in the labour force, therefore their wages are zero. The Heckit procedure involves two steps, estimating both the selection equation and the equation of interest. Function selection() in the sampleSelection package performs both steps; therefore, it needs both equations among its arguments. (The selection equation is, in fact, a probit model.) data(&quot;mroz&quot;, package=&#39;wooldridge&#39;) wage.heckit &lt;- selection(inlf~age+educ+I(kidslt6+kidsge6)+mtr, log(wage)~educ+exper, data=mroz, method=&quot;ml&quot;) summary(wage.heckit) ## -------------------------------------------- ## Tobit 2 model (sample selection model) ## Maximum Likelihood estimation ## Newton-Raphson maximisation, 4 iterations ## Return code 2: successive function values within tolerance limit ## Log-Likelihood: -913.5131 ## 753 observations (325 censored and 428 observed) ## 10 free parameters (df = 743) ## Probit selection equation: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.53798 0.61889 2.485 0.0132 * ## age -0.01346 0.00603 -2.232 0.0259 * ## educ 0.06278 0.02180 2.879 0.0041 ** ## I(kidslt6 + kidsge6) -0.05108 0.03276 -1.559 0.1194 ## mtr -2.20864 0.54620 -4.044 5.81e-05 *** ## Outcome equation: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.646221 0.235569 2.743 0.00623 ** ## educ 0.066462 0.016573 4.010 6.68e-05 *** ## exper 0.011972 0.004085 2.931 0.00348 ** ## Error terms: ## Estimate Std. Error t value Pr(&gt;|t|) ## sigma 0.84112 0.04302 19.55 &lt;2e-16 *** ## rho -0.82768 0.03911 -21.16 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## -------------------------------------------- References "]
]
