[
["index.html", "MEM52220 - Applied Econometrics Introduction Prerequisites Resources Software information and conventions Acknowledgements", " MEM52220 - Applied Econometrics Nicolas Reigl 2018-10-28 Introduction Figure .: Normal and simulated Gaussian densities on a regression line Welcome to MEM5220 - Applied Econometrics. This handout is designed for the use with MEM5220 - Applied Econometrics at Tallinn University of Technology. Note that this workbook is still under heavy development! Prerequisites A basic knowledge of the R (Team 2013) programming language is required. In order to reproduce the examples in this script You need the statistical software package R. Additionally we recommend using the RStudio integrated developer environment (IDE) which will improve Your R working experience. Installation of R and RStudio Go to the following webpages and download the software adequate for Your operating system: The Statistical Software Package R: The GUI RStudio: Install R first and then proceed with RStudio. Afterwards start RStudio. For accessing packages and datasets from github we need to install the devtools package. On Windows, download and install Rtools, and devtools takes care of the rest. On Mac, install the Xcode command line tools. On Linux, install the R development package, usually called r-devel or r-base-dev. Resources Our primary resource is Heiss (2016).1 The book is available as free online version. For theoretical concepts I refer to Wooldridge (2015)2. For the timeseries part of the course a good reference material is Introductory Econometircs for Finance. Standing on the shoulders of giants This lecture material would not possible without many other open-source econometrics teaching materials and of course the R package developers. In addition to the main resources, examples and code of this workbook have been drawn from a number of free econometrics eBooks, blogs, R-vignette help pages and other researchers teaching materials. Attribution Chapter 1.1.2 Simulating SLR is based on Heiss (2016) but adds parts of Dalpiaz (2016) and Colonescu (2018) teaching materials. Chapter 1.2.2 includes additonal material from Hanck et al. (2018). Chapter 1.4 Heteroskedasticity has been amended with material from Rodrigues (2018) blog. Using R for Introduction to Econometrics, Hanck et al. (2018) Applied Statistics with R, Dalpiaz (2016) Principles of Econometrics with R, Colonescu (2018) Introduction to Econometrics with R, Oswald and Robin (2018) Broadening Your Statistical Horizons Software information and conventions The R session information when compiling this book is shown below: sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] zip_1.0.0 Rcpp_0.12.19 highr_0.7 ## [4] cellranger_1.1.0 pillar_1.3.0 compiler_3.5.1 ## [7] forcats_0.3.0 tools_3.5.1 htmldeps_0.1.1 ## [10] digest_0.6.18 evaluate_0.12 tibble_1.4.2 ## [13] lattice_0.20-35 pkgconfig_2.0.2 rlang_0.3.0.1 ## [16] openxlsx_4.1.0 rstudioapi_0.8 curl_3.2 ## [19] yaml_2.2.0 xfun_0.4 haven_1.1.2 ## [22] rio_0.5.10 stringr_1.3.1 knitr_1.20 ## [25] hms_0.4.2.9001 lmtest_0.9-36 grid_3.5.1 ## [28] data.table_1.11.8 readxl_1.1.0 dynlm_0.3-5 ## [31] foreign_0.8-71 rmarkdown_1.10.14 bookdown_0.7.21 ## [34] carData_3.0-2 car_3.0-2 magrittr_1.5 ## [37] htmltools_0.3.6 abind_1.4-5 stringi_1.2.4 ## [40] crayon_1.3.4 zoo_1.8-4 I do not add prompts (&gt; and +) to R source code in this book, and I comment out the text output with two hashes ## by default, as you can see from the R session information above. This is for your convenience when you want to copy and run the code (the text output will be ignored since it is commented out). Package names are in bold text (e.g., rmarkdown), and inline code and function arguments are formatted in a typewriter font (e.g., knitr::knit('foo.Rmd')). Function names are followed by parentheses (e.g., bookdown::render_book()). The double-colon operator :: means accessing an object from a package. Content with Note, Your turn and Overthinking is surrounded by two horizontal lines. Overthinking sections serve as additional information for the interested reader but will not be covered in detail in class. Acknowledgements I thank Kadri Männasoo and Juan Carlos Cuestas for proofreading and their useful comments. References "],
["linearregression.html", "Chapter 1 Linear Regression 1.1 Simple Linear Regression 1.2 Multiple Linear Regression 1.3 MLR Analysis with Qualitative Regressors 1.4 Heteroskedasticity 1.5 Weighted least squares 1.6 Model specification and Parameter Heterogeneity 1.7 Least absolute Deviations (LAD) Estimation", " Chapter 1 Linear Regression To load the dataset and necessary functions: # This function 1. checks if the packages are installed. 2. It installs the packages if they were not in the list of installed packages. 3. It loads the packages into the workspace # devtools::install_github(&quot;ccolonescu/PoEdata&quot;) PACKAGES&lt;-c(&quot;PoEdata&quot;, # R data sets for &quot;Principles of Econometrics&quot; by Hill, Griffiths, an d Lim, 4e, Wiley. https://github.com/ccolonescu/PoEdata &quot;wooldridge&quot;, # Wooldrige Datasets &quot;tidyverse&quot;, # for data manipulation and ggplots &quot;broom&quot;, # Tidy regression output &quot;ggpubr&quot;, # Multiple ggplots on a page. Note that, the installation of ggpubr will automatically install the gridExtra and the cowplot package; so you don’t need to re-install them. &quot;ggfortify&quot;, # Simple ggplot recipe for lm objects) &quot;plot3D&quot;, # 3D graphs &quot;car&quot;, # Companion to applied regression &quot;knitr&quot;, # knit functions # &quot;kableExtra&quot;, # extended knit functions for objects exported from other packages &quot;huxtable&quot;, # Regression tables, broom compatible &quot;mice&quot;, # multiple imputation &quot;VIM&quot;, # visualizing missing data &quot;stargazer&quot;, # Regression tables &quot;AER&quot;, # Functions, data sets, examples, demos, and vignettes for the book Christian Kleiber and Achim Zeileis (2008) &quot;MASS&quot;, # Functions and datasets to support Venables and Ripley, &quot;Modern Applied Statistics with S&quot; &quot;mvtnorm&quot;, # Multivariate Normal and t Distributions &quot;summarytools&quot;, # Report regression summary tables &quot;scales&quot;, # scale helper functions such as percent &quot;OutliersO3&quot;, # Outlier comparison method &quot;robustbase&quot;, # Basic robust statistics &quot;quantreg&quot;, # Quantile regression &quot;modelr&quot;, # model simulation/ boostraping the modern way &quot;magrittr&quot;) # pipes inst&lt;-match(PACKAGES, .packages(all=TRUE)) need&lt;-which(is.na(inst)) if (length(need)&gt;0) install.packages(PACKAGES[need]) lapply(PACKAGES, require, character.only=T) 1.1 Simple Linear Regression We start off with a simple OLS Regression. We will work with multiple data sources: Data from Wooldridge (2015) : Introductory Econometrics: A Modern Approach. R data sets for “Principles of Econometrics” by Hill et al. (2008) Build in examples such as the airquality dataset Classic examples of quantities modeled with simple linear regression: College GPA \\(\\sim\\) SAT scores \\(\\beta &gt; 0\\) Change in GDP \\(\\sim\\) change in unemployment \\(\\beta &lt; 0\\) House price \\(\\sim\\) number of bedrooms \\(\\beta &gt; 0\\) Species heart weight \\(\\sim\\) species body weight \\(\\beta &gt; 0\\) Fatalities per year \\(\\sim\\) speed limit \\(\\beta &lt; 0\\) Notice that these simple linear regressions are simplifications of more complex relationships between the variables in question. In this exercise we use the dataset ceosal1. Let us analyse the dataset first data(&quot;ceosal1&quot;) help(&quot;ceosal1&quot;) ?ceosal1 As we see from the R documentation the ceosal1 dataset contain of a random sample of data reported in the May 6, 1991 issue of Businessweek. To get a first look at the data you can use the View() function inside R Studio. View(ceosal1) # For the compilation we omit the full View() Table 1.1: A table of the first eight columns and ten rows of the ceosal1 data. salary pcsalary sales roe pcroe ros indus finance 1095 20 27595.0 14.1 106.4 191 1 0 1001 32 9958.0 10.9 -30.6 13 1 0 1122 9 6125.9 23.5 -16.3 14 1 0 578 -9 16246.0 5.9 -25.7 -21 1 0 1368 7 21783.2 13.8 -3.0 56 1 0 1145 5 6021.4 20.0 1.0 55 1 0 1078 10 2266.7 16.4 -5.9 62 1 0 1094 7 2966.8 16.3 -1.6 44 1 0 1237 16 4570.2 10.5 -70.2 37 1 0 833 5 2830.0 26.3 -23.9 37 1 0 We could also take a look at the variable names, the dimension of the data frame, and some sample observations with str(). str(ceosal1) ## &#39;data.frame&#39;: 209 obs. of 12 variables: ## $ salary : int 1095 1001 1122 578 1368 1145 1078 1094 1237 833 ... ## $ pcsalary: int 20 32 9 -9 7 5 10 7 16 5 ... ## $ sales : num 27595 9958 6126 16246 21783 ... ## $ roe : num 14.1 10.9 23.5 5.9 13.8 ... ## $ pcroe : num 106.4 -30.6 -16.3 -25.7 -3 ... ## $ ros : int 191 13 14 -21 56 55 62 44 37 37 ... ## $ indus : int 1 1 1 1 1 1 1 1 1 1 ... ## $ finance : int 0 0 0 0 0 0 0 0 0 0 ... ## $ consprod: int 0 0 0 0 0 0 0 0 0 0 ... ## $ utility : int 0 0 0 0 0 0 0 0 0 0 ... ## $ lsalary : num 7 6.91 7.02 6.36 7.22 ... ## $ lsales : num 10.23 9.21 8.72 9.7 9.99 ... ## - attr(*, &quot;time.stamp&quot;)= chr &quot;25 Jun 2011 23:03&quot; As we have seen before in the general R tutorial, there are a number of additional functions to access some of this information directly. dim(ceosal1) ## [1] 209 12 nrow(ceosal1) ## [1] 209 ncol(ceosal1) ## [1] 12 summary(ceosal1) ## salary pcsalary sales roe ## Min. : 223 Min. :-61.00 Min. : 175.2 Min. : 0.50 ## 1st Qu.: 736 1st Qu.: -1.00 1st Qu.: 2210.3 1st Qu.:12.40 ## Median : 1039 Median : 9.00 Median : 3705.2 Median :15.50 ## Mean : 1281 Mean : 13.28 Mean : 6923.8 Mean :17.18 ## 3rd Qu.: 1407 3rd Qu.: 20.00 3rd Qu.: 7177.0 3rd Qu.:20.00 ## Max. :14822 Max. :212.00 Max. :97649.9 Max. :56.30 ## pcroe ros indus finance ## Min. :-98.9 Min. :-58.0 Min. :0.0000 Min. :0.0000 ## 1st Qu.:-21.2 1st Qu.: 21.0 1st Qu.:0.0000 1st Qu.:0.0000 ## Median : -3.0 Median : 52.0 Median :0.0000 Median :0.0000 ## Mean : 10.8 Mean : 61.8 Mean :0.3206 Mean :0.2201 ## 3rd Qu.: 19.5 3rd Qu.: 81.0 3rd Qu.:1.0000 3rd Qu.:0.0000 ## Max. :977.0 Max. :418.0 Max. :1.0000 Max. :1.0000 ## consprod utility lsalary lsales ## Min. :0.0000 Min. :0.0000 Min. :5.407 Min. : 5.166 ## 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:6.601 1st Qu.: 7.701 ## Median :0.0000 Median :0.0000 Median :6.946 Median : 8.217 ## Mean :0.2871 Mean :0.1722 Mean :6.950 Mean : 8.292 ## 3rd Qu.:1.0000 3rd Qu.:0.0000 3rd Qu.:7.249 3rd Qu.: 8.879 ## Max. :1.0000 Max. :1.0000 Max. :9.604 Max. :11.489 The interesting task here is to determine how far a high the CEO salary is, for a given return on equity. Your turn What sign would be expect of \\(\\beta\\) (the slope)? A: Without seeing the data my prior is that \\(\\beta &gt; 0\\). Note A simple linear model as assumes that the mean of each \\(y_{i}\\) conditioned on \\(x_{i}\\) is a linear function of \\(x_{i}\\). But notice that simple linear regressions are simplifications of more complex relationships between the variables in question Dalpiaz (2016). # Use ggplot style ggplot(ceosal1, aes(x = roe, y = salary)) + geom_point() Figure 1.1: Relationship between ROE and Salary Consider a simple regression model \\(salary = \\beta_0 + \\beta_1roe + u\\) In the general form the linear regression model can be written as: \\[\\begin{equation} y = \\beta_{0} + \\beta_{1}x + u \\tag{1.1} \\end{equation}\\] We are concerned with the population parameter \\(\\beta_{0}\\) and \\(\\beta_{1}\\). The ordinary least squares (OLS) estimators are: \\[\\begin{equation} \\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x} \\tag{1.2} \\end{equation}\\] The ordinary least squares (OLS) estimators are \\[\\begin{equation} \\hat{\\beta}_{1} = \\frac{Cov(x,y)}{Var(x)} \\tag{1.3} \\end{equation}\\] Ingredients for the OLS formulas attach(ceosal1) cov(roe, salary) ## [1] 1342.538 var(roe) ## [1] 72.56499 mean(salary) ## [1] 1281.12 Manual calculation of the OLS coefficients b1hat &lt;- cov(roe,salary)/var(roe) b0hat &lt;- mean(salary) - b1hat * mean(roe) Or use the lm() function lm(salary ~ roe, data=ceosal1) ## ## Call: ## lm(formula = salary ~ roe, data = ceosal1) ## ## Coefficients: ## (Intercept) roe ## 963.2 18.5 lm1_ceosal1 &lt;- lm(salary ~ roe, data=ceosal1) summary(lm1_ceosal1) ## ## Call: ## lm(formula = salary ~ roe, data = ceosal1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1160.2 -526.0 -254.0 138.8 13499.9 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 963.19 213.24 4.517 1.05e-05 *** ## roe 18.50 11.12 1.663 0.0978 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1367 on 207 degrees of freedom ## Multiple R-squared: 0.01319, Adjusted R-squared: 0.008421 ## F-statistic: 2.767 on 1 and 207 DF, p-value: 0.09777 Plot the linear regression fit the base r way. plot(salary~ roe, data = ceosal1, xlab = &quot;Return on equity&quot;, ylab = &quot;Salary&quot;, main = &quot;Salary vs return on equity&quot;, pch = 20, cex = 2, col = &quot;grey&quot;) abline(lm1_ceosal1, lwd = 3, col = &quot;darkorange&quot;) Figure 1.2: OLS regression base Rstyle Or use ggplot ggplot(ceosal1, aes(x = roe, y = salary)) + geom_point() + stat_smooth(method = &quot;lm&quot;, col = &quot;red&quot;) Figure 1.3: OLS regression ggplot2 style Determine the names of the elements of the list using the names() command. names(lm1_ceosal1) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; Extract one element, for example the residuals from the list object head(lm1_ceosal1$residuals) # head() just prints out the first 6 residual values ## 1 2 3 4 5 6 ## -129.0581 -163.8543 -275.9692 -494.3483 149.4923 -188.2151 Another way to access stored information in lm1_ceosal1 are the coef(), resid(), and fitted() functions. These return the coefficients, residuals, and fitted values, respectively. coef(lm1_ceosal1) ## (Intercept) roe ## 963.19134 18.50119 The function summary() is useful in many situations. We see that when it is called on our model, it returns a good deal of information. summary(lm1_ceosal1) ## ## Call: ## lm(formula = salary ~ roe, data = ceosal1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1160.2 -526.0 -254.0 138.8 13499.9 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 963.19 213.24 4.517 1.05e-05 *** ## roe 18.50 11.12 1.663 0.0978 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1367 on 207 degrees of freedom ## Multiple R-squared: 0.01319, Adjusted R-squared: 0.008421 ## F-statistic: 2.767 on 1 and 207 DF, p-value: 0.09777 The summary() command also returns a list, and we can again use names() to learn what about the elements of this list. names(summary(lm1_ceosal1)) ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; So, for example, if we wanted to directly access the value of \\(R^2\\), instead of copy and pasting it out of the printed statement from summary(), we could do so. summary(lm1_ceosal1)$r.squared ## [1] 0.01318862 Your turn Recall that the explained sum of squares (SSE) is \\[\\begin{equation} SSE = \\sum_{i=1}^{n}(\\hat{y}_{i} - \\bar{y})^2 = (n-1) \\times Var(\\hat{y}) \\tag{1.4} \\end{equation}\\] and the residual sum of squares (SSR) is \\[\\begin{equation} R^2 = \\frac{Var(\\hat{y})}{Var(y)} = 1 - \\frac{Var(\\hat{u})}{Var(y)} \\tag{1.5} \\end{equation}\\] One can see that the correlation between observed and fitted values is a square root of \\(R^2\\). Calculate \\(R^2\\) manually: var(fitted(lm1_ceosal1))/var(ceosal1$salary) ## [1] 0.01318862 1 - var(residuals(lm1_ceosal1))/var(ceosal1$salary) ## [1] 0.01318862 Another useful function is the predict() function. set.seed(123) # unique(ceosal1$roe) roe_sample &lt;-sample(ceosal1$roe, 1) roe_sample ## [1] 20.3 Let’s make a prediction for salary when the return on equity is 20.2999992. b0hat_sample &lt;- mean(salary) - b1hat * roe_sample We are not restricted to observed values of the explanatory variable. Instead we can supply also our own predictor values predict(lm1_ceosal1, newdata = data.frame(roe = 30)) ## 1 ## 1518.227 The above code reads “predict the salary when the return on equity is 30 using the lm1_ceosal1 model.” Overthinking 1.1.1 Regression through the Origin and Regression on a Constant Regression without intercept (through origin) lm2 &lt;- lm(salary ~ 0 + roe, data = ceosal1) Regression without slope lm3 &lt;- lm(salary ~ 1, data = ceosal1) plot(salary~ roe, data = ceosal1, xlab = &quot;Return on equity&quot;, ylab = &quot;Salary&quot;, main = &quot;Salary vs return on equity&quot;, pch = 20, cex = 2, col = &quot;grey&quot;) abline(lm1_ceosal1, lwd = 3, lty = 1, col = &quot;darkorange&quot;) abline(lm2,lwd = 3, lty = 2, col = &quot;darkblue&quot;) abline(lm3, lwd = 3, lty = 3, col = &quot;black&quot;) legend(&quot;topleft&quot;, c(&quot;full&quot;, &quot;through origin&quot;, &quot;constant only&quot;), lwd =2, lty = 1:3) Figure 1.4: Regression through the Origin and on a Constant In models without the intercept the \\(R^2\\) loses its interpretatation. The reason is that the \\(R^2\\) is the ratio of explained variance to total variance only if the intercept is included. Overthinking 1.1.2 Simulating SLR 1.1.2.0.1 Expected Values, Variance, and Standard Errors The Gauss–Markov theorem tells us that when estimating the parameters of the simple linear regression model \\(\\beta_{0}\\) and \\(\\beta_{1}\\), the \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) which we derived are the best linear unbiased estimates, or BLUE for short. (The actual conditions for the Gauss–Markov theorem are more relaxed than the SLR model.) In short those assumptions are: SLR.1 Linear population regression function \\(y = \\beta_0 + \\beta_{1} \\times x + u\\) SLR.2 Random sampling of x and y from the population SLR.3 Variation in the sample values: \\(x_{1}, \\dots , x_{n}\\) SLR.4 Zero conditional mean: \\(\\mathbf{E}(u|x) = 0\\) SLR.5 Homeskedasticity: \\(Var(u|x) = \\sigma^2\\) Recall that under SLR.1 - SLR.4 the OLS parameter estimators are unbiased. Under SLR.1 - SLR.4 the OLS parameter estimators have a specific sampling variance. Simulating a model is an important concept. In practice you will almost never have a true model, and you will use data to attempt to recover information about the unknown true model. With simulation, we decide the true model and simulate data from it. Then, we apply a method to the data, in this case least squares. Now, since we know the true model, we can assess how well it did. Simulation also helps to grasp the concepts of estimators, estimates, unbiasedness, the sampling variance of the estimators, and the consequences of violated assumptions. Sample size n &lt;- 200 True parameters b0&lt;- 1 b1 &lt;- 0.5 sigma &lt;- 2 # standard deviation of the error term u x1 &lt;- 5 Determine the distribution of the independent variable yhat1 &lt;- b0 + b1 * x1 # Note that we do not include the error term Plot a Gaussian distribution of the dependent variable based on the parameters curve(dnorm(x, mean = yhat1, sd = sigma), -5, 15, col = &quot;blue&quot;) abline(v = yhat1, col = &quot;blue&quot;, lty = 2) legend(&quot;topright&quot;, legend = c(&quot;f(y|x = 5)&quot;), lty = 1, col = c(&quot;blue&quot;)) This represent the theoretical (true) probability distribution of \\(y\\), given \\(x\\) We can calculate the variance of \\(b_{1}\\) and plot the corresponding density function. \\[\\begin{equation} var(b_2) = \\frac{\\sigma^2}{\\sum{}{}(x_1 - \\bar{x})^2} \\tag{1.6} \\end{equation}\\] Assume that \\(x_{2}\\) represents a second possible predictor of \\(y\\) x2 &lt;- 18 x &lt;- c(rep(x1, n/2), rep(x2, n/2)) xbar &lt;- mean(x) sumxbar &lt;- sum((x-xbar)^2) varb &lt;- (sigma^2)/sumxbar sdb &lt;-sqrt(varb) leftlim &lt;- b1-3*sdb rightlim &lt;- b1+3*sdb curve(dnorm(x, mean = b1, sd = sdb), leftlim, rightlim) abline(v = b1, col = &quot;blue&quot;, lty = 2) Figure 1.5: The theoretical (true) probability density function of b1 Draw sample of size \\(n\\) x &lt;- rnorm(n, 4, sigma) # Another way is to assume that the values for x are fixed and know # x= seq(from = 0, to = 10, length.out = n) u &lt;- rnorm(n, 0, sigma) y &lt;- b0 + b1 * x + u Estimate parameter by OLS olsreg &lt;- lm(y ~x ) simulation.df &lt;- data.frame(x,y) population.df &lt;- data.frame(b0, b1) plot(simulation.df, xlab = &quot;x&quot;, ylab = &quot;y&quot;, # main = &quot;Simulate least squares regression&quot;, pch = 20, cex = 2, col = &quot;grey&quot;) abline(olsreg, lwd = 3, lty = 1, col = &quot;darkorange&quot;) abline(b0, b1, lwd = 3, lty = 2, col = &quot;darkblue&quot;) legend(&quot;topleft&quot;, c(&quot;OLS regression function&quot;, &quot;Population regression function&quot;), lwd =2, lty = 1:2) Figure 1.6: Simulated Sample and OLS Regression Line lable1 &lt;- &quot;OLS regression function&quot; ggplot(simulation.df, aes(x = x, y = y)) + geom_point() + geom_abline(aes(intercept=b0,slope=b1,colour=&quot;Population regression function&quot;), linetype =&quot;dashed&quot;, show.legend = TRUE)+ stat_smooth(aes(colour =&quot;OLS regression function&quot;), method = &quot;lm&quot;,se=FALSE, show.legend =TRUE)+ labs(colour = &quot;Regression functions&quot; # , title = &quot;Simulate least squares regression&quot; ) Figure 1.7: Simulated Sample and OLS Regression Line (gpplot Style) Since the expected values and variances of our estimators are defined over separate random samples from the same population, it makes sense to repeat our simulation exercise over many simulated samples. # Set the random seed set.seed(1234567) # set sample size and number of simulations n&lt;-1000; r&lt;-10000 # set true parameters: betas and sd of u b0&lt;-1.0; b1&lt;-0.5; sigma&lt;-2 # initialize b0hat and b1hat to store results later: b0hat &lt;- numeric(r) b1hat &lt;- numeric(r) # Draw a sample of x, fixed over replications: x &lt;- rnorm(n,4,1) # repeat r times: for(j in 1:r) { # Draw a sample of y: u &lt;- rnorm(n,0,sigma) y &lt;- b0 + b1*x + u # estimate parameters by OLS and store them in the vectors bhat &lt;- coefficients( lm(y~x) ) b0hat[j] &lt;- bhat[&quot;(Intercept)&quot;] b1hat[j] &lt;- bhat[&quot;x&quot;] } # MC estimate of the expected values: mean(b0hat) ## [1] 0.9985388 mean(b1hat) ## [1] 0.5000466 # MC estimate of the variances: var(b0hat) ## [1] 0.0690833 var(b1hat) ## [1] 0.004069063 # Initialize empty plot plot( NULL, xlim=c(0,8), ylim=c(0,6), xlab=&quot;x&quot;, ylab=&quot;y&quot;) # add OLS regression lines for (j in 1:10) abline(b0hat[j],b1hat[j],col=&quot;gray&quot;) # add population regression line abline(b0,b1,lwd=2) # add legend legend(&quot;topleft&quot;,c(&quot;Population&quot;,&quot;OLS regressions&quot;), lwd=c(2,1),col=c(&quot;black&quot;,&quot;gray&quot;)) Figure 1.8: Population and Simulated OLS Regression Lines Even though the loop solution is transparent, let us take a look at a different, more modern approach. # define a function the returns the alpha -- its point estimate, standard error, etc. -- from the OLS x &lt;- rnorm(n,4,1) # NOTE 1: Although a normal distribution is usually defined by its mean and variance, &#39;rnorm()&#39; requires the standard deviation as input for the second moment. # NOTE 2: We use the same values for x in all samples since we draw them outside of the loop. iteration &lt;- function() { u &lt;- rnorm(n,0,sigma) y &lt;- b0 + b1*x + u lm(y~x) %&gt;% broom::tidy() # %&gt;% # dplyr::filter(term == &#39;x&#39;) # One could only extract the slope } # 1000 iterations of the above simulation MC_coef&lt;- map_df(1:1000, ~iteration()) str(MC_coef) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 2000 obs. of 5 variables: ## $ term : chr &quot;(Intercept)&quot; &quot;x&quot; &quot;(Intercept)&quot; &quot;x&quot; ... ## $ estimate : num 1.577 0.372 1.44 0.387 1.355 ... ## $ std.error: num 0.2672 0.0639 0.2623 0.0628 0.2626 ... ## $ statistic: num 5.9 5.82 5.49 6.17 5.16 ... ## $ p.value : num 4.94e-09 7.91e-09 5.13e-08 9.92e-10 2.99e-07 ... Instead of plotting simulated and true parameter regression lines we can take a look at the kernel density of the simulated parameter estimates Figure 1.9 shows the simulated distribution of \\(\\beta_{0}\\) and \\(\\beta_{1}\\) the theoretical one. # plot the results str(MC_coef) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 2000 obs. of 5 variables: ## $ term : chr &quot;(Intercept)&quot; &quot;x&quot; &quot;(Intercept)&quot; &quot;x&quot; ... ## $ estimate : num 1.577 0.372 1.44 0.387 1.355 ... ## $ std.error: num 0.2672 0.0639 0.2623 0.0628 0.2626 ... ## $ statistic: num 5.9 5.82 5.49 6.17 5.16 ... ## $ p.value : num 4.94e-09 7.91e-09 5.13e-08 9.92e-10 2.99e-07 ... MC_coef&lt;- MC_coef %&gt;% mutate(OLScoeff = ifelse(term == &quot;x&quot;, &quot;b1hat&quot;, &quot;b0hat&quot;)) %&gt;% # rename the x to b1hat and (Intercept) to b0hat and create a new column mutate(Simulated = ifelse(term == &quot;x&quot;, &quot;b1&quot;, &quot;b0&quot;)) # %&gt;% ggplot(data= MC_coef, aes(estimate)) + geom_histogram() + geom_vline(data = dplyr::filter(MC_coef, OLScoeff == &quot;b0hat&quot;), aes(xintercept=b0), colour=&quot;pink&quot;) + geom_vline(data = dplyr::filter(MC_coef, OLScoeff == &quot;b1hat&quot;), aes(xintercept=b1), colour=&quot;darkgreen&quot;) + geom_text(data=MC_coef[3,], mapping=aes(x=estimate, y=8, label=paste(&quot;True parameter: &quot;, MC_coef[3,7])), colour = &quot;pink&quot;) + geom_text(data=MC_coef[4,], mapping=aes(x=estimate, y=8, label=paste(&quot;True parameter: &quot;, MC_coef[4,7])), colour = &quot;darkgreen&quot;) + facet_wrap( ~ OLScoeff, scales = &quot;free&quot;) + labs( title = &quot;Histogram Monte Carlo Simulations and True population parameters&quot;) + theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 1.9: Histogram b0 and b1 and true parameter b1_sim &lt;- MC_coef %&gt;% dplyr::filter(Simulated == &quot;b1&quot;) mean(b1_sim$estimate) ## [1] 0.5011414 var(b1_sim$estimate) == (sd(b1_sim$estimate))^2 ## [1] FALSE all.equal(var(b1_sim$estimate) , (sd(b1_sim$estimate))^2) # Floating point arithmetic! ## [1] TRUE ggplot(data= b1_sim, aes(estimate)) + geom_density(aes(fill = Simulated), alpha = 0.2) + # computes and draws the kernel density, which is the smoothed version of the histogram # stat_function(fun = dnorm, args = list(mean = mean(b1_sim$estimate), sd = sd(b1_sim$estimate)), aes(colour = &quot;true&quot;)) + stat_function(fun = dnorm, args = list(mean = 0.5, sd = sd(b1_sim$estimate)), aes(colour = &quot;Population&quot;)) + # labs( # title = &quot;Kernel Density Monte Carlo Simulations vs. True population parameters&quot; # ) + scale_color_discrete(name=&quot;&quot;) Figure 1.10: Kernel Density Monte Carlo Simulations vs. True population parameters of b1 1.1.2.0.2 Violation of SLR.4 To implement a violation of SLR.4 (zero conditional mean) consider a case where in the population \\(u\\) is not mean independent of \\(x\\), for example \\[ \\mathbf{E}(u|x) = \\frac{x-4}{5} \\] # Set the random seed set.seed(1234567) # set sample size and number of simulations n&lt;-1000; r&lt;-10000 # set true parameters: betas and sd of u b0&lt;-1; b1&lt;-0.5; su&lt;-2 # initialize b0hat and b1hat to store results later: b0hat &lt;- numeric(r) b1hat &lt;- numeric(r) # Draw a sample of x, fixed over replications: x &lt;- rnorm(n,4,1) # repeat r times: for(j in 1:r) { # Draw a sample of y: u &lt;- rnorm(n, (x-4)/5, su) # this is where manipulate the assumption of zero conditional mean y &lt;- b0 + b1*x + u # estimate parameters by OLS and store them in the vectors bhat &lt;- coefficients( lm(y~x) ) b0hat[j] &lt;- bhat[&quot;(Intercept)&quot;] b1hat[j] &lt;- bhat[&quot;x&quot;] } OLS coefficients # MC estimate of the expected values: mean(b0hat) ## [1] 0.1985388 mean(b1hat) ## [1] 0.7000466 # MC estimate of the variances: var(b0hat) ## [1] 0.0690833 var(b1hat) ## [1] 0.004069063 The average estimates are far from the population parameters \\(\\beta_0=1\\) and \\(\\beta_1 = 0.5\\)! 1.1.2.0.3 Violation of SLR.5 Homoskedasticity is not required for unbiasedness but for it is a requirement for the theorem of sampling variance. Consider the following heteroskedastic behavior of \\(u\\) given \\(x\\). # Set the random seed set.seed(1234567) # set sample size and number of simulations n&lt;-1000; r&lt;-10000 # set true parameters: betas and sd of u b0&lt;-1; b1&lt;-0.5; su&lt;-2 # initialize b0hat and b1hat to store results later: b0hat &lt;- numeric(r) b1hat &lt;- numeric(r) # Draw a sample of x, fixed over replications: x &lt;- rnorm(n,4,1) # repeat r times: for(j in 1:r) { # Draw a sample of y: varu &lt;- 4/exp(4.5) * exp(x) u &lt;- rnorm(n, 0, sqrt(varu) ) y &lt;- b0 + b1*x + u # estimate parameters by OLS and store them in the vectors lm_heterosced &lt;- lm(y~x) bhat &lt;- coefficients( lm(y~x) ) b0hat[j] &lt;- bhat[&quot;(Intercept)&quot;] b1hat[j] &lt;- bhat[&quot;x&quot;] } summary(lm_heterosced) # just the last sample of the MC-simulation ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23.6742 -0.9033 0.0052 1.0012 9.3411 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.24088 0.27158 4.569 5.51e-06 *** ## x 0.44561 0.06593 6.759 2.37e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.075 on 998 degrees of freedom ## Multiple R-squared: 0.04377, Adjusted R-squared: 0.04281 ## F-statistic: 45.68 on 1 and 998 DF, p-value: 2.367e-11 Plot the residual against the regressor suspected of creating heteroskedasticity, or more generally, the fitted values of the regression. res &lt;- residuals(lm_heterosced) yhat &lt;- fitted(lm_heterosced) par(mfrow = c(1,2)) plot(x, res, ylab = &quot;residuals&quot;) plot(yhat, res, xlab = &quot;fitted values&quot;, ylab = &quot;residuals&quot;) Figure 1.11: Heteroskedasticity in the simulated data # MC estimate of the expected values: mean(b0hat) ## [1] 1.0019 mean(b1hat) ## [1] 0.4992376 # MC estimate of the variances: var(b0hat) ## [1] 0.08967037 var(b1hat) ## [1] 0.007264373 Unbiasedness is provided but sampling variance is incorrect (compared to the results provided above). 1.1.3 Nonlinearities Sometimes the scatter plot diagram or some theoretical considerations suggest a non-linear relationship. The most popular non-linear transformation involve logarithms of the dependent or independent variables and polynomial functions. We will use a new dataset, wage1, for this section. A detailed exploratory analysis of the dataset is left to the reader. data(&quot;wage1&quot;) 1.1.3.1 Predictor variable transformation A common variance stabilizing transformation (VST) is necessary when we see increasing variance in a fitted versus residuals plot. To use the log of an independent variable is to make its distribution closer to the normal distribution. # wage1$logwage &lt;- log(wage1$wage) # one could also create a new variable p1_wagehisto &lt;- ggplot(wage1) + geom_histogram(aes(x = wage), fill = &quot;red&quot;, alpha = 0.6) p2_wagehisto &lt;- ggplot(wage1) + geom_histogram(aes(x = wage), fill = &quot;blue&quot;, alpha = 0.6) + scale_x_continuous(trans=&#39;log2&#39;, &quot;Log Wage&quot;) # instead of creating a new variable with simply define that the x-scale undergoes a logarithmic transformation ggarrange(p1_wagehisto, p2_wagehisto, labels = c(&quot;A&quot;, &quot;B&quot;), ncol = 2, nrow = 1) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 1.12: Histogram of wage and log(wage) A model with a log transformed response: \\[\\begin{equation} log(Y_{i}) = \\beta_{0} + \\beta_{1} \\times x_{i} + \\epsilon_{i} \\end{equation}\\] lm_wage &lt;- lm(wage ~ educ, data = wage1) lm_wage1 &lt;- lm(log(wage) ~ educ, data = wage1) summary(lm_wage) ## ## Call: ## lm(formula = wage ~ educ, data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.3396 -2.1501 -0.9674 1.1921 16.6085 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.90485 0.68497 -1.321 0.187 ## educ 0.54136 0.05325 10.167 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.378 on 524 degrees of freedom ## Multiple R-squared: 0.1648, Adjusted R-squared: 0.1632 ## F-statistic: 103.4 on 1 and 524 DF, p-value: &lt; 2.2e-16 summary(lm_wage1) ## ## Call: ## lm(formula = log(wage) ~ educ, data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.21158 -0.36393 -0.07263 0.29712 1.52339 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.583773 0.097336 5.998 3.74e-09 *** ## educ 0.082744 0.007567 10.935 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4801 on 524 degrees of freedom ## Multiple R-squared: 0.1858, Adjusted R-squared: 0.1843 ## F-statistic: 119.6 on 1 and 524 DF, p-value: &lt; 2.2e-16 Plotting Diagnostics for Linear Models plot(lm_wage) Figure 1.13: Regression diagnostics plot base R - Linear Relationship Figure 1.13: Regression diagnostics plot base R - Linear Relationship Figure 1.13: Regression diagnostics plot base R - Linear Relationship Figure 1.13: Regression diagnostics plot base R - Linear Relationship autoplot(lm_wage, which = 1:6, colour = &#39;dodgerblue3&#39;, smooth.colour = &#39;red&#39;, smooth.linetype = &#39;dashed&#39;, ad.colour = &#39;blue&#39;, label = FALSE, label.size = 3, label.n = 5, label.colour = &#39;blue&#39;, ncol = 3) + theme_bw() Figure 1.14: Regression diagnostics autoplot(ggplot) - Linear Relationship autoplot(lm_wage1, which = 1:6, colour = &#39;dodgerblue3&#39;, smooth.colour = &#39;red&#39;, smooth.linetype = &#39;dashed&#39;, ad.colour = &#39;blue&#39;, label = FALSE, label.size = 3, label.n = 5, label.colour = &#39;blue&#39;, ncol = 3) + theme_bw() Figure 1.15: Regression diagnostics - Non-Linear Relationship p1_nonlinearities &lt;- ggplot(wage1, aes(x = educ, y = wage )) + geom_point() + scale_y_continuous(trans=&#39;log2&#39;, &quot;Log Wage&quot;) + stat_smooth(aes(fill=&quot;Linear Model&quot;),size=1,method = &quot;lm&quot; ,span =0.3, se=F) + guides(fill = guide_legend(&quot;Model Type&quot;)) + theme_bw() p1_nonlinearities Note that if we re-scale the model from a log scale back to the original scale of the data, we now have \\[\\begin{equation} Y_{i} = exp(\\beta_{0} + \\beta_{1} \\times x_{i}) \\times exp(\\epsilon_{i}) \\end{equation}\\] which has errors entering in a multiplicative fashion. log.model.df &lt;- data.frame(x = wage1$educ, y = exp(fitted(lm_wage1))) # This is essentially exp(b0_wage1 + b1_wage1 * wage1$educ) p2_nonlinearities &lt;- ggplot(wage1, aes(x = educ, y = wage)) + geom_point() + geom_line(data = log.model.df, aes(x, y, color = &quot;Log Model&quot;), size = 1, linetype = 2) + guides(color = guide_legend(&quot;Model Type&quot;)) + theme_bw() ggarrange(p1_nonlinearities, p2_nonlinearities, labels = c(&quot;A&quot;, &quot;B&quot;), ncol = 2, nrow = 1) Figure 1.16: Wages by Education - Different transformations A: Plotting the data on the transformed log scale and adding the fitted line, the relationship again appears linear, and the variation about the fitted line looks more constant. B: By plotting the data on the original scale, and adding the fitted regression, we see an exponential relationship. However, this is still a linear model, since the new transformed response, \\(log(Y_{i}\\), is still a linear combination of the predictors. In other words, only \\(\\beta\\) needs to be linear, not the \\(x\\) values. Quadratic Model \\[\\begin{equation} Y_{i} = \\beta_{0} + \\beta_{1} \\times x_{i} \\beta_{2} \\times x^2_{i} + \\epsilon_{i} \\end{equation}\\] New dataset from Wooldrige: Collected from the real estate pages of the Boston Globe during 1990. These are homes that sold in the Boston, MA area. data(&quot;hprice1&quot;, package = &quot;wooldridge&quot;) In R, independent variables involving mathematical operators can be included in regression equation with the function I() We estimate the following regression: \\[\\begin{equation} hprice ~ \\beta_{0} + \\beta_{1} sqrft + u \\end{equation}\\] lm_hprice &lt;- lm(price ~ sqrft, data = hprice1) and a regression model that includes a squared term \\[\\begin{equation} hprice ~ \\beta_{0} + \\beta_{1} sqrft + \\beta_{2} sqrft^2 + u \\end{equation}\\] lm_hprice1 &lt;- lm(price ~ sqrft + I(sqrft^2), data = hprice1) Alternatively use the poly() function. Be careful of the additional argument raw. lm_hprice2 &lt;- lm(price ~ poly(sqrft, degree = 2), data = hprice1) lm_hprice3 &lt;- lm(price ~ poly(sqrft, degree = 2, raw = TRUE), data = hprice1) # if true, use raw and not orthogonal polynomials. unname(coef(lm_hprice1)) ## [1] 1.849453e+02 -1.710855e-02 3.262809e-05 unname(coef(lm_hprice2)) ## [1] 293.5460 754.8517 135.6051 unname(coef(lm_hprice3)) ## [1] 1.849453e+02 -1.710855e-02 3.262809e-05 all.equal(unname(coef(lm_hprice1)), unname(coef(lm_hprice2))) ## [1] &quot;Mean relative difference: 5.401501&quot; all.equal(unname(coef(lm_hprice1)), unname(coef(lm_hprice3))) ## [1] TRUE all.equal(fitted(lm_hprice1), fitted(lm_hprice2)) ## [1] TRUE all.equal(fitted(lm_hprice1), fitted(lm_hprice3)) ## [1] TRUE With the function all.equal() we can test if all elements in two vectors are the “nearly” the same. “Nearly” refers to the case when tolerance values are exceeded. Why are those values not the same depending on the poly() function argument raw = True? In the case of raw = True R uses raw and not orthogonal polynomials. This can have importan implications in the case when multiple polynimoals are used in the regression. The linear regressar \\(sqrft\\) is uncorrelated linearly with the squared explanatory variable \\(sqrft^2\\). However, if we add a cubic term \\(sqrft^3\\), multicollinearty between \\(sqrft^2\\) and \\(sqrft^3\\) might become and issues of the polynomials are NOT orthogonalized. We can also extract the standard error of our estimated coefficents manually. Starting with the variance-covariance matrix of the regression model vcov_lm_hprice2 &lt;- vcov(lm_hprice2) Extracting only the diagonal element of the matrix and taking the square root we obtain the standard errors as reported in summary() function call. sqrt(diag(vcov_lm_hprice2)) ## (Intercept) poly(sqrft, degree = 2)1 poly(sqrft, degree = 2)2 ## 6.638736 62.276866 62.276866 summary(lm_hprice2) ## ## Call: ## lm(formula = price ~ poly(sqrft, degree = 2), data = hprice1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -158.261 -35.158 -7.924 24.262 223.516 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 293.546 6.639 44.217 &lt;2e-16 *** ## poly(sqrft, degree = 2)1 754.852 62.277 12.121 &lt;2e-16 *** ## poly(sqrft, degree = 2)2 135.605 62.277 2.177 0.0322 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 62.28 on 85 degrees of freedom ## Multiple R-squared: 0.6408, Adjusted R-squared: 0.6324 ## F-statistic: 75.83 on 2 and 85 DF, p-value: &lt; 2.2e-16 1.2 Multiple Linear Regression Note A (general) linear model is similar to the simple variant, but with a multivariate \\(x \\epsilon \\!R^{\\rho}\\) and a mean given by a hyperplane in place of a single line. General principles are the same as the simple case Math is more difficult because we need to use matrices Interpretation is more difficult because the \\(\\beta_{j}\\) are effects conditional on the other variables Many would retain the same signs as the simple linear regression, but the magnitudes would be smaller. In some cases, it is possible for the relationship to flip directions when a second (highly correlated) variable is added Dalpiaz (2016). \\[\\begin{equation} y = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\dots + \\beta_{k}x_{k} + u \\tag{1.7} \\end{equation}\\] The next example from Wooldrige relates the college GPA (“cloGPA”) to the high school GPA (“hsGPA”) and achievement test score (“ACT”) for a sample of 141 students. data(&quot;gpa1&quot;, package = &quot;wooldridge&quot;) attach(gpa1) ## The following object is masked from package:robustbase: ## ## alcohol ## The following objects are masked from package:wooldridge: ## ## alcohol, campus ?gpa1 Obtain parameter estimates GPAres &lt;- lm(colGPA ~ hsGPA + ACT, data = gpa1) summary(GPAres) ## ## Call: ## lm(formula = colGPA ~ hsGPA + ACT, data = gpa1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.85442 -0.24666 -0.02614 0.28127 0.85357 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.286328 0.340822 3.774 0.000238 *** ## hsGPA 0.453456 0.095813 4.733 5.42e-06 *** ## ACT 0.009426 0.010777 0.875 0.383297 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3403 on 138 degrees of freedom ## Multiple R-squared: 0.1764, Adjusted R-squared: 0.1645 ## F-statistic: 14.78 on 2 and 138 DF, p-value: 1.526e-06 coef(GPAres)[[1]] ## [1] 1.286328 In the multiple linear regression setting, some of the interpretations of the coefficients change slightly. Here, \\(\\hat\\beta_{0} =\\) 1.2863278 is our estimate for \\(\\beta_{0}\\) when all of the predictors are 0. In this example this makes sense but think of the following example: Your turn Assume the following model: mpg_model = lm(hp ~ wt + cyl, data = mtcars) coef(mpg_model) ## (Intercept) wt cyl ## -51.805567 1.330463 31.387901 How do you interpret the intercept coefficient estimate? A: Here, \\(\\hat\\beta_{0} =\\) -51.8055669 is our estimate for \\(\\beta_{0}\\), the mean gross horsepower for a car that weights 0 pounds and has 0 cylinders. We see our estimate here is negative, which is a physical impossibility. However, this isn’t unexpected, as we shouldn’t expect our model to be accurate for cars which weight 0 pounds and have no cylinders to propel the engine. with (gpa1, { # find min-max seq for grid construction min_hsGPA &lt;- min(gpa1$hsGPA) max_hsGPA &lt;- max(gpa1$hsGPA) min_ACT &lt;- min(gpa1$ACT) max_ACT &lt;- max(gpa1$ACT) # linear regression fit &lt;- lm(colGPA ~ hsGPA + ACT) # predict values on regular xy grid hsGPA.pred &lt;- seq(min_hsGPA, max_hsGPA, length.out = 30) ACT.pred &lt;- seq(min_ACT, max_ACT, length.out = 30) xy &lt;- expand.grid(hsGPA = hsGPA.pred, ACT = ACT.pred) colGPA.pred &lt;- matrix (nrow = 30, ncol = 30, data = predict(fit, newdata = data.frame(xy), interval = &quot;prediction&quot;)) # fitted points for droplines to surface fitpoints &lt;- predict(fit) scatter3D(z = colGPA, x = hsGPA, y = ACT, pch = 18, cex = 2, theta = 20, phi = 20, ticktype = &quot;detailed&quot;, xlab = &quot;hsGPA&quot;, ylab = &quot;ACT&quot;, zlab = &quot;colGPA&quot;, surf = list(x = hsGPA.pred, y = ACT.pred, z = colGPA.pred, facets = NA, fit = fitpoints), main = &quot;colGPA&quot;) }) Figure 1.17: College GPA High School GPA + Achievment test score The data points (\\(x_{i1}\\),\\(x_{i2}\\),\\(y_{i}\\)) now exist in 3-dimensional space, so instead of fitting a line to the data, we will fit a plane. 1.2.1 Ceteris Paribus Interpretation and Omitted Variable bias Consider a regression with two explanatory variables \\[\\begin{equation} \\hat{y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1} + \\hat{\\beta}_{2}x_{2} \\tag{1.8} \\end{equation}\\] # Parameter estimates for full and simple model: beta.hat &lt;- coef( lm(colGPA ~ ACT+hsGPA, data=gpa1)) beta.hat ## (Intercept) ACT hsGPA ## 1.286327767 0.009426012 0.453455885 Now, lets omit one variable in the regression \\[\\begin{equation} \\hat{y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1} \\tag{1.9} \\end{equation}\\] # Relation between regressors: delta.tilde &lt;- coef( lm(hsGPA ~ ACT, data=gpa1) ) delta.tilde ## (Intercept) ACT ## 2.46253658 0.03889675 The parameter \\(\\hat\\beta_1\\) is the estimated effect of increasing \\(x_1\\) by one unit (and NOT keeping \\(x_2\\) fixed). It can be related to \\(\\hat\\beta_1\\) using the formula \\[\\begin{equation} \\hat{y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{1} + \\hat{\\beta}_{2} \\tilde\\delta_{1} \\tag{1.10} \\end{equation}\\] where \\(\\tilde\\delta_{1}\\) is the slope parameters of the linear regression of \\(x_2\\) on \\(x_1\\) \\[\\begin{equation} \\hat{x}_2 = \\tilde\\delta_{0} + \\tilde\\delta_{1} x_{1} \\tag{1.11} \\end{equation}\\] # Omitted variables formula for beta1.tilde: beta.hat[&quot;ACT&quot;] + beta.hat[&quot;hsGPA&quot;]*delta.tilde[&quot;ACT&quot;] ## ACT ## 0.02706397 # Actual regression with hsGPA omitted: summary(lm(colGPA ~ ACT, data=gpa1)) ## ## Call: ## lm(formula = colGPA ~ ACT, data = gpa1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.85251 -0.25251 -0.04426 0.26400 0.89336 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.40298 0.26420 9.095 8.8e-16 *** ## ACT 0.02706 0.01086 2.491 0.0139 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3656 on 139 degrees of freedom ## Multiple R-squared: 0.04275, Adjusted R-squared: 0.03586 ## F-statistic: 6.207 on 1 and 139 DF, p-value: 0.0139 In this example, the indirect effect is actually stronger than the direct effect. ACT predicts colGPA mainly because it is related to hsGPA which in turn is strongly related to colGPA. 1.2.2 Standard errors, Multicollinearity and VIF Multicollinearity means that two or more regressors in a multiple regression model are strongly correlated. If the correlation between two or more regressors is perfect, that is, one regressor can be written as a linear combination of the other(s), we have perfect multicollinearity. While strong multicollinearity in general is unpleasant as it causes the variance of the OLS estimator to be large (we will discuss this in more detail later), the presence of perfect multicollinearity makes it impossible to solve for the OLS estimator, i.e., the model cannot be estimated in the first place. 1.2.2.1 Perfect multicollinearity We will work first with the CAschools data from the AER package to simulate an example of perfect multicollinearity data(&quot;CASchools&quot;, package = &quot;AER&quot;) ?CASchools # define the fraction of English learners CASchools$FracEL &lt;- CASchools$english / 100 # check the correlation between CASchools$FracEL and CASchools$english cor(CASchools$FracEL, CASchools$english) ## [1] 1 # estimate the model mult.mod &lt;- lm(read ~ students + english + FracEL, data = CASchools) # obtain a summary of the model summary(mult.mod) ## ## Call: ## lm(formula = read ~ students + english + FracEL, data = CASchools) ## ## Residuals: ## Min 1Q Median 3Q Max ## -51.077 -9.767 -0.695 9.097 40.005 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.665e+02 9.771e-01 682.054 &lt;2e-16 *** ## students 3.326e-04 1.941e-04 1.714 0.0873 . ## english -7.843e-01 4.153e-02 -18.886 &lt;2e-16 *** ## FracEL NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.53 on 417 degrees of freedom ## Multiple R-squared: 0.4802, Adjusted R-squared: 0.4777 ## F-statistic: 192.6 on 2 and 417 DF, p-value: &lt; 2.2e-16 The row FracEL in the coefficients section of the output consists of NA entries since FracEL was excluded from the model. Another example of perfect multicollinearity is known as the dummy variable trap. This may occur when multiple dummy variables are used as regressors. A common case for this is when dummies are used to sort the data into mutually exclusive categories. For example, suppose we have spatial information that indicates whether a school is located in the North, West, South or East of California. # set seed for reproducibility set.seed(1) # generate artificial data on location CASchools$direction &lt;- sample(c(&quot;West&quot;, &quot;North&quot;, &quot;South&quot;, &quot;East&quot;), 420, replace = T) # estimate the model mult.mod &lt;- lm(read ~ students + english + direction, data = CASchools) # obtain a model summary summary(mult.mod) ## ## Call: ## lm(formula = read ~ students + english + direction, data = CASchools) ## ## Residuals: ## Min 1Q Median 3Q Max ## -50.518 -9.690 -1.146 8.926 39.698 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.659e+02 1.547e+00 430.575 &lt;2e-16 *** ## students 3.177e-04 1.945e-04 1.634 0.103 ## english -7.844e-01 4.154e-02 -18.882 &lt;2e-16 *** ## directionNorth 9.185e-01 1.934e+00 0.475 0.635 ## directionSouth -1.119e+00 2.081e+00 -0.537 0.591 ## directionWest 2.436e+00 2.057e+00 1.184 0.237 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.53 on 414 degrees of freedom ## Multiple R-squared: 0.484, Adjusted R-squared: 0.4778 ## F-statistic: 77.67 on 5 and 414 DF, p-value: &lt; 2.2e-16 Notice that R solves the problem on its own by generating and including the dummies directionNorth, directionSouth and directionWest but omitting directionEast. Of course, the omission of every other dummy instead would achieve the same. Another solution would be to exclude the constant and to include all dummies instead. A last example considers the case where a perfect linear relationship arises from redundant regressors. Suppose we have a regressor Spanish speakers, spanish, , the percentage of English speakers in the school where \\[\\begin{equation} spanish = 100 - english \\end{equation}\\] and both spanish and english are included in a regression model. # Percentage of english speakers CASchools$spanish &lt;- 100 - CASchools$english # estimate the model mult.mod &lt;- lm(read ~ students + english + spanish, data = CASchools) # obtain a model summary summary(mult.mod) ## ## Call: ## lm(formula = read ~ students + english + spanish, data = CASchools) ## ## Residuals: ## Min 1Q Median 3Q Max ## -51.077 -9.767 -0.695 9.097 40.005 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.665e+02 9.771e-01 682.054 &lt;2e-16 *** ## students 3.326e-04 1.941e-04 1.714 0.0873 . ## english -7.843e-01 4.153e-02 -18.886 &lt;2e-16 *** ## spanish NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.53 on 417 degrees of freedom ## Multiple R-squared: 0.4802, Adjusted R-squared: 0.4777 ## F-statistic: 192.6 on 2 and 417 DF, p-value: &lt; 2.2e-16 Once more, lm() refuses to estimate the full model using OLS and excludes spanish. 1.2.2.2 Imperfect multicollinearity As opposed to perfect multicollinearity, imperfect multicollinearity is — to a certain extent — less of a problem. In fact, imperfect multicollinearity is the reason why we are interested in estimating multiple regression models in the first place: the OLS estimator allows us to isolate influences of correlated regressors on the dependent variable. If it was not for these dependencies, there would not be a reason to resort to a multiple regression approach and we could simply work with a single-regressor model. However, this is rarely the case in applications. We already know that ignoring dependencies among regressors which influence the outcome variable has an adverse effect on estimation results. Simulation study: imperfect multicollinearity # set number of observations n &lt;- 50 # initialize vectors of coefficients coefs1 &lt;- cbind(&quot;hat_beta_1&quot; = numeric(10000), &quot;hat_beta_2&quot; = numeric(10000)) coefs2 &lt;- coefs1 # set seed set.seed(1) # loop sampling and estimation for (i in 1:1000) { # for cov(X_1,X_2) = 0.25 X &lt;- rmvnorm(n, c(50, 100), sigma = cbind(c(10, 2.5), c(2.5, 10))) # function from the mvtnorm package u &lt;- rnorm(n, sd = 5) Y &lt;- 5 + 2.5 * X[, 1] + 3 * X[, 2] + u coefs1[i, ] &lt;- lm(Y ~ X[, 1] + X[, 2])$coefficients[-1] # for cov(X_1,X_2) = 0.85 X &lt;- rmvnorm(n, c(50, 100), sigma = cbind(c(10, 8.5), c(8.5, 10))) Y &lt;- 5 + 2.5 * X[, 1] + 3 * X[, 2] + u imperf_multicol &lt;- lm(Y ~ X[, 1] + X[, 2]) coefs2[i, ] &lt;- lm(Y ~ X[, 1] + X[, 2])$coefficients[-1] } # obtain variance estimates diag(var(coefs1)) ## hat_beta_1 hat_beta_2 ## 0.5698281 0.8163287 diag(var(coefs2)) ## hat_beta_1 hat_beta_2 ## 0.5834243 0.8402187 We are interested in the variances which are the diagonal elements. We see that due to the high collinearity, the variances of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) and have increased, meaning it is more difficult to precisely estimate the true coefficients. The variance inflation factor, VIF, accounts for (imperfect) multicollinearity.If \\(x_t\\) is highly related to the other regressors, \\(R^2_j\\) and therefore also \\(VIF_j\\) and the variance of \\(\\hat\\beta_j\\) are large. \\[\\begin{equation} \\frac{1}{1-R^2_j} \\tag{1.12} \\end{equation}\\] GPAres &lt;- lm(colGPA ~ hsGPA + ACT, data = gpa1) SER&lt;-summary(GPAres)$sigma # regressing hsGPA on ACT for calculation of R2 &amp; VIF ( R2.hsGPA &lt;- summary( lm(hsGPA~ACT, data=gpa1) )$r.squared ) ## [1] 0.1195815 ( VIF.hsGPA &lt;- 1/(1-R2.hsGPA) ) ## [1] 1.135823 The car package implements the command vif() for each regressor vif(GPAres) ## hsGPA ACT ## 1.135823 1.135823 vif(imperf_multicol) # from the simulated data ## X[, 1] X[, 2] ## 4.932864 4.932864 1.2.3 Reporting Regression Results As we start moving towards the comparing different regression models this section provides a discussion on how to report regression reports in R. Depending on your script (R scripts, R Markdown, bookdown) and what your desired output format is (LaTeX, word, html) the exact approach might differ. There are multiple packages to format regression or table output, most notably stargazer3, huxtable, Hmisc and xtable. One can also tidy the the regression output as well as tables with broom or summarytool. The wrapper knitr::kable() is a support function that renders the table in an R Markdown in a pretty way. 1.2.3.1 Table knitr::kable( head(gpa1[,1:8], 10), booktabs = TRUE, caption = &quot;A table of the first eight columns and ten rows of the gpa1 data.&quot; ) Table 1.2: A table of the first eight columns and ten rows of the gpa1 data. age soph junior senior senior5 male campus business 21 0 0 1 0 0 0 1 21 0 0 1 0 0 0 1 20 0 1 0 0 0 0 1 19 1 0 0 0 1 1 1 20 0 1 0 0 0 0 1 20 0 0 1 0 1 1 1 22 0 0 0 1 0 0 1 22 0 0 0 1 0 0 0 22 0 0 0 1 0 0 0 19 1 0 0 0 0 0 1 knitr::kable( descr(gpa1[,1:3], stats = c(&quot;mean&quot;, &quot;sd&quot;, &quot;min&quot;, &quot;med&quot;, &quot;max&quot;), transpose = TRUE, omit.headings = TRUE, style = &quot;rmarkdown&quot;) ) Mean Std.Dev Min Median Max age 20.8865248 1.2710637 19 21 30 soph 0.0212766 0.1448194 0 0 1 junior 0.3829787 0.4878462 0 0 1 model1 &lt;- lm(colGPA ~ hsGPA , data = gpa1) model2 &lt;- lm(colGPA ~ hsGPA + ACT, data = gpa1) model3 &lt;- lm(colGPA ~ hsGPA + ACT + age, data = gpa1) invisible(stargazer( list(model1, model2, model3) ,keep.stat = c(&quot;n&quot;, &quot;rsq&quot;), type = &quot;latex&quot;, header = FALSE))# to have number of observations and R^2 reported stargazer( list(model1, model2, model3) ,keep.stat = c(&quot;n&quot;, &quot;rsq&quot;), type = &quot;html&quot;, header = FALSE) # to have number of observations and R^2 reported Dependent variable: colGPA (1) (2) (3) hsGPA 0.482*** 0.453*** 0.482*** (0.090) (0.096) (0.099) ACT 0.009 0.009 (0.011) (0.011) age 0.027 (0.023) Constant 1.415*** 1.286*** 0.618 (0.307) (0.341) (0.663) Observations 141 141 141 R2 0.172 0.176 0.185 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Including the knitr::kable() wrapper 1.2.4 Model Formulae 1.2.4.1 Arithmetic operations within a formula A model relating to birth weight to cigarette smoking of the mother during pregnancy and the family income. data(&quot;bwght&quot;) attach(bwght) ## The following object is masked _by_ .GlobalEnv: ## ## bwght ## The following object is masked from gpa1: ## ## male ## The following object is masked from package:wooldridge: ## ## bwght lm1 &lt;- lm(bwght ~ cigs + faminc, data = bwght) # Weights in pounds, direct way lm2 &lt;- lm(I(bwght/16) ~ cigs + faminc, data = bwght) # Packs of cigarettes lm3 &lt;- lm(bwght ~ I(cigs/20) + faminc, data = bwght) See table 1.3. huxreg(lm1, lm2, lm3) %&gt;% set_caption(&#39;(#tab:regressiontable) Regression table&#39;) # #tab:foo allows to reference to a table directly in a dynamic document. Table 1.3: Regression table (1) (2) (3) (Intercept) 116.974 *** 7.311 *** 116.974 *** (1.049)&nbsp;&nbsp;&nbsp; (0.066)&nbsp;&nbsp;&nbsp; (1.049)&nbsp;&nbsp;&nbsp; cigs -0.463 *** -0.029 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (0.092)&nbsp;&nbsp;&nbsp; (0.006)&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; faminc 0.093 **&nbsp; 0.006 **&nbsp; 0.093 **&nbsp; (0.029)&nbsp;&nbsp;&nbsp; (0.002)&nbsp;&nbsp;&nbsp; (0.029)&nbsp;&nbsp;&nbsp; I(cigs/20) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -9.268 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (1.832)&nbsp;&nbsp;&nbsp; N 1388&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1388&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1388&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R2 0.030&nbsp;&nbsp;&nbsp;&nbsp; 0.030&nbsp;&nbsp;&nbsp;&nbsp; 0.030&nbsp;&nbsp;&nbsp;&nbsp; logLik -6130.414&nbsp;&nbsp;&nbsp;&nbsp; -2282.061&nbsp;&nbsp;&nbsp;&nbsp; -6130.414&nbsp;&nbsp;&nbsp;&nbsp; AIC 12268.828&nbsp;&nbsp;&nbsp;&nbsp; 4572.122&nbsp;&nbsp;&nbsp;&nbsp; 12268.828&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. invisible(stargazer( # invisible supresses additional output such as the package author name when the regression table is compiled list(lm1, lm2, lm3) ,keep.stat = c(&quot;n&quot;, &quot;rsq&quot;), type = &quot;latex&quot;, header = FALSE))# to have number of observations and R^2 reported} Dividing the dependent variable by 16 changes all coefficients by the same factor \\(\\frac{1}{16}\\) and dividing the regressor by 20 changes its coefficients by the factor 20. Other statistics like \\(R^2\\) are unaffected. 1.2.4.2 Standardization: Beta coefficients The standardized dependent variable \\(y\\) and regressor \\(x_1\\) are \\[\\begin{equation} z_y=\\frac{y-\\bar{y}}{sd(y)} \\end{equation}\\] and \\[\\begin{equation} z_{x1}=\\frac{x_{1}-\\bar{x}_{x1}}{sd(x_{1})} \\end{equation}\\] They measure by how many standard deviations \\(y\\) changes as the respective independent variable increases by one standard deviation. The model does not include a constant because all averages are removed in the standardization. data(hprice2) lm(scale(price)~0 + scale(crime) + scale(rooms) + scale(dist) + scale(stratio), data = hprice2) ## ## Call: ## lm(formula = scale(price) ~ 0 + scale(crime) + scale(rooms) + ## scale(dist) + scale(stratio), data = hprice2) ## ## Coefficients: ## scale(crime) scale(rooms) scale(dist) scale(stratio) ## -0.191397 0.565694 0.003809 -0.246953 1.2.4.3 Logarithms, Quadratics and Polynomials The model for house prices as in Wooldrige: \\[\\begin{equation} log(price) = \\beta_0 + \\beta_1 log(nox) + \\beta_2 log(dist) + \\beta_3 rooms + \\beta_4 rooms^{2} + \\beta_5 stratio + u \\end{equation}\\] lm_hprice2 &lt;- lm(log(price)~ log(nox) + log(dist) + rooms + I(rooms^2) + stratio, data = hprice2) summary(lm_hprice2) ## ## Call: ## lm(formula = log(price) ~ log(nox) + log(dist) + rooms + I(rooms^2) + ## stratio, data = hprice2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.04285 -0.12774 0.02038 0.12650 1.25272 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.385477 0.566473 23.630 &lt; 2e-16 *** ## log(nox) -0.901682 0.114687 -7.862 2.34e-14 *** ## log(dist) -0.086781 0.043281 -2.005 0.04549 * ## rooms -0.545113 0.165454 -3.295 0.00106 ** ## I(rooms^2) 0.062261 0.012805 4.862 1.56e-06 *** ## stratio -0.047590 0.005854 -8.129 3.42e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2592 on 500 degrees of freedom ## Multiple R-squared: 0.6028, Adjusted R-squared: 0.5988 ## F-statistic: 151.8 on 5 and 500 DF, p-value: &lt; 2.2e-16 The quadratic term of rooms significantly positive coefficient \\(\\hat\\beta_4\\) implying that the semi-elasticity increases with more rooms The negative coefficient for rooms indicates that for small number of rooms the price decreases and the positive coefficient for \\(rooms^2\\) implies that for “large” value of rooms the price increases The number of rooms implying the smallest price can be found as \\[\\begin{equation} rooms^{\\star} = \\frac{-\\beta_3}{2\\beta_4} \\approx 4.4 \\end{equation}\\] beta3 &lt;- lm_hprice2$coefficients[[4]] beta4 &lt;- lm_hprice2$coefficients[[5]] -beta3 / (2 * beta4) ## [1] 4.37763 1.2.4.4 Interaction terms Consider the following model, \\[\\begin{equation} Y = {\\beta}_{0} + {\\beta}_{1}x_{1} + {\\beta}_{2}x_{2} + {\\beta}_{3}x_{1}x_{2} + u \\tag{1.13} \\end{equation}\\] where \\(x_1\\), \\(x_2\\), and \\(Y\\) are the same as before, but we have added a new interaction term \\(x_1x_2\\) which multiplies \\(x_1\\) and \\(x_2\\), so we also have an additional \\(\\beta\\) parameter \\(\\beta_3\\). This model essentially creates two slopes and two intercepts, \\(\\beta_2\\) being the difference in intercepts and \\(\\beta_3\\) being the difference in slopes. Recall that R reads x1 times x2 as \\(y \\sim x_1+x_2+x_1x_2\\) and x1:x2 as \\(y \\sim x_1x_2\\). data(attend) 1.2.5 MLR Prediction data(gpa2) # Estimate model with interaction effect: myres&lt;-lm(stndfnl~atndrte*priGPA+ACT+I(priGPA^2)+I(ACT^2), data=attend) # Estimate for partial effect at priGPA=2.59: b &lt;- coef(myres) b[&quot;atndrte&quot;] + 2.59*b[&quot;atndrte:priGPA&quot;] ## atndrte ## 0.007754572 # Test partial effect for priGPA=2.59: Hnull &lt;- c(&quot;atndrte+2.59*atndrte:priGPA&quot;) linHyp &lt;- linearHypothesis(myres,Hnull) # broom::tidy(linHyp) # Regress and report coefficients reg &lt;- lm(colgpa~sat+hsperc+hsize+I(hsize^2),data=gpa2) reg ## ## Call: ## lm(formula = colgpa ~ sat + hsperc + hsize + I(hsize^2), data = gpa2) ## ## Coefficients: ## (Intercept) sat hsperc hsize I(hsize^2) ## 1.492652 0.001492 -0.013856 -0.060881 0.005460 # Generate data set containing the regressor values for predictions cvalues &lt;- data.frame(sat=1200, hsperc=30, hsize=5) # Point estimate of prediction predict(reg, cvalues) ## 1 ## 2.700075 # Point estimate and 95% confidence interval predict(reg, cvalues, interval = &quot;confidence&quot;) ## fit lwr upr ## 1 2.700075 2.661104 2.739047 # Define three sets of regressor variables cvalues &lt;- data.frame(sat=c(1200,900,1400), hsperc=c(30,20,5), hsize=c(5,3,1)) # Point estimates and 99% confidence intervals for these predict(reg, cvalues, interval = &quot;confidence&quot;, level=0.99) ## fit lwr upr ## 1 2.700075 2.648850 2.751301 ## 2 2.425282 2.388540 2.462025 ## 3 3.457448 3.385572 3.529325 1.3 MLR Analysis with Qualitative Regressors 1.3.1 Dummy variabes data(wage1) lm1_wage1 &lt;- lm(wage ~ female+educ+exper+tenure, data=wage1) summary(lm1_wage1) ## ## Call: ## lm(formula = wage ~ female + educ + exper + tenure, data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.7675 -1.8080 -0.4229 1.0467 14.0075 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.56794 0.72455 -2.164 0.0309 * ## female -1.81085 0.26483 -6.838 2.26e-11 *** ## educ 0.57150 0.04934 11.584 &lt; 2e-16 *** ## exper 0.02540 0.01157 2.195 0.0286 * ## tenure 0.14101 0.02116 6.663 6.83e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.958 on 521 degrees of freedom ## Multiple R-squared: 0.3635, Adjusted R-squared: 0.3587 ## F-statistic: 74.4 on 4 and 521 DF, p-value: &lt; 2.2e-16 On average a women makes $ 2 per less than a man with the same education, experience, and tenure. lm2_wage1 &lt;- lm(log(wage)~married*female+educ+exper+I(exper^2)+tenure+I(tenure^2), data=wage1) summary(lm2_wage1) ## ## Call: ## lm(formula = log(wage) ~ married * female + educ + exper + I(exper^2) + ## tenure + I(tenure^2), data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.89697 -0.24060 -0.02689 0.23144 1.09197 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.3213781 0.1000090 3.213 0.001393 ** ## married 0.2126757 0.0553572 3.842 0.000137 *** ## female -0.1103502 0.0557421 -1.980 0.048272 * ## educ 0.0789103 0.0066945 11.787 &lt; 2e-16 *** ## exper 0.0268006 0.0052428 5.112 4.50e-07 *** ## I(exper^2) -0.0005352 0.0001104 -4.847 1.66e-06 *** ## tenure 0.0290875 0.0067620 4.302 2.03e-05 *** ## I(tenure^2) -0.0005331 0.0002312 -2.306 0.021531 * ## married:female -0.3005931 0.0717669 -4.188 3.30e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3933 on 517 degrees of freedom ## Multiple R-squared: 0.4609, Adjusted R-squared: 0.4525 ## F-statistic: 55.25 on 8 and 517 DF, p-value: &lt; 2.2e-16 Your turn What is the reference group in this model? Ceteris paribus, how much more wage do single males make relative to the reference group? Ceteris paribus, how much more wage do single females make relative to the reference group? Ceteris paribus, how much less do married females make than single females? Do the results make sense economically. What socio-economic factors could explain the results? df_lm2_wage1 &lt;- tidy(lm2_wage1) # Singe male marriedmale &lt;- df_lm2_wage1 %&gt;% dplyr::filter(term == &quot;married&quot;) %&gt;% dplyr::select(estimate) %&gt;% pull() # pull out the single coefficient value of the dataframe # Single female singlefemale &lt;- df_lm2_wage1 %&gt;% dplyr::filter(term == &quot;female&quot;) %&gt;% dplyr::select(estimate) %&gt;% pull() # pull out the single coefficient value of the dataframe marriedfemale &lt;- df_lm2_wage1 %&gt;% dplyr::filter(term == &quot;married:female&quot;) %&gt;% dplyr::select(estimate) %&gt;% pull() # pull out the single coefficient value of the dataframe married&lt;- df_lm2_wage1 %&gt;% dplyr::filter(term == &quot;married&quot;) %&gt;% # dplyr::select(estimate) %&gt;% pull() # pull out the single coefficient value of the dataframe A: Reference group: single and male Cp. married males make 21.3% (percent(marriedmale)) more than single males. Cp. a single female makes -11.0% (percent(singlefemale)) less than the reference group. Married females make 8.79% (percent(abs(marriedfemale) - abs(married))) less than single females. There seems to be a marriage premium for men but for women the marriage premium is negative. 1.3.2 Logical variables # replace &quot;female&quot; with logical variable wage1$female &lt;- as.logical(wage1$female) table(wage1$female) ## ## FALSE TRUE ## 274 252 # regression with logical variable lm(wage ~ female+educ+exper+tenure, data=wage1) ## ## Call: ## lm(formula = wage ~ female + educ + exper + tenure, data = wage1) ## ## Coefficients: ## (Intercept) femaleTRUE educ exper tenure ## -1.5679 -1.8109 0.5715 0.0254 0.1410 1.3.3 Factor variables As discussed in the R introduction, categorical variables encoded as factors are special animals in R. They are immensely useful in a regression when you have a categorical variable with many levels (e.g. “Very Bad”, “Bad”, “Good”, “Very Good”) but can create a set of subtle issues. Here, we discuss the base R way and the more robust tidyverse way of dealing with factors in the area of regression modelling. Factor variables can be directly added to the list of regressors. R is clever enough to implicitly add \\(g-1\\) dummy variables if the factor has \\(g\\) outcomes. data(CPS1985,package=&quot;AER&quot;) str(CPS1985) ## &#39;data.frame&#39;: 534 obs. of 11 variables: ## $ wage : num 5.1 4.95 6.67 4 7.5 ... ## $ education : num 8 9 12 12 12 13 10 12 16 12 ... ## $ experience: num 21 42 1 4 17 9 27 9 11 9 ... ## $ age : num 35 57 19 22 35 28 43 27 33 27 ... ## $ ethnicity : Factor w/ 3 levels &quot;cauc&quot;,&quot;hispanic&quot;,..: 2 1 1 1 1 1 1 1 1 1 ... ## $ region : Factor w/ 2 levels &quot;south&quot;,&quot;other&quot;: 2 2 2 2 2 2 1 2 2 2 ... ## $ gender : Factor w/ 2 levels &quot;male&quot;,&quot;female&quot;: 2 2 1 1 1 1 1 1 1 1 ... ## $ occupation: Factor w/ 6 levels &quot;worker&quot;,&quot;technical&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ sector : Factor w/ 3 levels &quot;manufacturing&quot;,..: 1 1 1 3 3 3 3 3 1 3 ... ## $ union : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 2 1 1 1 1 ... ## $ married : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 1 1 2 1 1 1 2 1 ... # Table of categories and frequencies for two factor variables: table(CPS1985$gender) ## ## male female ## 289 245 table(CPS1985$occupation) ## ## worker technical services office sales management ## 156 105 83 97 38 55 levels(CPS1985$occupation) ## [1] &quot;worker&quot; &quot;technical&quot; &quot;services&quot; &quot;office&quot; &quot;sales&quot; ## [6] &quot;management&quot; levels(CPS1985$gender) ## [1] &quot;male&quot; &quot;female&quot; # Directly using factor variables in regression formula: lm(log(wage) ~ education+experience+gender+occupation, data=CPS1985) ## ## Call: ## lm(formula = log(wage) ~ education + experience + gender + occupation, ## data = CPS1985) ## ## Coefficients: ## (Intercept) education experience ## 0.97629 0.07586 0.01188 ## genderfemale occupationtechnical occupationservices ## -0.22385 0.14246 -0.21004 ## occupationoffice occupationsales occupationmanagement ## -0.05477 -0.20757 0.15254 # Fragile method (base R) # Manually redefine the reference category: CPS1985$gender &lt;- relevel(CPS1985$gender,&quot;female&quot;) CPS1985$occupation &lt;- relevel(CPS1985$occupation,&quot;management&quot;) # Rerun regression: lm(log(wage) ~ education+experience+gender+occupation, data=CPS1985) ## ## Call: ## lm(formula = log(wage) ~ education + experience + gender + occupation, ## data = CPS1985) ## ## Coefficients: ## (Intercept) education experience ## 0.90498 0.07586 0.01188 ## gendermale occupationworker occupationtechnical ## 0.22385 -0.15254 -0.01009 ## occupationservices occupationoffice occupationsales ## -0.36259 -0.20731 -0.36011 # Robust method (tidyverse) # Manually redefine the reference category (back to default): CPS1985 &lt;- CPS1985 %&gt;% mutate(gender = fct_relevel(gender, &quot;female&quot;)) %&gt;% mutate(occupation = fct_relevel(occupation, &quot;worker&quot;)) lm(log(wage) ~ education+experience+gender+occupation, data=CPS1985) ## ## Call: ## lm(formula = log(wage) ~ education + experience + gender + occupation, ## data = CPS1985) ## ## Coefficients: ## (Intercept) education experience ## 0.75244 0.07586 0.01188 ## gendermale occupationmanagement occupationtechnical ## 0.22385 0.15254 0.14246 ## occupationservices occupationoffice occupationsales ## -0.21004 -0.05477 -0.20757 1.3.3.1 Breaking a numeric variable into categories data(lawsch85) str(lawsch85$rank) ## int [1:156] 128 104 34 49 95 98 124 157 145 91 ... # Define cut points for the rank cutpts &lt;- c(0,10,25,40,60,100,175) # Create factor variable containing ranges for the rank lawsch85$rankcat &lt;- cut(lawsch85$rank, cutpts) # Display frequencies table(lawsch85$rankcat) ## ## (0,10] (10,25] (25,40] (40,60] (60,100] (100,175] ## 10 16 13 18 37 62 # Choose reference category lawsch85$rankcat &lt;- relevel(lawsch85$rankcat,&quot;(100,175]&quot;) # Run regression res &lt;- lm(log(salary)~rankcat+LSAT+GPA+log(libvol)+log(cost), data=lawsch85) We can perform an Anova test on the regression object # ANOVA table car::Anova(res) Sum Sq Df F value Pr(&gt;F) 1.87&nbsp;&nbsp;&nbsp;&nbsp; 5 51&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.17e-28 0.0253&nbsp;&nbsp; 1 3.45&nbsp;&nbsp;&nbsp; 0.0655&nbsp;&nbsp; 0.000251 1 0.0342&nbsp; 0.854&nbsp;&nbsp;&nbsp; 0.0143&nbsp;&nbsp; 1 1.95&nbsp;&nbsp;&nbsp; 0.165&nbsp;&nbsp;&nbsp; 8.21e-06 1 0.00112 0.973&nbsp;&nbsp;&nbsp; 0.924&nbsp;&nbsp;&nbsp; 126 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The regression results imply that graduates from the top 100 schools collect a starting salary which is around 70% higher than those of the schools below rank 100. This approximation is inaccurate with these large numbers and the coefficient of 0.7 actually implies a difference of ex(0.7-1) = 1.103 or 101.3%. # Robust method (tidyverse) # Manually redefine the reference category (back to default): CPS1985 &lt;- CPS1985 %&gt;% mutate(gender = fct_relevel(gender, &quot;female&quot;)) %&gt;% mutate(occupation = fct_relevel(occupation, &quot;worker&quot;)) lm(log(wage) ~ education+experience+gender+occupation, data=CPS1985) ## ## Call: ## lm(formula = log(wage) ~ education + experience + gender + occupation, ## data = CPS1985) ## ## Coefficients: ## (Intercept) education experience ## 0.75244 0.07586 0.01188 ## gendermale occupationmanagement occupationtechnical ## 0.22385 0.15254 0.14246 ## occupationservices occupationoffice occupationsales ## -0.21004 -0.05477 -0.20757 1.3.3.2 Breaking a numeric variable into categories data(lawsch85) str(lawsch85$rank) ## int [1:156] 128 104 34 49 95 98 124 157 145 91 ... # Define cut points for the rank cutpts &lt;- c(0,10,25,40,60,100,175) # Create factor variable containing ranges for the rank lawsch85$rankcat &lt;- cut(lawsch85$rank, cutpts) # Display frequencies table(lawsch85$rankcat) ## ## (0,10] (10,25] (25,40] (40,60] (60,100] (100,175] ## 10 16 13 18 37 62 # Choose reference category lawsch85$rankcat &lt;- relevel(lawsch85$rankcat,&quot;(100,175]&quot;) # Run regression (res &lt;- lm(log(salary)~rankcat+LSAT+GPA+log(libvol)+log(cost), data=lawsch85)) ## ## Call: ## lm(formula = log(salary) ~ rankcat + LSAT + GPA + log(libvol) + ## log(cost), data = lawsch85) ## ## Coefficients: ## (Intercept) rankcat(0,10] rankcat(10,25] rankcat(25,40] ## 9.1652952 0.6995659 0.5935434 0.3750763 ## rankcat(40,60] rankcat(60,100] LSAT GPA ## 0.2628191 0.1315950 0.0056908 0.0137255 ## log(libvol) log(cost) ## 0.0363619 0.0008412 # ANOVA table car::Anova(res) Sum Sq Df F value Pr(&gt;F) 1.87&nbsp;&nbsp;&nbsp;&nbsp; 5 51&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.17e-28 0.0253&nbsp;&nbsp; 1 3.45&nbsp;&nbsp;&nbsp; 0.0655&nbsp;&nbsp; 0.000251 1 0.0342&nbsp; 0.854&nbsp;&nbsp;&nbsp; 0.0143&nbsp;&nbsp; 1 1.95&nbsp;&nbsp;&nbsp; 0.165&nbsp;&nbsp;&nbsp; 8.21e-06 1 0.00112 0.973&nbsp;&nbsp;&nbsp; 0.924&nbsp;&nbsp;&nbsp; 126 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The regression results imply that graduates from the top 100 schools collect a starting salary which is around 70% higher than those of the schools below rank 100. This approximation is inaccurate with these large numbers and the coefficient of 0.7 actually implies a difference of ex(0.7-1) = 1.103 or 101.3%. 1.3.4 Interactions and differences in regression functions across groups Dummy variables and factor variables can be interacted just like any other variable Use the subset option of lm() to directly define the estimation sample The dummy variable female is interacted with all other regressor The F test for all interaction effects is performed using the function linearHypothesis() from the car package data(gpa3) # Model with full interactions with female dummy (only for spring data) reg&lt;-lm(cumgpa~female*(sat+hsperc+tothrs), data=gpa3, subset=(spring==1)) summary(reg) ## ## Call: ## lm(formula = cumgpa ~ female * (sat + hsperc + tothrs), data = gpa3, ## subset = (spring == 1)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.51370 -0.28645 -0.02306 0.27555 1.24760 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.4808117 0.2073336 7.142 5.17e-12 *** ## female -0.3534862 0.4105293 -0.861 0.38979 ## sat 0.0010516 0.0001811 5.807 1.40e-08 *** ## hsperc -0.0084516 0.0013704 -6.167 1.88e-09 *** ## tothrs 0.0023441 0.0008624 2.718 0.00688 ** ## female:sat 0.0007506 0.0003852 1.949 0.05211 . ## female:hsperc -0.0005498 0.0031617 -0.174 0.86206 ## female:tothrs -0.0001158 0.0016277 -0.071 0.94331 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4678 on 358 degrees of freedom ## Multiple R-squared: 0.4059, Adjusted R-squared: 0.3943 ## F-statistic: 34.95 on 7 and 358 DF, p-value: &lt; 2.2e-16 # F-Test from package &quot;car&quot;. H0: the interaction coefficients are zero # matchCoefs(...) selects all coeffs with names containing &quot;female&quot; linearHypothesis(reg, matchCoefs(reg, &quot;female&quot;)) Res.Df RSS Df Sum of Sq F Pr(&gt;F) 362 85.5 &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 358 78.4 4 7.16 8.18 2.54e-06 As the p-value is much less than 0.05, we reject the null that the interaction with female dummy is statistically insignificant. 1.3.4.1 Visualizing coefficients treg &lt;- tidy(reg, conf.int = TRUE) ggplot(treg, aes(estimate, term, color = term)) + geom_point() + geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) + geom_vline(xintercept = 0, color = &quot;grey&quot;) Figure 1.18: Coefficient plots To recap, the general form in which we specify regression models in R: ## response ~ terms ## ## y ~ age + sex # age + sex main effects ## y ~ age + sex + age:sex # add second-order interaction ## y ~ age*sex # second-order interaction + ## # all main effects ## y ~ (age + sex + pressure)^2 ## # age+sex+pressure+age:sex+age:pressure... ## y ~ (age + sex + pressure)^2 - sex:pressure ## # all main effects and all 2nd order ## # interactions except sex:pressure ## y ~ (age + race)*sex # age+race+sex+age:sex+race:sex ## y ~ treatment*(age*race + age*sex) # no interact. with race,sex ## sqrt(y) ~ sex*sqrt(age) + race ## # functions, with dummy variables generated if ## # race is an R factor (classification) variable ## y ~ sex + poly(age,2) # poly generates orthogonal polynomials ## race.sex &lt;- interaction(race,sex) ## y ~ age + race.sex # for when you want dummy variables for ## # all combinations of the factors 1.4 Heteroskedasticity The homoskedasticity assumptions SLR.5 and MLR.5 require that the variance of the error term is unrelated to the regressors, i.e. \\[\\begin{equation} Var(u|x_1, \\dots , x_n) = \\sigma^2 \\end{equation}\\] Unbiasedness and consistency do not depend on this assumption, but the sampling distribution does. If homoskedasticity is violated, the standard errors are invalid and all inferences from \\(t\\), \\(F\\), and other tests based on them are unreliable. There are various ways of dealing with heteroskedasticity in R. The car package provides linear hypothesis. For high-dimensional fixed effects the lfe package is a good alternative. It also allows to specify clusters as part of the formula. A good balance between functionality and ease of use is provided by the sandwich package Zeileis et al. (2017). 1.4.1 Spotting Heteroskedasticity in Scatter Plots data(&quot;food&quot;,package=&quot;PoEdata&quot;) mod1 &lt;- lm(food_exp~income, data=food) plot(food$income,food$food_exp, type=&quot;p&quot;, xlab=&quot;income&quot;, ylab=&quot;food expenditure&quot;) abline(mod1) Figure 1.19: Heteroskedasticity in the ‘food’ data Another useful method to visualize possible heteroskedasticity is to plot the residuals against the regressors suspected of creating heteroskedasticity, or, more generally, against the fitted values of the regression. res &lt;- residuals(mod1) yhat &lt;- fitted(mod1) plot(food$income,res, xlab=&quot;income&quot;, ylab=&quot;residuals&quot;) Figure 1.20: Residual plots in the ‘food’ model plot(yhat,res, xlab=&quot;fitted values&quot;, ylab=&quot;residuals&quot;) Figure 1.20: Residual plots in the ‘food’ model 1.4.2 Heteroskedasticity Tests data(gpa3, package=&#39;wooldridge&#39;) # Estimate model (only for spring data) reg &lt;- lm(cumgpa~sat+hsperc+tothrs+female+black+white, data=gpa3, subset=(spring==1)) # Breusch-Pagan (BP) Test bptest(reg) ## ## studentized Breusch-Pagan test ## ## data: reg ## BP = 44.557, df = 6, p-value = 5.732e-08 The R function that does this job is hccm(), which is part of the car package and yields a heteroskedasticity-robust coefficient covariance matrix. This matrix can then be used with other functions, such as coeftest() (instead of summary), waldtest() (instead of anova), or linearHypothesis() to perform hypothesis testing. The function hccm() takes several arguments, among which is the model for which we want the robust standard errors and the type of standard errors we wish to calculate. # Usual SE: coeftest(reg) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.47006477 0.22980308 6.3971 4.942e-10 *** ## sat 0.00114073 0.00017856 6.3885 5.197e-10 *** ## hsperc -0.00856636 0.00124042 -6.9060 2.275e-11 *** ## tothrs 0.00250400 0.00073099 3.4255 0.0006847 *** ## female 0.30343329 0.05902033 5.1412 4.497e-07 *** ## black -0.12828368 0.14737012 -0.8705 0.3846164 ## white -0.05872173 0.14098956 -0.4165 0.6772953 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Refined White heteroscedasticity-robust SE: coeftest(reg, vcov=hccm) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.47006477 0.22938036 6.4089 4.611e-10 *** ## sat 0.00114073 0.00019532 5.8402 1.169e-08 *** ## hsperc -0.00856636 0.00144359 -5.9341 6.963e-09 *** ## tothrs 0.00250400 0.00074930 3.3418 0.00092 *** ## female 0.30343329 0.06003964 5.0539 6.911e-07 *** ## black -0.12828368 0.12818828 -1.0007 0.31762 ## white -0.05872173 0.12043522 -0.4876 0.62615 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 cov3 &lt;- hccm(reg, type=&quot;hc3&quot;) # hc3 is the standard method ref.HC3 &lt;- coeftest(reg, vcov.=cov3) # Supply other White corrections cov1 &lt;- hccm(reg, type=&quot;hc1&quot;) ref.HC1 &lt;- coeftest(reg, vcov.=cov1) Another way of dealing with heteroskedasticity is to use the lmrob() function from the robustbase package4. This package is quite interesting, and offers quite a lot of functions for robust linear, and nonlinear, regression models. Running a robust linear regression is just the same as with lm(): regrobfit &lt;- lmrob(cumgpa~sat+hsperc+tothrs+female+black+white, data=gpa3, subset=(spring==1)) summary(regrobfit) ## ## Call: ## lmrob(formula = cumgpa ~ sat + hsperc + tothrs + female + black + white, ## data = gpa3, subset = (spring == 1)) ## \\--&gt; method = &quot;MM&quot; ## Residuals: ## Min 1Q Median 3Q Max ## -1.57535 -0.30124 -0.02834 0.26687 1.27950 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.4693758 0.2315018 6.347 6.62e-10 *** ## sat 0.0011185 0.0001953 5.727 2.17e-08 *** ## hsperc -0.0079056 0.0014293 -5.531 6.14e-08 *** ## tothrs 0.0021841 0.0007750 2.818 0.0051 ** ## female 0.3002542 0.0599150 5.011 8.50e-07 *** ## black -0.1281927 0.1268974 -1.010 0.3131 ## white -0.0305168 0.1181863 -0.258 0.7964 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Robust residual standard error: 0.4201 ## Multiple R-squared: 0.411, Adjusted R-squared: 0.4012 ## Convergence in 15 IRWLS iterations ## ## Robustness weights: ## 22 weights are ~= 1. The remaining 344 ones are summarized as ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.1291 0.8670 0.9471 0.8933 0.9854 0.9987 ## Algorithmic parameters: ## tuning.chi bb tuning.psi refine.tol ## 1.548e+00 5.000e-01 4.685e+00 1.000e-07 ## rel.tol scale.tol solve.tol eps.outlier ## 1.000e-07 1.000e-10 1.000e-07 2.732e-04 ## eps.x warn.limit.reject warn.limit.meanrw ## 2.601e-09 5.000e-01 5.000e-01 ## nResample max.it best.r.s k.fast.s k.max ## 500 50 2 1 200 ## maxit.scale trace.lev mts compute.rd fast.s.large.n ## 200 0 1000 0 2000 ## psi subsampling cov ## &quot;bisquare&quot; &quot;nonsingular&quot; &quot;.vcov.avar1&quot; ## compute.outlier.stats ## &quot;SM&quot; ## seed : int(0) This however, gives you different estimates than when fitting a linear regression model. The estimates should be the same, only the standard errors should be different. This is because the estimation method is different, and is also robust to outlines (at least that’s my understanding, I haven’t read the theoretical papers behind the package yet). Finally, it is also possible to bootstrap the standard errors. For this I will use the bootstrap() function from the modelr package: resamples &lt;- 100 boot_gpa3 &lt;- gpa3 %&gt;% modelr::bootstrap(resamples) The column strap contains resamples of the original data. I will run my linear regression from before on each of the resamples: boot_lin_reg &lt;- boot_gpa3 %&gt;% mutate(regressions = map(strap, ~lm(cumgpa~sat+hsperc+tothrs+female+black+white, data= . , subset=(spring==1))) ) We have added a new column called regressions which contains the linear regressions on each bootstrapped sample. Now, I will create a list of tidied regression results: tidied &lt;- boot_lin_reg %&gt;% mutate(tidy_lm = map(regressions, broom::tidy)) tidied$tidy_lm[[1]] term estimate std.error statistic p.value (Intercept) 1.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.228&nbsp;&nbsp;&nbsp; 6.59&nbsp; 1.53e-10 sat 0.000925 0.000179 5.17&nbsp; 3.78e-07 hsperc -0.00786&nbsp; 0.00121&nbsp; -6.52&nbsp; 2.36e-10 tothrs 0.003&nbsp;&nbsp;&nbsp; 0.000726 4.13&nbsp; 4.54e-05 female 0.291&nbsp;&nbsp;&nbsp; 0.0572&nbsp;&nbsp; 5.08&nbsp; 6.08e-07 black -0.0371&nbsp;&nbsp; 0.143&nbsp;&nbsp;&nbsp; -0.258 0.796&nbsp;&nbsp;&nbsp; white 0.0946&nbsp;&nbsp; 0.133&nbsp;&nbsp;&nbsp; 0.71&nbsp; 0.478&nbsp;&nbsp;&nbsp; list_mods &lt;- tidied %&gt;% pull(tidy_lm) mods_df &lt;- map2_df(list_mods, seq(1, resamples), ~mutate(.x, resample = .y)) head(mods_df, 5) term estimate std.error statistic p.value resample (Intercept) 1.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.228&nbsp;&nbsp;&nbsp; 6.59 1.53e-10 1 sat 0.000925 0.000179 5.17 3.78e-07 1 hsperc -0.00786&nbsp; 0.00121&nbsp; -6.52 2.36e-10 1 tothrs 0.003&nbsp;&nbsp;&nbsp; 0.000726 4.13 4.54e-05 1 female 0.291&nbsp;&nbsp;&nbsp; 0.0572&nbsp;&nbsp; 5.08 6.08e-07 1 r.std.error &lt;- mods_df %&gt;% group_by(term) %&gt;% summarise(r.std.error = sd(estimate)) reg %&gt;% broom::tidy() %&gt;% full_join(r.std.error) ## Joining, by = &quot;term&quot; term estimate std.error statistic p.value r.std.error (Intercept) 1.47&nbsp;&nbsp;&nbsp; 0.23&nbsp;&nbsp;&nbsp;&nbsp; 6.4&nbsp;&nbsp; 4.94e-10 0.215&nbsp;&nbsp;&nbsp; sat 0.00114 0.000179 6.39&nbsp; 5.2e-10&nbsp; 0.000188 hsperc -0.00857 0.00124&nbsp; -6.91&nbsp; 2.27e-11 0.00129&nbsp; tothrs 0.0025&nbsp; 0.000731 3.43&nbsp; 0.000685 0.00066&nbsp; female 0.303&nbsp;&nbsp; 0.059&nbsp;&nbsp;&nbsp; 5.14&nbsp; 4.5e-07&nbsp; 0.0572&nbsp;&nbsp; black -0.128&nbsp;&nbsp; 0.147&nbsp;&nbsp;&nbsp; -0.87&nbsp; 0.385&nbsp;&nbsp;&nbsp; 0.123&nbsp;&nbsp;&nbsp; white -0.0587&nbsp; 0.141&nbsp;&nbsp;&nbsp; -0.416 0.677&nbsp;&nbsp;&nbsp; 0.12&nbsp;&nbsp;&nbsp;&nbsp; Using the whole bootstrapping procedure is longer than simply using either one of the first two methods. However, this procedure is very flexible and can thus be adapted to a very large range of situations. 1.5 Weighted least squares Weighted Least Squares (WLS) attempts to provide a more efficient alternative to OLS. It is a special version of a feasible generalized least squares (FGLS) estimator. data(&quot;k401k&quot;) # OLS (only for singles: fsize==1) lm(nettfa ~ inc + I((age-25)^2) + male + e401k, data=k401ksubs, subset=(fsize==1)) ## ## Call: ## lm(formula = nettfa ~ inc + I((age - 25)^2) + male + e401k, data = k401ksubs, ## subset = (fsize == 1)) ## ## Coefficients: ## (Intercept) inc I((age - 25)^2) male ## -20.98499 0.77058 0.02513 2.47793 ## e401k ## 6.88622 Following Wooldrige, we assume that the variance is proportional to the income variable inc.. Therefore, the optimal weight is \\(\\frac{1}{inc}\\) which is given as weight in the lm() call. # WLS lm(nettfa ~ inc + I((age-25)^2) + male + e401k, weight=1/inc, data=k401ksubs, subset=(fsize==1)) ## ## Call: ## lm(formula = nettfa ~ inc + I((age - 25)^2) + male + e401k, data = k401ksubs, ## subset = (fsize == 1), weights = 1/inc) ## ## Coefficients: ## (Intercept) inc I((age - 25)^2) male ## -16.70252 0.74038 0.01754 1.84053 ## e401k ## 5.18828 We can also use heteroscedasticity-robust statistics to account for the fact that our variance function might be misspecified. # WLS wlsreg &lt;- lm(nettfa ~ inc + I((age-25)^2) + male + e401k, weight=1/inc, data=k401ksubs, subset=(fsize==1)) # non-robust results coeftest(wlsreg) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -16.7025205 1.9579947 -8.5304 &lt; 2.2e-16 *** ## inc 0.7403843 0.0643029 11.5140 &lt; 2.2e-16 *** ## I((age - 25)^2) 0.0175373 0.0019315 9.0796 &lt; 2.2e-16 *** ## male 1.8405293 1.5635872 1.1771 0.239287 ## e401k 5.1882807 1.7034258 3.0458 0.002351 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # robust results (Refined White SE:) coeftest(wlsreg,hccm) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -16.7025205 2.2482355 -7.4292 1.606e-13 *** ## inc 0.7403843 0.0752396 9.8403 &lt; 2.2e-16 *** ## I((age - 25)^2) 0.0175373 0.0025924 6.7650 1.742e-11 *** ## male 1.8405293 1.3132477 1.4015 0.1612159 ## e401k 5.1882807 1.5743329 3.2955 0.0009994 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 coeftest(wlsreg, vcov. = vcovHC) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -16.7025205 2.2482355 -7.4292 1.606e-13 *** ## inc 0.7403843 0.0752396 9.8403 &lt; 2.2e-16 *** ## I((age - 25)^2) 0.0175373 0.0025924 6.7650 1.742e-11 *** ## male 1.8405293 1.3132477 1.4015 0.1612159 ## e401k 5.1882807 1.5743329 3.2955 0.0009994 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 mySummary &lt;- function(model, VCOV) { print(coeftest(model, vcov. = VCOV)) print(waldtest(model, vcov = VCOV)) } mySummary(wlsreg, VCOV = vcovHAC) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -16.7025205 2.2425229 -7.4481 1.397e-13 *** ## inc 0.7403843 0.0752621 9.8374 &lt; 2.2e-16 *** ## I((age - 25)^2) 0.0175373 0.0025797 6.7981 1.392e-11 *** ## male 1.8405293 1.3056244 1.4097 0.158785 ## e401k 5.1882807 1.5733280 3.2976 0.000992 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Wald test ## ## Model 1: nettfa ~ inc + I((age - 25)^2) + male + e401k ## Model 2: nettfa ~ 1 ## Res.Df Df F Pr(&gt;F) ## 1 2012 ## 2 2016 -4 39.602 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The assumption that the variance is proportional to a regressor is usually hard to justify. Typically, we do not know the variance function; we have to estimate it. We can estimate the relation between variance and regressors using a linear regression of the log of the squared residuals from an initial OLS regression \\(log(\\hat{u}^{2})\\) as the dependent variable. Wooldrige suggests two version for the selection of regressors: the regressors \\(x_1, \\dots , x_k\\) from the original model similar to the BP test \\(\\hat{y}\\) and \\(\\hat{y}^{2}\\) from the original model similar to the White test data(&quot;smoke&quot;) # OLS olsreg&lt;-lm(cigs~log(income)+log(cigpric)+educ+age+I(age^2)+restaurn, data=smoke) olsreg ## ## Call: ## lm(formula = cigs ~ log(income) + log(cigpric) + educ + age + ## I(age^2) + restaurn, data = smoke) ## ## Coefficients: ## (Intercept) log(income) log(cigpric) educ age ## -3.639826 0.880268 -0.750862 -0.501498 0.770694 ## I(age^2) restaurn ## -0.009023 -2.825085 # BP test bptest(olsreg) ## ## studentized Breusch-Pagan test ## ## data: olsreg ## BP = 32.258, df = 6, p-value = 1.456e-05 # FGLS: estimation of the variance function logu2 &lt;- log(resid(olsreg)^2) varreg&lt;-lm(logu2~log(income)+log(cigpric)+educ+age+I(age^2)+restaurn, data=smoke) # FGLS: WLS w &lt;- 1/exp(fitted(varreg)) lm(cigs~log(income)+log(cigpric)+educ+age+I(age^2)+restaurn, weight=w ,data=smoke) ## ## Call: ## lm(formula = cigs ~ log(income) + log(cigpric) + educ + age + ## I(age^2) + restaurn, data = smoke, weights = w) ## ## Coefficients: ## (Intercept) log(income) log(cigpric) educ age ## 5.635463 1.295239 -2.940312 -0.463446 0.481948 ## I(age^2) restaurn ## -0.005627 -3.461064 1.6 Model specification and Parameter Heterogeneity 1.6.1 Functional Form Misspecifcation We have seen many ways to specify the relation between the dependent variable and the regressors. An obvious question is to ask whether or not a given specification is “correct”. 1.6.1.1 RESET The Regression Equation Specification Error Test (RESET) is a convenient tool to test the null hypothesis that the functional form is adequate. We can run the test ourselves or use the boxed routine resettest() from the package lmtest. data(&quot;hprice1&quot;) # original linear regression orig &lt;- lm(price ~ lotsize+sqrft+bdrms, data=hprice1) # regression for RESET test RESETreg &lt;- lm(price ~ lotsize+sqrft+bdrms+I(fitted(orig)^2)+ I(fitted(orig)^3), data=hprice1) RESETreg ## ## Call: ## lm(formula = price ~ lotsize + sqrft + bdrms + I(fitted(orig)^2) + ## I(fitted(orig)^3), data = hprice1) ## ## Coefficients: ## (Intercept) lotsize sqrft ## 1.661e+02 1.537e-04 1.760e-02 ## bdrms I(fitted(orig)^2) I(fitted(orig)^3) ## 2.175e+00 3.534e-04 1.546e-06 # RESET test. H0: all coeffs including &quot;fitted&quot; are=0 linearHypothesis(RESETreg, matchCoefs(RESETreg,&quot;fitted&quot;)) Res.Df RSS Df Sum of Sq F Pr(&gt;F) 84 3.01e+05 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; 82 2.7e+05&nbsp; 2 3.07e+04 4.67 0.012 Automatic routine: # original linear regression orig &lt;- lm(price ~ lotsize+sqrft+bdrms, data=hprice1) # RESET test resettest(orig) ## ## RESET test ## ## data: orig ## RESET = 4.6682, df1 = 2, df2 = 82, p-value = 0.01202 Wooldrige (2016, Section 9.2) also discusses tests of non-nested models. We can use the encomptest() function from the package lmtest. Two alternative models for the housing price \\[\\begin{equation} price = \\beta_0 + \\beta_1 lotsize +\\beta_2 sqrft +\\beta_3 bdrms + u \\end{equation}\\] \\[\\begin{equation} price = \\beta_0 + \\beta_1 log(lotsize) +\\beta_2 log(sqrft) +\\beta_3 log(bdrms) + u \\end{equation}\\] # two alternative models model1 &lt;- lm(price ~ lotsize + sqrft + bdrms, data=hprice1) model2 &lt;- lm(price ~ log(lotsize) + log(sqrft) + bdrms, data=hprice1) # Test against comprehensive model encomptest(model1,model2, data=hprice1) Res.Df Df F Pr(&gt;F) 82 -2 7.86 0.000753 82 -2 7.05 0.00149&nbsp; The output shows the “encompassing model” \\(E\\) with all variables. Both models are rejected against this comprehensive model. 1.6.1.2 Outlying observations Dealing with outliers is a tricky business. R offers different packages to test and adjust for outliers. But outliers can be a matter of opinion and not all outlier detection methods give the same results. With the OutliersO3 package we can compare different outlier detection methods. data(rdchem) s3 &lt;- O3prep(rdchem, method=c(&quot;HDo&quot;, &quot;adjOut&quot;, &quot;DDC&quot;)) O3s3 &lt;- O3plotM(s3) print(O3s3$nOut) ## HDo adjOut DDC ## 10 1 11 O3s3$gO3 An O3 plot of stackloss using the methods HDoutliers, adjOutlyingness and DectectDeviatingCells. The darker the cell, the more methods agree. If they all agree, the cell is coloured red and if all but one agree then orange. We use functions from the car package to obtain a table of different measures of leverage and influence for all observations. # Regression reg &lt;- lm(rdintens~sales+profmarg, data=rdchem) # Studentized residuals for all observations: studres &lt;- rstudent(reg) # Display extreme values: min(studres) ## [1] -1.818039 max(studres) ## [1] 4.555033 # Histogram (and overlayed density plot): hist(studres, freq=FALSE) lines(density(studres), lwd=2) 1.6.1.3 Missing Data Missing values in data is a common phenomenon in real world problems. In R, missing data can be represented by different values of the variable. NA (not available) indicates that we do not have the information NaN (not a number) indicates that the value is not defined, for example when we take the log of a negative number Base R offers many functions to detect missing observations. Sometimes using mice and VIM package for looking at missing data pattern and imputing missing data is even easier. data(&quot;lawsch85&quot;, package = &quot;wooldridge&quot; ) # extract LSAT lsat &lt;- lawsch85$LSAT # Create logical indicator for missings missLSAT &lt;- is.na(lawsch85$LSAT) # LSAT and indicator for Schools No. 120-129: rbind(lsat,missLSAT)[,120:129] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## lsat 156 159 157 167 NA 158 155 157 NA 163 ## missLSAT 0 0 0 0 1 0 0 0 1 0 # Frequencies of indicator table(missLSAT) ## missLSAT ## FALSE TRUE ## 150 6 # Missings for all variables in data frame (counts) colSums(is.na(lawsch85)) ## rank salary cost LSAT GPA libvol faculty age clsize ## 0 8 6 6 7 1 4 45 3 ## north south east west lsalary studfac top10 r11_25 r26_40 ## 0 0 0 0 8 6 0 0 0 ## r41_60 llibvol lcost ## 0 1 6 # Indicator for complete cases compl &lt;- complete.cases(lawsch85) table(compl) ## compl ## FALSE TRUE ## 66 90 # MICE package function to display msising values head(md.pattern(lawsch85, plot = FALSE)) ## rank north south east west top10 r11_25 r26_40 r41_60 libvol llibvol ## 90 1 1 1 1 1 1 1 1 1 1 1 ## 41 1 1 1 1 1 1 1 1 1 1 1 ## 6 1 1 1 1 1 1 1 1 1 1 1 ## 1 1 1 1 1 1 1 1 1 1 1 1 ## 3 1 1 1 1 1 1 1 1 1 1 1 ## 1 1 1 1 1 1 1 1 1 1 1 1 ## clsize faculty cost LSAT studfac lcost GPA salary lsalary age ## 90 1 1 1 1 1 1 1 1 1 1 0 ## 41 1 1 1 1 1 1 1 1 1 0 1 ## 6 1 1 1 1 1 1 1 0 0 1 2 ## 1 1 1 1 1 1 1 1 0 0 0 3 ## 3 1 1 1 0 1 1 0 1 1 1 2 ## 1 1 1 1 0 1 1 0 1 1 0 3 aggr_plot &lt;- aggr(lawsch85, col=c(&#39;navyblue&#39;,&#39;red&#39;), numbers=TRUE, sortVars=TRUE, labels=names(lawsch85), cex.axis=.7, gap=3, ylab=c(&quot;Histogram of missing data&quot;,&quot;Pattern&quot;)) Figure 1.21: Visualizing missing data ## ## Variables sorted by number of missings: ## Variable Count ## age 0.288461538 ## salary 0.051282051 ## lsalary 0.051282051 ## GPA 0.044871795 ## cost 0.038461538 ## LSAT 0.038461538 ## studfac 0.038461538 ## lcost 0.038461538 ## faculty 0.025641026 ## clsize 0.019230769 ## libvol 0.006410256 ## llibvol 0.006410256 ## rank 0.000000000 ## north 0.000000000 ## south 0.000000000 ## east 0.000000000 ## west 0.000000000 ## top10 0.000000000 ## r11_25 0.000000000 ## r26_40 0.000000000 ## r41_60 0.000000000 Regression command like lm() have as argument na.rm=TRUE! # Mean of a variable with missings: mean(lawsch85$LSAT) ## [1] NA mean(lawsch85$LSAT,na.rm=TRUE) ## [1] 158.2933 # Regression with missings summary(lm(log(salary)~LSAT+cost+age, data=lawsch85)) ## ## Call: ## lm(formula = log(salary) ~ LSAT + cost + age, data = lawsch85) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.40989 -0.09438 0.00317 0.10436 0.45483 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.384e+00 6.781e-01 6.465 4.94e-09 *** ## LSAT 3.722e-02 4.501e-03 8.269 1.06e-12 *** ## cost 1.114e-05 4.321e-06 2.577 0.011563 * ## age 1.503e-03 4.354e-04 3.453 0.000843 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1545 on 91 degrees of freedom ## (61 observations deleted due to missingness) ## Multiple R-squared: 0.6708, Adjusted R-squared: 0.6599 ## F-statistic: 61.81 on 3 and 91 DF, p-value: &lt; 2.2e-16 R packages provide multiple imputation algorithms. Without going into detail how those algorithms work, we can use for example the meth=“pmm” argument from the mice package to apply a predictive mean matching as imputation method. # We use a diffferent dataset to speed up the imputation process data &lt;- airquality data[4:10,3] &lt;- rep(NA,7) data[1:5,4] &lt;- NA tempData &lt;- mice(data,m=5,maxit=50,meth=&#39;pmm&#39;,seed=500) summary(tempData) ## Class: mids ## Number of multiple imputations: 5 ## Imputation methods: ## Ozone Solar.R Wind Temp Month Day ## &quot;pmm&quot; &quot;pmm&quot; &quot;pmm&quot; &quot;pmm&quot; &quot;&quot; &quot;&quot; ## PredictorMatrix: ## Ozone Solar.R Wind Temp Month Day ## Ozone 0 1 1 1 1 1 ## Solar.R 1 0 1 1 1 1 ## Wind 1 1 0 1 1 1 ## Temp 1 1 1 0 1 1 ## Month 1 1 1 1 0 1 ## Day 1 1 1 1 1 0 1.7 Least absolute Deviations (LAD) Estimation As an alternative to OLS, the least absolute deviations (LAD) is less sensitive to outliers. Instead of minimizing the sum of squared residuals, it minimizes the sum of the absolute values of the residuals. In R, general quantile regression (and LAD as the default special case) can easily be implemented with the command reg() from the quantreg package. # OLS Regression ols &lt;- lm(rdintens ~ I(sales/1000) +profmarg, data=rdchem) # LAD Regression lad &lt;- rq(rdintens ~ I(sales/1000) +profmarg, data=rdchem) # regression table stargazer(ols,lad, type = &quot;text&quot;) ## ## ================================================= ## Dependent variable: ## ----------------------------- ## rdintens ## OLS quantile ## regression ## (1) (2) ## ------------------------------------------------- ## I(sales/1000) 0.053 0.019 ## (0.044) (0.059) ## ## profmarg 0.045 0.118** ## (0.046) (0.049) ## ## Constant 2.625*** 1.623*** ## (0.586) (0.509) ## ## ------------------------------------------------- ## Observations 32 32 ## R2 0.076 ## Adjusted R2 0.012 ## Residual Std. Error 1.862 (df = 29) ## F Statistic 1.195 (df = 2; 29) ## ================================================= ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Note: LAD inferences are only valid asymptotically, so the results in this example with \\(n =32\\) should be taken with caution. References "],
["binarymodels.html", "Chapter 2 Qualitative and LDV Models 2.1 Linear probability models 2.2 Logit and Probit Models: Estimation 2.3 Count data: The Poisson Regression Model 2.4 Censored and Truncated Regression Models", " Chapter 2 Qualitative and LDV Models To load the dataset and necessary functions: # This function 1. checks if the packages are installed. 2. It installs the packages if they were not in the list of installed packages. 3. It loads the packages into the workspace # devtools::install_github(&quot;ccolonescu/PoEdata&quot;) PACKAGES&lt;-c( &quot;tidyverse&quot;, # for data manipulation and ggplots &quot;broom&quot;, # Tidy regression output &quot;Hmisc&quot;, # Harrell Miscellaneous functions &quot;psych&quot;, # Procedures for Psychometric research &quot;car&quot;, # Companion to applied regression &quot;knitr&quot;, # knit functions # &quot;kableExtra&quot;, # extended knit functions for objects exported from other packages &quot;huxtable&quot;, # Regression tables, broom compatible &quot;stargazer&quot;, # Regression tables &quot;AER&quot;, # Functions, data sets, examples, demos, and vignettes for the book Christian Kleiber and Achim Zeileis (2008) &quot;PoEdata&quot;, # R data sets for &quot;Principles of Econometrics&quot; by Hill, Griffiths, and Lim, 4e, Wiley. https://github.com/ccolonescu/PoEdata &quot;wooldridge&quot;, # Wooldrige Datasets &quot;MCMCpack&quot;, # Contains functions to perform Bayesian inference using posterior simulation for a number of ssatistical models. &quot;sampleSelection&quot;, # Two-step and maximum likelihood estimation of Heckman-type sample selection models &quot;scales&quot;, # scale helper functions such as percent &quot;lmtest&quot;, &quot;margins&quot;, # Stata like margin functions &quot;prediction&quot;, # Type stable predictions &quot;nnet&quot;, # Multinomial logit &quot;survival&quot;, # Survival Analysis &quot;sampleSelection&quot;, # Heckman type sample selection &quot;censReg&quot;, # Censored Regression models &quot;magrittr&quot;) # pipes inst&lt;-match(PACKAGES, .packages(all=TRUE)) need&lt;-which(is.na(inst)) if (length(need)&gt;0) install.packages(PACKAGES[need]) lapply(PACKAGES, require, character.only=T) Binary dependent variables are frequently studied in applied economics. Because a dummy variable \\(y\\) can only take values 0 and 1, its (conditional) expected value is equal to the (conditional) probability that \\(y=1\\): \\(E(y|x) = 0 \\times P(y = 0|x) + 1 \\times P(y = 1|x) = P(y=1|x)\\) An important class of models specifies the success probability as \\(P(y = 1 | x) = G(\\beta_0 + \\beta_1 + \\dots \\beta_k x_k) = G(\\boldsymbol{x} \\boldsymbol{\\beta})\\) The following table is taken from Dalpiaz (2016) and summarizes three examples of a generalized linear model: Linear Regression Poisson Regression Logistic Regression \\(Y \\mid {\\bf X} = {\\bf x}\\) \\(N(\\mu({\\bf x}), \\sigma^2)\\) \\(\\text{Pois}(\\lambda({\\bf x}))\\) \\(\\text{Bern}(p({\\bf x}))\\) Distribution Name Normal Poisson Bernoulli (Binomial) \\(\\text{E}[Y \\mid {\\bf X} = {\\bf x}]\\) \\(\\mu({\\bf x})\\) \\(\\lambda({\\bf x})\\) \\(p({\\bf x})\\) Support Real: \\((-\\infty, \\infty)\\) Integer: \\(0, 1, 2, \\ldots\\) Integer: \\(0, 1\\) Usage Numeric Data Count (Integer) Data Binary (Class ) Data Link Name Identity Log Logit Link Function \\(\\eta({\\bf x}) = \\mu({\\bf x})\\) \\(\\eta({\\bf x}) = \\log(\\lambda({\\bf x}))\\) \\(\\eta({\\bf x}) = \\log \\left(\\frac{p({\\bf x})}{1 - p({\\bf x})} \\right)\\) Mean Function \\(\\mu({\\bf x}) = \\eta({\\bf x})\\) \\(\\lambda({\\bf x}) = e^{\\eta({\\bf x})}\\) \\(p({\\bf x}) = \\frac{e^{\\eta({\\bf x})}}{1 + e^{\\eta({\\bf x})}} = \\frac{1}{1 + e^{-\\eta({\\bf x})}}\\) Like ordinary linear regression, we will seek to “fit” the model by estimating the \\(\\beta\\) parameters. To do so, we will use the method of maximum likelihood. Note that a Bernoulli distribution is a specific case of a binomial distribution where the \\(n\\) parameter of a binomial is \\(1\\). Binomial regression is also possible, but we’ll focus on the much more popular Bernoulli case. So, in general, GLMs relate the mean of the response to a linear combination of the predictors, \\(\\eta({\\bf x})\\), through the use of a link function, \\(g()\\). That is, \\[ \\eta({\\bf x}) = g\\left(\\text{E}[Y \\mid {\\bf X} = {\\bf x}]\\right). \\] The mean is then \\[ \\text{E}[Y \\mid {\\bf X} = {\\bf x}] = g^{-1}(\\eta({\\bf x})). \\] 2.1 Linear probability models If a dummy variable is used as the dependent variable \\(y\\), we can still use OLS to estimate its relation to the regressors \\(x\\). data(&quot;mroz&quot;, package = &quot;wooldridge&quot;) Let us use the describe() function from the Hmisc package to use a different way to describe the datatset. describe(mroz) vars n mean sd median trimmed mad min max range skew kurtosis se 1 753 0.568&nbsp;&nbsp;&nbsp; 0.496&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.585&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -0.276&nbsp; -1.93&nbsp;&nbsp; 0.0181&nbsp; 2 753 741&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 871&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 288&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 634&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 427&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.95e+03 4.95e+03 0.921&nbsp; 0.185&nbsp; 31.8&nbsp;&nbsp;&nbsp;&nbsp; 3 753 0.238&nbsp;&nbsp;&nbsp; 0.524&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.119&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3&nbsp;&nbsp;&nbsp; 5.23&nbsp;&nbsp; 0.0191&nbsp; 4 753 1.35&nbsp;&nbsp;&nbsp;&nbsp; 1.32&nbsp;&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.21&nbsp;&nbsp;&nbsp;&nbsp; 1.48&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.906&nbsp; 0.8&nbsp;&nbsp;&nbsp; 0.0481&nbsp; 5 753 42.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8.07&nbsp;&nbsp;&nbsp;&nbsp; 43&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 42.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 10.4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 30&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 60&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 30&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.151&nbsp; -1.02&nbsp;&nbsp; 0.294&nbsp;&nbsp; 6 753 12.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.28&nbsp;&nbsp;&nbsp;&nbsp; 12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 17&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.021&nbsp; 0.734&nbsp; 0.0831&nbsp; 7 428 4.18&nbsp;&nbsp;&nbsp;&nbsp; 3.31&nbsp;&nbsp;&nbsp;&nbsp; 3.48&nbsp;&nbsp;&nbsp;&nbsp; 3.67&nbsp;&nbsp;&nbsp;&nbsp; 1.94&nbsp;&nbsp;&nbsp;&nbsp; 0.128&nbsp;&nbsp; 25&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 24.9&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.07&nbsp;&nbsp; 13.7&nbsp;&nbsp;&nbsp; 0.16&nbsp;&nbsp;&nbsp; 8 753 1.85&nbsp;&nbsp;&nbsp;&nbsp; 2.42&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.44&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 9.98&nbsp;&nbsp;&nbsp;&nbsp; 9.98&nbsp;&nbsp;&nbsp;&nbsp; 1.17&nbsp;&nbsp; 0.724&nbsp; 0.0882&nbsp; 9 753 2.27e+03 596&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.16e+03 2.24e+03 421&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 175&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.01e+03 4.84e+03 0.565&nbsp; 1.84&nbsp;&nbsp; 21.7&nbsp;&nbsp;&nbsp;&nbsp; 10 753 45.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8.06&nbsp;&nbsp;&nbsp;&nbsp; 46&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 45.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 10.4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 30&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 60&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 30&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -0.0643 -1.01&nbsp;&nbsp; 0.294&nbsp;&nbsp; 11 753 12.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.02&nbsp;&nbsp;&nbsp;&nbsp; 12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12.6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.97&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 17&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 14&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -0.276&nbsp; -0.343&nbsp; 0.11&nbsp;&nbsp;&nbsp; 12 753 7.48&nbsp;&nbsp;&nbsp;&nbsp; 4.23&nbsp;&nbsp;&nbsp;&nbsp; 6.98&nbsp;&nbsp;&nbsp;&nbsp; 7.04&nbsp;&nbsp;&nbsp;&nbsp; 3.25&nbsp;&nbsp;&nbsp;&nbsp; 0.412&nbsp;&nbsp; 40.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 40.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.26&nbsp;&nbsp; 11.5&nbsp;&nbsp;&nbsp; 0.154&nbsp;&nbsp; 13 753 2.31e+04 1.22e+04 2.09e+04 2.17e+04 9.08e+03 1.5e+03 9.6e+04&nbsp; 9.45e+04 1.9&nbsp;&nbsp;&nbsp; 6.46&nbsp;&nbsp; 444&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 14 753 0.679&nbsp;&nbsp;&nbsp; 0.0835&nbsp;&nbsp; 0.692&nbsp;&nbsp;&nbsp; 0.687&nbsp;&nbsp;&nbsp; 0.0445&nbsp;&nbsp; 0.442&nbsp;&nbsp; 0.942&nbsp;&nbsp;&nbsp; 0.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -0.64&nbsp;&nbsp; 1.38&nbsp;&nbsp; 0.00304 15 753 9.25&nbsp;&nbsp;&nbsp;&nbsp; 3.37&nbsp;&nbsp;&nbsp;&nbsp; 10&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 9.24&nbsp;&nbsp;&nbsp;&nbsp; 4.45&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 17&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 17&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -0.118&nbsp; -0.104&nbsp; 0.123&nbsp;&nbsp; 16 753 8.81&nbsp;&nbsp;&nbsp;&nbsp; 3.57&nbsp;&nbsp;&nbsp;&nbsp; 7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8.76&nbsp;&nbsp;&nbsp;&nbsp; 4.45&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 17&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 17&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.197&nbsp; -0.0463 0.13&nbsp;&nbsp;&nbsp; 17 753 8.62&nbsp;&nbsp;&nbsp;&nbsp; 3.11&nbsp;&nbsp;&nbsp;&nbsp; 7.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8.57&nbsp;&nbsp;&nbsp;&nbsp; 3.71&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 14&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 11&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.195&nbsp; -0.649&nbsp; 0.114&nbsp;&nbsp; 18 753 0.643&nbsp;&nbsp;&nbsp; 0.48&nbsp;&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.678&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -0.595&nbsp; -1.65&nbsp;&nbsp; 0.0175&nbsp; 19 753 10.6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8.07&nbsp;&nbsp;&nbsp;&nbsp; 9&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 9.76&nbsp;&nbsp;&nbsp;&nbsp; 7.41&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 45&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 45&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.959&nbsp; 0.692&nbsp; 0.294&nbsp;&nbsp; 20 753 20.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 11.6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 17.7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 18.6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8.24&nbsp;&nbsp;&nbsp;&nbsp; -0.0291&nbsp; 96&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 96&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.21&nbsp;&nbsp; 8.35&nbsp;&nbsp; 0.424&nbsp;&nbsp; 21 428 1.19&nbsp;&nbsp;&nbsp;&nbsp; 0.723&nbsp;&nbsp;&nbsp; 1.25&nbsp;&nbsp;&nbsp;&nbsp; 1.22&nbsp;&nbsp;&nbsp;&nbsp; 0.586&nbsp;&nbsp;&nbsp; -2.05&nbsp;&nbsp;&nbsp; 3.22&nbsp;&nbsp;&nbsp;&nbsp; 5.27&nbsp;&nbsp;&nbsp;&nbsp; -0.683&nbsp; 2.53&nbsp;&nbsp; 0.035&nbsp;&nbsp; 22 753 178&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 250&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 81&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 124&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 107&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.02e+03 2.02e+03 2.58&nbsp;&nbsp; 8.69&nbsp;&nbsp; 9.1&nbsp;&nbsp;&nbsp;&nbsp; The function rcorr() can be useful to check visually for correlation. rcorr(as.matrix(mroz)) ## inlf hours kidslt6 kidsge6 age educ wage repwage hushrs ## inlf 1.00 0.74 -0.21 0.00 -0.08 0.19 NaN 0.63 -0.07 ## hours 0.74 1.00 -0.22 -0.09 -0.03 0.11 -0.10 0.61 -0.06 ## kidslt6 -0.21 -0.22 1.00 0.08 -0.43 0.11 0.03 -0.13 0.02 ## kidsge6 0.00 -0.09 0.08 1.00 -0.39 -0.06 -0.08 -0.07 0.10 ## age -0.08 -0.03 -0.43 -0.39 1.00 -0.12 0.03 -0.06 -0.08 ## educ 0.19 0.11 0.11 -0.06 -0.12 1.00 0.34 0.27 0.08 ## wage NaN -0.10 0.03 -0.08 0.03 0.34 1.00 0.42 -0.03 ## repwage 0.63 0.61 -0.13 -0.07 -0.06 0.27 0.42 1.00 -0.07 ## hushrs -0.07 -0.06 0.02 0.10 -0.08 0.08 -0.03 -0.07 1.00 ## husage -0.07 -0.03 -0.44 -0.35 0.89 -0.13 0.03 -0.06 -0.10 ## huseduc 0.05 -0.01 0.13 0.01 -0.16 0.61 0.17 0.11 0.11 ## huswage -0.07 -0.10 0.03 -0.03 0.03 0.28 0.22 0.02 -0.24 ## faminc 0.10 0.15 -0.03 -0.02 0.05 0.36 0.30 0.21 0.13 ## mtr -0.14 -0.19 0.06 0.15 -0.06 -0.41 -0.31 -0.24 -0.14 ## motheduc 0.09 0.06 0.11 0.03 -0.23 0.44 0.06 0.09 0.05 ## fatheduc 0.06 0.01 0.10 -0.03 -0.16 0.44 0.11 0.10 0.05 ## unem -0.03 -0.06 -0.01 0.01 0.08 0.07 0.03 0.01 -0.16 ## city -0.01 -0.02 -0.04 -0.03 0.10 0.16 0.12 0.09 -0.11 ## exper 0.34 0.40 -0.19 -0.30 0.33 0.07 0.05 0.34 -0.10 ## nwifeinc -0.12 -0.12 0.04 0.02 0.06 0.28 0.14 -0.06 0.16 ## lwage NaN -0.02 -0.02 -0.12 0.05 0.34 0.83 0.51 0.00 ## expersq 0.26 0.34 -0.18 -0.30 0.38 0.02 0.04 0.28 -0.07 ## husage huseduc huswage faminc mtr motheduc fatheduc unem city ## inlf -0.07 0.05 -0.07 0.10 -0.14 0.09 0.06 -0.03 -0.01 ## hours -0.03 -0.01 -0.10 0.15 -0.19 0.06 0.01 -0.06 -0.02 ## kidslt6 -0.44 0.13 0.03 -0.03 0.06 0.11 0.10 -0.01 -0.04 ## kidsge6 -0.35 0.01 -0.03 -0.02 0.15 0.03 -0.03 0.01 -0.03 ## age 0.89 -0.16 0.03 0.05 -0.06 -0.23 -0.16 0.08 0.10 ## educ -0.13 0.61 0.28 0.36 -0.41 0.44 0.44 0.07 0.16 ## wage 0.03 0.17 0.22 0.30 -0.31 0.06 0.11 0.03 0.12 ## repwage -0.06 0.11 0.02 0.21 -0.24 0.09 0.10 0.01 0.09 ## hushrs -0.10 0.11 -0.24 0.13 -0.14 0.05 0.05 -0.16 -0.11 ## husage 1.00 -0.20 0.02 0.04 -0.04 -0.23 -0.14 0.05 0.07 ## huseduc -0.20 1.00 0.39 0.38 -0.44 0.32 0.37 0.06 0.23 ## huswage 0.02 0.39 1.00 0.73 -0.72 0.13 0.19 0.16 0.32 ## faminc 0.04 0.38 0.73 1.00 -0.88 0.16 0.21 0.06 0.25 ## mtr -0.04 -0.44 -0.72 -0.88 1.00 -0.19 -0.25 -0.06 -0.26 ## motheduc -0.23 0.32 0.13 0.16 -0.19 1.00 0.57 0.02 0.07 ## fatheduc -0.14 0.37 0.19 0.21 -0.25 0.57 1.00 0.06 0.15 ## unem 0.05 0.06 0.16 0.06 -0.06 0.02 0.06 1.00 0.18 ## city 0.07 0.23 0.32 0.25 -0.26 0.07 0.15 0.18 1.00 ## exper 0.27 -0.04 -0.10 -0.03 -0.03 -0.08 -0.08 0.00 0.01 ## nwifeinc 0.05 0.36 0.76 0.94 -0.81 0.13 0.19 0.07 0.24 ## lwage 0.04 0.16 0.17 0.35 -0.37 0.05 0.08 0.04 0.10 ## expersq 0.31 -0.07 -0.12 -0.04 -0.02 -0.10 -0.10 -0.02 -0.01 ## exper nwifeinc lwage expersq ## inlf 0.34 -0.12 NaN 0.26 ## hours 0.40 -0.12 -0.02 0.34 ## kidslt6 -0.19 0.04 -0.02 -0.18 ## kidsge6 -0.30 0.02 -0.12 -0.30 ## age 0.33 0.06 0.05 0.38 ## educ 0.07 0.28 0.34 0.02 ## wage 0.05 0.14 0.83 0.04 ## repwage 0.34 -0.06 0.51 0.28 ## hushrs -0.10 0.16 0.00 -0.07 ## husage 0.27 0.05 0.04 0.31 ## huseduc -0.04 0.36 0.16 -0.07 ## huswage -0.10 0.76 0.17 -0.12 ## faminc -0.03 0.94 0.35 -0.04 ## mtr -0.03 -0.81 -0.37 -0.02 ## motheduc -0.08 0.13 0.05 -0.10 ## fatheduc -0.08 0.19 0.08 -0.10 ## unem 0.00 0.07 0.04 -0.02 ## city 0.01 0.24 0.10 -0.01 ## exper 1.00 -0.17 0.17 0.94 ## nwifeinc -0.17 1.00 0.14 -0.17 ## lwage 0.17 0.14 1.00 0.13 ## expersq 0.94 -0.17 0.13 1.00 ## ## n ## inlf hours kidslt6 kidsge6 age educ wage repwage hushrs husage ## inlf 753 753 753 753 753 753 428 753 753 753 ## hours 753 753 753 753 753 753 428 753 753 753 ## kidslt6 753 753 753 753 753 753 428 753 753 753 ## kidsge6 753 753 753 753 753 753 428 753 753 753 ## age 753 753 753 753 753 753 428 753 753 753 ## educ 753 753 753 753 753 753 428 753 753 753 ## wage 428 428 428 428 428 428 428 428 428 428 ## repwage 753 753 753 753 753 753 428 753 753 753 ## hushrs 753 753 753 753 753 753 428 753 753 753 ## husage 753 753 753 753 753 753 428 753 753 753 ## huseduc 753 753 753 753 753 753 428 753 753 753 ## huswage 753 753 753 753 753 753 428 753 753 753 ## faminc 753 753 753 753 753 753 428 753 753 753 ## mtr 753 753 753 753 753 753 428 753 753 753 ## motheduc 753 753 753 753 753 753 428 753 753 753 ## fatheduc 753 753 753 753 753 753 428 753 753 753 ## unem 753 753 753 753 753 753 428 753 753 753 ## city 753 753 753 753 753 753 428 753 753 753 ## exper 753 753 753 753 753 753 428 753 753 753 ## nwifeinc 753 753 753 753 753 753 428 753 753 753 ## lwage 428 428 428 428 428 428 428 428 428 428 ## expersq 753 753 753 753 753 753 428 753 753 753 ## huseduc huswage faminc mtr motheduc fatheduc unem city exper ## inlf 753 753 753 753 753 753 753 753 753 ## hours 753 753 753 753 753 753 753 753 753 ## kidslt6 753 753 753 753 753 753 753 753 753 ## kidsge6 753 753 753 753 753 753 753 753 753 ## age 753 753 753 753 753 753 753 753 753 ## educ 753 753 753 753 753 753 753 753 753 ## wage 428 428 428 428 428 428 428 428 428 ## repwage 753 753 753 753 753 753 753 753 753 ## hushrs 753 753 753 753 753 753 753 753 753 ## husage 753 753 753 753 753 753 753 753 753 ## huseduc 753 753 753 753 753 753 753 753 753 ## huswage 753 753 753 753 753 753 753 753 753 ## faminc 753 753 753 753 753 753 753 753 753 ## mtr 753 753 753 753 753 753 753 753 753 ## motheduc 753 753 753 753 753 753 753 753 753 ## fatheduc 753 753 753 753 753 753 753 753 753 ## unem 753 753 753 753 753 753 753 753 753 ## city 753 753 753 753 753 753 753 753 753 ## exper 753 753 753 753 753 753 753 753 753 ## nwifeinc 753 753 753 753 753 753 753 753 753 ## lwage 428 428 428 428 428 428 428 428 428 ## expersq 753 753 753 753 753 753 753 753 753 ## nwifeinc lwage expersq ## inlf 753 428 753 ## hours 753 428 753 ## kidslt6 753 428 753 ## kidsge6 753 428 753 ## age 753 428 753 ## educ 753 428 753 ## wage 428 428 428 ## repwage 753 428 753 ## hushrs 753 428 753 ## husage 753 428 753 ## huseduc 753 428 753 ## huswage 753 428 753 ## faminc 753 428 753 ## mtr 753 428 753 ## motheduc 753 428 753 ## fatheduc 753 428 753 ## unem 753 428 753 ## city 753 428 753 ## exper 753 428 753 ## nwifeinc 753 428 753 ## lwage 428 428 428 ## expersq 753 428 753 ## ## P ## inlf hours kidslt6 kidsge6 age educ wage repwage hushrs ## inlf 0.0000 0.0000 0.9470 0.0272 0.0000 0.0000 0.0738 ## hours 0.0000 0.0000 0.0128 0.3642 0.0036 0.0435 0.0000 0.1224 ## kidslt6 0.0000 0.0000 0.0209 0.0000 0.0028 0.5168 0.0002 0.5057 ## kidsge6 0.9470 0.0128 0.0209 0.0000 0.1063 0.1017 0.0596 0.0063 ## age 0.0272 0.3642 0.0000 0.0000 0.0009 0.5306 0.1098 0.0206 ## educ 0.0000 0.0036 0.0028 0.1063 0.0009 0.0000 0.0000 0.0304 ## wage 0.0435 0.5168 0.1017 0.5306 0.0000 0.0000 0.5061 ## repwage 0.0000 0.0000 0.0002 0.0596 0.1098 0.0000 0.0000 0.0521 ## hushrs 0.0738 0.1224 0.5057 0.0063 0.0206 0.0304 0.5061 0.0521 ## husage 0.0458 0.3943 0.0000 0.0000 0.0000 0.0002 0.5966 0.1288 0.0088 ## huseduc 0.2082 0.7915 0.0002 0.7960 0.0000 0.0000 0.0006 0.0033 0.0030 ## huswage 0.0567 0.0068 0.3749 0.4156 0.4592 0.0000 0.0000 0.5974 0.0000 ## faminc 0.0066 0.0000 0.4465 0.5930 0.1505 0.0000 0.0000 0.0000 0.0004 ## mtr 0.0000 0.0000 0.1010 0.0000 0.1037 0.0000 0.0000 0.0000 0.0002 ## motheduc 0.0130 0.1126 0.0031 0.3749 0.0000 0.0000 0.2387 0.0188 0.1436 ## fatheduc 0.1135 0.7080 0.0083 0.4622 0.0000 0.0000 0.0258 0.0048 0.1676 ## unem 0.4311 0.0983 0.8042 0.6956 0.0345 0.0478 0.5054 0.8026 0.0000 ## city 0.8658 0.6568 0.2426 0.3577 0.0081 0.0000 0.0135 0.0164 0.0020 ## exper 0.0000 0.0000 0.0000 0.0000 0.0000 0.0692 0.2563 0.0000 0.0064 ## nwifeinc 0.0012 0.0006 0.2951 0.4974 0.1078 0.0000 0.0032 0.1258 0.0000 ## lwage 0.7315 0.7026 0.0130 0.2569 0.0000 0.0000 0.0000 0.9378 ## expersq 0.0000 0.0000 0.0000 0.0000 0.0000 0.5084 0.3814 0.0000 0.0406 ## husage huseduc huswage faminc mtr motheduc fatheduc unem ## inlf 0.0458 0.2082 0.0567 0.0066 0.0000 0.0130 0.1135 0.4311 ## hours 0.3943 0.7915 0.0068 0.0000 0.0000 0.1126 0.7080 0.0983 ## kidslt6 0.0000 0.0002 0.3749 0.4465 0.1010 0.0031 0.0083 0.8042 ## kidsge6 0.0000 0.7960 0.4156 0.5930 0.0000 0.3749 0.4622 0.6956 ## age 0.0000 0.0000 0.4592 0.1505 0.1037 0.0000 0.0000 0.0345 ## educ 0.0002 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0478 ## wage 0.5966 0.0006 0.0000 0.0000 0.0000 0.2387 0.0258 0.5054 ## repwage 0.1288 0.0033 0.5974 0.0000 0.0000 0.0188 0.0048 0.8026 ## hushrs 0.0088 0.0030 0.0000 0.0004 0.0002 0.1436 0.1676 0.0000 ## husage 0.0000 0.5897 0.2670 0.2214 0.0000 0.0002 0.1455 ## huseduc 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.1315 ## huswage 0.5897 0.0000 0.0000 0.0000 0.0005 0.0000 0.0000 ## faminc 0.2670 0.0000 0.0000 0.0000 0.0000 0.0000 0.0916 ## mtr 0.2214 0.0000 0.0000 0.0000 0.0000 0.0000 0.1138 ## motheduc 0.0000 0.0000 0.0005 0.0000 0.0000 0.0000 0.6141 ## fatheduc 0.0002 0.0000 0.0000 0.0000 0.0000 0.0000 0.1085 ## unem 0.1455 0.1315 0.0000 0.0916 0.1138 0.6141 0.1085 ## city 0.0636 0.0000 0.0000 0.0000 0.0000 0.0623 0.0000 0.0000 ## exper 0.0000 0.3198 0.0045 0.4478 0.3725 0.0241 0.0306 0.9033 ## nwifeinc 0.2091 0.0000 0.0000 0.0000 0.0000 0.0004 0.0000 0.0398 ## lwage 0.4211 0.0010 0.0005 0.0000 0.0000 0.3305 0.1086 0.3933 ## expersq 0.0000 0.0413 0.0011 0.2458 0.6564 0.0060 0.0068 0.5108 ## city exper nwifeinc lwage expersq ## inlf 0.8658 0.0000 0.0012 0.0000 ## hours 0.6568 0.0000 0.0006 0.7315 0.0000 ## kidslt6 0.2426 0.0000 0.2951 0.7026 0.0000 ## kidsge6 0.3577 0.0000 0.4974 0.0130 0.0000 ## age 0.0081 0.0000 0.1078 0.2569 0.0000 ## educ 0.0000 0.0692 0.0000 0.0000 0.5084 ## wage 0.0135 0.2563 0.0032 0.0000 0.3814 ## repwage 0.0164 0.0000 0.1258 0.0000 0.0000 ## hushrs 0.0020 0.0064 0.0000 0.9378 0.0406 ## husage 0.0636 0.0000 0.2091 0.4211 0.0000 ## huseduc 0.0000 0.3198 0.0000 0.0010 0.0413 ## huswage 0.0000 0.0045 0.0000 0.0005 0.0011 ## faminc 0.0000 0.4478 0.0000 0.0000 0.2458 ## mtr 0.0000 0.3725 0.0000 0.0000 0.6564 ## motheduc 0.0623 0.0241 0.0004 0.3305 0.0060 ## fatheduc 0.0000 0.0306 0.0000 0.1086 0.0068 ## unem 0.0000 0.9033 0.0398 0.3933 0.5108 ## city 0.7583 0.0000 0.0446 0.7565 ## exper 0.7583 0.0000 0.0004 0.0000 ## nwifeinc 0.0000 0.0000 0.0034 0.0000 ## lwage 0.0446 0.0004 0.0034 0.0090 ## expersq 0.7565 0.0000 0.0000 0.0090 Estimate linear probability model linprob &lt;- lm(inlf~nwifeinc+educ+exper+I(exper^2)+age+kidslt6+kidsge6,data=mroz) # Regression table with heteroscedasticity-robust SE and t tests: coeftest(linprob,vcov=hccm) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.58551922 0.15358032 3.8125 0.000149 *** ## nwifeinc -0.00340517 0.00155826 -2.1852 0.029182 * ## educ 0.03799530 0.00733982 5.1766 2.909e-07 *** ## exper 0.03949239 0.00598359 6.6001 7.800e-11 *** ## I(exper^2) -0.00059631 0.00019895 -2.9973 0.002814 ** ## age -0.01609081 0.00241459 -6.6640 5.183e-11 *** ## kidslt6 -0.26181047 0.03215160 -8.1430 1.621e-15 *** ## kidsge6 0.01301223 0.01366031 0.9526 0.341123 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The estimated coefficient educ can be interpreted as: an additional year of schooling increases the probability that a woman is in the labor force ceteris paribus by 3.80%. One problem with linear probability models is that \\(P(y=1|x)\\) is specified as a linear function of the regressors. By construction, there are more or less realistic combinations of regressor values that yield \\(\\hat{y} &lt; 0\\) or \\(\\hat{y} &gt; 1\\). Predictions for two “extreme” women: xpred &lt;- list(nwifeinc=c(100,0),educ=c(5,17),exper=c(0,30), age=c(20,52),kidslt6=c(2,0),kidsge6=c(0,0)) predict(linprob,xpred) ## 1 2 ## -0.4104582 1.0428084 2.2 Logit and Probit Models: Estimation For binary response models, the most widely used specification for G are the probit model with \\(G(z) = \\phi(z)\\), the standard cdf and the logit model with \\(G(z) = \\Lambda(z) = \\frac{exp(z)}{1+ exp(z)}\\), the cdf of the logistic distribution. In R many generalized linear models can be estimated by the glm() command. It accepts the additional option family = binomial (link = logit) for the logit model or family = binomial (link = probit) for the probit model # Estimate logit model logitres&lt;-glm(inlf~nwifeinc+educ+exper+I(exper^2)+age+kidslt6+kidsge6, family=binomial(link=logit),data=mroz) # Summary of results: summary(logitres) ## ## Call: ## glm(formula = inlf ~ nwifeinc + educ + exper + I(exper^2) + age + ## kidslt6 + kidsge6, family = binomial(link = logit), data = mroz) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1770 -0.9063 0.4473 0.8561 2.4032 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.425452 0.860365 0.495 0.62095 ## nwifeinc -0.021345 0.008421 -2.535 0.01126 * ## educ 0.221170 0.043439 5.091 3.55e-07 *** ## exper 0.205870 0.032057 6.422 1.34e-10 *** ## I(exper^2) -0.003154 0.001016 -3.104 0.00191 ** ## age -0.088024 0.014573 -6.040 1.54e-09 *** ## kidslt6 -1.443354 0.203583 -7.090 1.34e-12 *** ## kidsge6 0.060112 0.074789 0.804 0.42154 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1029.75 on 752 degrees of freedom ## Residual deviance: 803.53 on 745 degrees of freedom ## AIC: 819.53 ## ## Number of Fisher Scoring iterations: 4 # Log likelihood value: logLik(logitres) ## &#39;log Lik.&#39; -401.7652 (df=8) # McFadden&#39;s pseudo R2: 1 - logitres$deviance/logitres$null.deviance ## [1] 0.2196814 We can also extract the fitted values from the specified model and compare them the observed predicated binomial explained variable. fit &lt;- data.frame(response = mroz$inlf, predicted = round(fitted(logitres), 0)) xtabs(~ predicted + response, data = fit) ## response ## predicted 0 1 ## 0 207 81 ## 1 118 347 If you want to interpret the estimated effects as relative odds ratios (plus their confidence intervals): exp(logitres$coefficients) ## (Intercept) nwifeinc educ exper I(exper^2) age ## 1.5302825 0.9788810 1.2475360 1.2285929 0.9968509 0.9157386 ## kidslt6 kidsge6 ## 0.2361344 1.0619557 exp(confint(logitres)) # Lets convert the confidence intervals the same way ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 0.2829827 8.2907776 ## nwifeinc 0.9625447 0.9949193 ## educ 1.1473738 1.3607465 ## exper 1.1540225 1.3092803 ## I(exper^2) 0.9948707 0.9988906 ## age 0.8894633 0.9418229 ## kidslt6 0.1567171 0.3485513 ## kidsge6 0.9175642 1.2306237 resultsTable &lt;- exp(cbind(OddsRatios = coef(logitres), confint(logitres))) ## Waiting for profiling to be done... round(resultsTable, digits = 5) ## OddsRatios 2.5 % 97.5 % ## (Intercept) 1.53028 0.28298 8.29078 ## nwifeinc 0.97888 0.96254 0.99492 ## educ 1.24754 1.14737 1.36075 ## exper 1.22859 1.15402 1.30928 ## I(exper^2) 0.99685 0.99487 0.99889 ## age 0.91574 0.88946 0.94182 ## kidslt6 0.23613 0.15672 0.34855 ## kidsge6 1.06196 0.91756 1.23062 This gives you \\(e^\\beta\\), the multiplicative change in the odds ratio for \\(y=1\\) if the covariate associated with \\(\\beta\\) increases by 1. 2.2.1 Multinomial Logit A relatively common R function that fits multinomial logit models is multinom() from package nnet. Let us use the dataset nels_small for an example of how multinom works. The variable grades in this dataset is an index, with best grades represented by lower values of grade. We try to explain the choice of a secondary institution (psechoice) only by the high school grade. The variable pschoice can take one of three values: psechoice = 1 no college, psechoice = 2 two year college psechoice = 3 four year college data(&quot;nels_small&quot;, package=&quot;PoEdata&quot;) nels.multinom &lt;- multinom(psechoice~grades, data=nels_small) ## # weights: 9 (4 variable) ## initial value 1098.612289 ## iter 10 value 875.313116 ## final value 875.313099 ## converged summary(nels.multinom) ## Call: ## multinom(formula = psechoice ~ grades, data = nels_small) ## ## Coefficients: ## (Intercept) grades ## 2 2.505273 -0.3086404 ## 3 5.770170 -0.7062468 ## ## Std. Errors: ## (Intercept) grades ## 2 0.4183944 0.05228532 ## 3 0.4043290 0.05292638 ## ## Residual Deviance: 1750.626 ## AIC: 1758.626 medGrades &lt;- median(nels_small$grades) fifthPercentileGrades &lt;- quantile(nels_small$grades, .05) newdat &lt;- data.frame(grades=c(medGrades, fifthPercentileGrades)) pred &lt;- predict(nels.multinom, newdat, &quot;probs&quot;) pred ## 1 2 3 ## 0.18101808 0.28557312 0.5334088 ## 5% 0.01781764 0.09662199 0.8855604 2.2.2 The Conditional Logit Model In the multinomial logit model all individuals faced the same external conditions and each individual’s choice is only determined by an individual’s circumstances or preferences. The conditional logit model allows for individuals to face individual-specific external conditions, such as the price of a product. Suppose we want to study the effect of price on an individual’s decision about choosing one of three brands of soft drinks: pepsi sevenup coke R offers several alternatives that allow fitting conditional logit models, one of which is the function MCMCmnl() from the package MCMCpack (others are, for instance, clogit() in the survival package and mclogit() in the mclogit package). The following code is adapted from Adkins (2014). data(&quot;cola&quot;, package=&quot;PoEdata&quot;) N &lt;- nrow(cola) N3 &lt;- N/3 price1 &lt;- cola$price[seq(1,N,by=3)] price2 &lt;- cola$price[seq(2,N,by=3)] price3 &lt;- cola$price[seq(3,N,by=3)] bchoice &lt;- rep(&quot;1&quot;, N3) for (j in 1:N3){ if(cola$choice[3*j-1]==1) bchoice[j] &lt;- &quot;2&quot; if(cola$choice[3*j]==1) bchoice[j] &lt;- &quot;3&quot; } cola.clogit &lt;- MCMCmnl(bchoice ~ choicevar(price1, &quot;b2&quot;, &quot;1&quot;)+ choicevar(price2, &quot;b2&quot;, &quot;2&quot;)+ choicevar(price3, &quot;b2&quot;, &quot;3&quot;), baseline=&quot;3&quot;, mcmc.method=&quot;IndMH&quot;) ## Calculating MLEs and large sample var-cov matrix. ## This may take a moment... ## Inverting Hessian to get large sample var-cov matrix. The output from function multinom gives coefficient estimates for each level of the response variable psechoice, except for the first level, which is the benchmark. We can calculate the probability that individual \\(i\\) chooses pepsi and sevenup for some given values of the prices that individual \\(i\\). Coke is not shown as it is the baseline choice. sclogit &lt;- summary(cola.clogit) tabMCMC &lt;- as.data.frame(sclogit$statistics)[,1:2] row.names(tabMCMC)&lt;- c(&quot;b2&quot;,&quot;b11&quot;,&quot;b12&quot;) kable(tabMCMC, digits=4, align=&quot;c&quot;, caption=&quot;Conditional logit estimates for the &#39;cola&#39; problem&quot;) Table 2.1: Conditional logit estimates for the ‘cola’ problem Mean SD b2 -2.2991 0.1382 b11 0.2839 0.0610 b12 0.1037 0.0621 We can also estimate the probabilities of individual \\(i\\) choosing Pepsi or Sevenup for a given price. pPepsi &lt;- 1 pSevenup &lt;- 1.25 pCoke &lt;- 1.10 b13 &lt;- 0 b2 &lt;- tabMCMC$Mean[1] b11 &lt;- tabMCMC$Mean[2] b12 &lt;- tabMCMC$Mean[3] # The probability that individual i chooses Pepsi: PiPepsi &lt;- exp(b11+b2*pPepsi)/ (exp(b11+b2*pPepsi)+exp(b12+b2*pSevenup)+ exp(b13+b2*pCoke)) # The probability that individual i chooses Sevenup: PiSevenup &lt;- exp(b12+b2*pSevenup)/ (exp(b11+b2*pPepsi)+exp(b12+b2*pSevenup)+ exp(b13+b2*pCoke)) # The probability that individual i chooses Coke: PiCoke &lt;- 1-PiPepsi-PiSevenup The calculatred probabilities are: \\(p_{i,pepsi}=0.483\\) \\(p_{i,sevenup}=0.227\\) \\(p_{i,coke}=0.289\\) The three probabilities are different for different individuals because different individuals face different prices; in a more complex model other regressors may be included, some of which may reflect individual characteristics. 2.2.3 Ordered Choice Models The order of choices in these models is meaningful, unlike the multinomial and conditional logit model we have studied so far. The following example explains the choice of higher education, when the choice variable is psechoice and the only regressor is grades; the dataset, nels_small , is already known to us. The R package MCMCpack is again used here, with its function MCMCoprobit(). nels.oprobit &lt;- MCMCoprobit(psechoice ~ grades, data=nels_small, mcmc=10000) sOprobit &lt;- summary(nels.oprobit) tabOprobit &lt;- sOprobit$statistics[, 1:2] kable(tabOprobit, digits=4, align=&quot;c&quot;, caption=&quot;Ordered probit estimates for the &#39;nels&#39; problem&quot;) Table 2.2: Ordered probit estimates for the ‘nels’ problem Mean SD (Intercept) 2.9542 0.1478 grades -0.3074 0.0193 gamma2 0.8616 0.0487 The results from MCMCoprobit can be translated into the textbook notations as follows: \\(\\mu_1\\) =− (Intercept) \\(\\beta\\) = grades \\(\\mu_2\\) = gamma2 − (Intercept) The probabilities for each choice can be calculated as in the next code fragment: mu1 &lt;- -tabOprobit[1] b &lt;- tabOprobit[2] mu2 &lt;- tabOprobit[3]-tabOprobit[1] xGrade &lt;- c(mean(nels_small$grades), quantile(nels_small$grades, 0.05)) # Probabilities: prob1 &lt;- pnorm(mu1-b*xGrade) prob2 &lt;- pnorm(mu2-b*xGrade)-pnorm(mu1-b*xGrade) prob3 &lt;- 1-pnorm(mu2-b*xGrade) # Marginal effects: Dp1DGrades &lt;- -pnorm(mu1-b*xGrade)*b Dp2DGrades &lt;- (pnorm(mu1-b*xGrade)-pnorm(mu2-b*xGrade))*b Dp3DGrades &lt;- pnorm(mu2-b*xGrade)*b For instance, the marginal effect of grades on the probability of attending a four-year college for a student with average grade and for a student in the top 5 percent are, respectively, -0.143 and -0.031. 2.2.4 Probit model par(mfrow=c(1,2)) x &lt;- seq(-3,3, .2) plot(x, pnorm(x), type=&quot;l&quot;, xlab=&quot;b1+b2x&quot;, ylab=&quot;P[y=1]&quot;) plot(x, dnorm(x), type=&quot;l&quot;) Figure 2.1: The shape of the probit function is the standard normal distribution detach(&quot;package:PoEdata&quot;, unload=TRUE) data(&quot;mroz&quot;, package=&#39;wooldridge&#39;) attach(mroz) ## The following objects are masked from bwght: ## ## faminc, fatheduc, motheduc ## The following object is masked from gpa1: ## ## age # Estimate probit model probitres&lt;-glm(inlf~nwifeinc+educ+exper+I(exper^2)+age+kidslt6+kidsge6, family=binomial(link=probit),data=mroz) # Summary of results: summary(probitres) ## ## Call: ## glm(formula = inlf ~ nwifeinc + educ + exper + I(exper^2) + age + ## kidslt6 + kidsge6, family = binomial(link = probit), data = mroz) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2156 -0.9151 0.4315 0.8653 2.4553 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.2700736 0.5080782 0.532 0.59503 ## nwifeinc -0.0120236 0.0049392 -2.434 0.01492 * ## educ 0.1309040 0.0253987 5.154 2.55e-07 *** ## exper 0.1233472 0.0187587 6.575 4.85e-11 *** ## I(exper^2) -0.0018871 0.0005999 -3.145 0.00166 ** ## age -0.0528524 0.0084624 -6.246 4.22e-10 *** ## kidslt6 -0.8683247 0.1183773 -7.335 2.21e-13 *** ## kidsge6 0.0360056 0.0440303 0.818 0.41350 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1029.7 on 752 degrees of freedom ## Residual deviance: 802.6 on 745 degrees of freedom ## AIC: 818.6 ## ## Number of Fisher Scoring iterations: 4 # Log likelihood value: logLik(probitres) ## &#39;log Lik.&#39; -401.3022 (df=8) # McFadden&#39;s pseudo R2: 1 - probitres$deviance/probitres$null.deviance ## [1] 0.2205805 Take a boostrap sample for the 95% confidence interval for each parameter. data(&quot;mroz&quot;, package=&#39;wooldridge&#39;) boot_probitres &lt;- mroz %&gt;% bootstrap(500) %&gt;% do(tidy(glm(inlf~nwifeinc+educ+exper+I(exper^2)+age+kidslt6+kidsge6, family=binomial(link=probit),.))) boot_probitres %&gt;% group_by(term) %&gt;% dplyr::summarise(low = quantile(estimate, .025), high = quantile(estimate, .0975)) term low high (Intercept) 0.27&nbsp;&nbsp;&nbsp; 0.27&nbsp;&nbsp;&nbsp; age -0.0529&nbsp; -0.0529&nbsp; educ 0.131&nbsp;&nbsp; 0.131&nbsp;&nbsp; exper 0.123&nbsp;&nbsp; 0.123&nbsp;&nbsp; I(exper^2) -0.00189 -0.00189 kidsge6 0.036&nbsp;&nbsp; 0.036&nbsp;&nbsp; kidslt6 -0.868&nbsp;&nbsp; -0.868&nbsp;&nbsp; nwifeinc -0.012&nbsp;&nbsp; -0.012&nbsp;&nbsp; 2.2.5 Inference We can implement the test for overall significance for the probit model using both manual and automatic calculations. # Test of overall significance: # Manual calculation of the LR test statistic: probitres$null.deviance - probitres$deviance ## [1] 227.142 # Automatic calculations including p-values,...: lrtest(probitres) #Df LogLik Df Chisq Pr(&gt;Chisq) 8 -401 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 -515 -7 227 2.01e-45 # Test of H0: experience and age are irrelevant restr &lt;- glm(inlf~nwifeinc+educ+ kidslt6+kidsge6, family=binomial(link=logit),data=mroz) lrtest(restr,probitres) #Df LogLik Df Chisq Pr(&gt;Chisq) 5 -465 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8 -401 3 127 2.12e-27 2.2.6 Predictions The command predict() can calculate predicted values for the estimation sample or arbitrary sets of regressor values. We can calculate # predictions for two &quot;extreme&quot; women: xpred &lt;- list(nwifeinc=c(100,0),educ=c(5,17),exper=c(0,30), age=c(20,52),kidslt6=c(2,0),kidsge6=c(0,0)) # Predictions from linear probability, probit and logit model: predict(linprob, xpred,type = &quot;response&quot;) ## 1 2 ## -0.4104582 1.0428084 predict(logitres, xpred,type = &quot;response&quot;) ## 1 2 ## 0.005218002 0.950049117 predict(probitres,xpred,type = &quot;response&quot;) ## 1 2 ## 0.001065043 0.959869044 # Simulated data set.seed(12345) y &lt;- rbinom(100, 1, 0.5) x &lt;- rnorm(100) + 2 * y # Estimation linpr.res &lt;- lm(y~x) logit.res &lt;-glm(y ~x, family = binomial(link = logit)) probit.res &lt;-glm(y ~x, family = binomial(link = probit)) # Prediction from a regular grid of x values xp &lt;- seq(from= min(x), to=max(x), length=50) linpr.p &lt;- predict(linpr.res, list(x = xp), type = &quot;response&quot;) logit.p &lt;- predict(logit.res, list(x = xp), type = &quot;response&quot;) probit.p &lt;- predict(probit.res, list(x = xp), type = &quot;response&quot;) plot(x,y) lines(xp, linpr.p, lwd=2, lty = 1) lines(xp, logit.p, lwd=2, lty = 2) lines(xp, probit.p, lwd=1, lty = 1) legend(&quot;topleft&quot;, c(&quot;linear prob.&quot;, &quot;logit&quot;, &quot;probit&quot;), lwd = c(2,2,1), lty = c(1,2,1)) Figure 2.2: Predictions from binary response models (simulated data) 2.2.7 Marginal (partial) effects Several packages provide estimates of marginal effects for different types of models. Among these are car, alr3, mfx, erer, among others. Unfortunately, none of these packages implement marginal effects correctly (i.e., correctly account for interrelated variables such as interaction terms (e.g., a:b) or power terms (e.g., I(a^2)) and the packages all implement quite different interfaces for different types of models. The margins and prediction packages are a combined effort to calculate marginal effects that include complex terms and provide a uniform interface for doing those calculations. To know how much a variable influences the labour force participation, one has to use margins() command: effects_logit_participation &lt;- margins(logitres) summary(effects_logit_participation) factor AME SE z p lower upper age -0.0157&nbsp; 0.00238 -6.6&nbsp;&nbsp; 4.04e-11 -0.0204&nbsp; -0.0111&nbsp;&nbsp; educ 0.0395&nbsp; 0.00729 5.41&nbsp; 6.15e-08 0.0252&nbsp; 0.0538&nbsp;&nbsp; exper 0.0254&nbsp; 0.00224 11.4&nbsp;&nbsp; 5.99e-30 0.021&nbsp;&nbsp; 0.0298&nbsp;&nbsp; kidsge6 0.0107&nbsp; 0.0133&nbsp; 0.805 0.421&nbsp;&nbsp;&nbsp; -0.0154&nbsp; 0.0369&nbsp;&nbsp; kidslt6 -0.258&nbsp;&nbsp; 0.0319&nbsp; -8.07&nbsp; 7.05e-16 -0.32&nbsp;&nbsp;&nbsp; -0.195&nbsp;&nbsp;&nbsp; nwifeinc -0.00381 0.00148 -2.57&nbsp; 0.0101&nbsp;&nbsp; -0.00672 -0.000906 plot(effects_logit_participation) If one desires subgroup effects, simply pass a subset of data to the data argument: summary(margins(logitres, data = subset(mroz, kidslt6 == 0))) # no kids &lt; 6 years factor AME SE z p lower upper age -0.0156&nbsp; 0.00234 -6.64&nbsp; 3.04e-11 -0.0202&nbsp; -0.011&nbsp;&nbsp;&nbsp; educ 0.0391&nbsp; 0.00726 5.39&nbsp; 7.13e-08 0.0249&nbsp; 0.0534&nbsp;&nbsp; exper 0.0246&nbsp; 0.00212 11.6&nbsp;&nbsp; 4.23e-31 0.0204&nbsp; 0.0287&nbsp;&nbsp; kidsge6 0.0106&nbsp; 0.0132&nbsp; 0.807 0.42&nbsp;&nbsp;&nbsp;&nbsp; -0.0152&nbsp; 0.0365&nbsp;&nbsp; kidslt6 -0.255&nbsp;&nbsp; 0.033&nbsp;&nbsp; -7.73&nbsp; 1.09e-14 -0.32&nbsp;&nbsp;&nbsp; -0.191&nbsp;&nbsp;&nbsp; nwifeinc -0.00378 0.00147 -2.57&nbsp; 0.0102&nbsp;&nbsp; -0.00666 -0.000894 ggplot(data = summary(effects_logit_participation)) + geom_point(aes(factor, AME)) + geom_errorbar(aes(x = factor, ymin = lower, ymax = upper)) + geom_hline(yintercept = 0, color=&quot;lightgrey&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45)) Figure 2.3: Logit effect plot You can also extract the marginal effects of a single variable, with dydx: head(dydx(mroz, logitres, &quot;educ&quot;)) dydx_educ 0.0464 0.0416 0.0463 0.0384 0.0538 0.0335 The function cplot() provides the commonly needed visual summaries of predictions or average marginal effects conditional on a covariate. cplot(logitres, x = &quot;educ&quot;, se.type = &quot;shade&quot;) ## xvals yvals upper lower ## 1 5.0 0.2549509 0.3787666 0.1311353 ## 2 5.5 0.2765192 0.3988913 0.1541471 ## 3 6.0 0.2991794 0.4190899 0.1792689 ## 4 6.5 0.3228678 0.4392936 0.2064421 ## 5 7.0 0.3475019 0.4594481 0.2355556 ## 6 7.5 0.3729801 0.4795186 0.2664417 ## 7 8.0 0.3991836 0.4994946 0.2988726 ## 8 8.5 0.4259774 0.5193953 0.3325594 ## 9 9.0 0.4532129 0.5392750 0.3671508 ## 10 9.5 0.4807315 0.5592299 0.4022331 ## 11 10.0 0.5083674 0.5794045 0.4373304 ## 12 10.5 0.5359524 0.5999963 0.4719084 ## 13 11.0 0.5633190 0.6212490 0.5053891 ## 14 11.5 0.5903055 0.6434188 0.5371922 ## 15 12.0 0.6167588 0.6666929 0.5668248 ## 16 12.5 0.6425384 0.6910722 0.5940047 ## 17 13.0 0.6675187 0.7162931 0.6187444 ## 18 13.5 0.6915913 0.7418704 0.6413122 ## 19 14.0 0.7146659 0.7672368 0.6620950 ## 20 14.5 0.7366715 0.7918749 0.6814681 Figure 2.4: Marginal effects for logit model 2.3 Count data: The Poisson Regression Model Instead of just 0/1-coded binary data, count data can take any non-negative integer \\(0, 1, 2, \\dots\\). If they take very large numbers (like the number of students in a school), they can be approximated reasonably well as contiouns variables in a linear models and estimated using OLS. If the numbers are relativly small, this approximation might not work well. The Poisson regression model is the most basic and convenient model explicitly model explicitly designed for count data. Poisson regression models can be estimated in R via the glm() function with the specification family= poisson Estimating the model with quasipoisson is to adjust for potential vioalations of the Poisson distribution. data(crime1, package=&#39;wooldridge&#39;) # Estimate linear model lm.res &lt;- lm(narr86~pcnv+avgsen+tottime+ptime86+qemp86+inc86+ black+hispan+born60, data=crime1) # Estimate Poisson model Poisson.res &lt;- glm(narr86~pcnv+avgsen+tottime+ptime86+qemp86+inc86+ black+hispan+born60, data=crime1, family=poisson) # Quasi-Poisson model QPoisson.res&lt;- glm(narr86~pcnv+avgsen+tottime+ptime86+qemp86+inc86+ black+hispan+born60, data=crime1, family=quasipoisson) stargazer(lm.res,Poisson.res,QPoisson.res,type=&quot;text&quot;,keep.stat=&quot;n&quot;) ## ## ================================================== ## Dependent variable: ## ------------------------------------- ## narr86 ## OLS Poisson glm: quasipoisson ## link = log ## (1) (2) (3) ## -------------------------------------------------- ## pcnv -0.132*** -0.402*** -0.402*** ## (0.040) (0.085) (0.105) ## ## avgsen -0.011 -0.024 -0.024 ## (0.012) (0.020) (0.025) ## ## tottime 0.012 0.024* 0.024 ## (0.009) (0.015) (0.018) ## ## ptime86 -0.041*** -0.099*** -0.099*** ## (0.009) (0.021) (0.025) ## ## qemp86 -0.051*** -0.038 -0.038 ## (0.014) (0.029) (0.036) ## ## inc86 -0.001*** -0.008*** -0.008*** ## (0.0003) (0.001) (0.001) ## ## black 0.327*** 0.661*** 0.661*** ## (0.045) (0.074) (0.091) ## ## hispan 0.194*** 0.500*** 0.500*** ## (0.040) (0.074) (0.091) ## ## born60 -0.022 -0.051 -0.051 ## (0.033) (0.064) (0.079) ## ## Constant 0.577*** -0.600*** -0.600*** ## (0.038) (0.067) (0.083) ## ## -------------------------------------------------- ## Observations 2,725 2,725 2,725 ## ================================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 By construction, the parameter estiamtes are the same but the standard errors are larger for the QMLE. 2.3.1 Corner Solution Response: The Tobit Model Corner solutions describe situations where the variable of interest is continous but restricted in range. Typically, it cannot be negative. The package censReg offers the command censReg() for estimating a Tobit model. We have already estimated labor supply mdoeld for the women in the dataset mroz, ignoring the fact that the hours worked is necessarily non-negative. # Estimate Tobit model using censReg: TobitRes &lt;- censReg(hours~nwifeinc+educ+exper+I(exper^2)+ age+kidslt6+kidsge6, data=mroz ) summary(TobitRes) ## ## Call: ## censReg(formula = hours ~ nwifeinc + educ + exper + I(exper^2) + ## age + kidslt6 + kidsge6, data = mroz) ## ## Observations: ## Total Left-censored Uncensored Right-censored ## 753 325 428 0 ## ## Coefficients: ## Estimate Std. error t value Pr(&gt; t) ## (Intercept) 965.30528 446.43613 2.162 0.030599 * ## nwifeinc -8.81424 4.45910 -1.977 0.048077 * ## educ 80.64561 21.58324 3.736 0.000187 *** ## exper 131.56430 17.27939 7.614 2.66e-14 *** ## I(exper^2) -1.86416 0.53766 -3.467 0.000526 *** ## age -54.40501 7.41850 -7.334 2.24e-13 *** ## kidslt6 -894.02174 111.87803 -7.991 1.34e-15 *** ## kidsge6 -16.21800 38.64139 -0.420 0.674701 ## logSigma 7.02289 0.03706 189.514 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Newton-Raphson maximisation, 7 iterations ## Return code 1: gradient close to zero ## Log-likelihood: -3819.095 on 9 Df # Partial Effects at the average x: margEff(TobitRes) ## nwifeinc educ exper I(exper^2) age kidslt6 ## -5.326442 48.734094 79.504232 -1.126509 -32.876918 -540.256831 ## kidsge6 ## -9.800526 Another alternative for estimating Tobit models is the command survreg from the package survival. It is less straightforward to use but more flexible. # Estimate Tobit model using survreg: res &lt;- survreg(Surv(hours, hours&gt;0, type=&quot;left&quot;) ~ nwifeinc+educ+exper+ I(exper^2)+age+kidslt6+kidsge6, data=mroz, dist=&quot;gaussian&quot;) summary(res) ## ## Call: ## survreg(formula = Surv(hours, hours &gt; 0, type = &quot;left&quot;) ~ nwifeinc + ## educ + exper + I(exper^2) + age + kidslt6 + kidsge6, data = mroz, ## dist = &quot;gaussian&quot;) ## Value Std. Error z p ## (Intercept) 965.3053 446.4361 2.16 0.03060 ## nwifeinc -8.8142 4.4591 -1.98 0.04808 ## educ 80.6456 21.5832 3.74 0.00019 ## exper 131.5643 17.2794 7.61 2.7e-14 ## I(exper^2) -1.8642 0.5377 -3.47 0.00053 ## age -54.4050 7.4185 -7.33 2.2e-13 ## kidslt6 -894.0217 111.8780 -7.99 1.3e-15 ## kidsge6 -16.2180 38.6414 -0.42 0.67470 ## Log(scale) 7.0229 0.0371 189.51 &lt; 2e-16 ## ## Scale= 1122 ## ## Gaussian distribution ## Loglik(model)= -3819.1 Loglik(intercept only)= -3954.9 ## Chisq= 271.59 on 7 degrees of freedom, p= 7e-55 ## Number of Newton-Raphson Iterations: 4 ## n= 753 2.4 Censored and Truncated Regression Models Censored regression models are closely related to Tobit models. In the basic Tobit model we observe \\(y = y^*\\) in the “uncensored” cases with \\(y^* &gt; 0\\) and we only know an upper bound for \\(y^* \\le 0\\) if we observe $y 0 = $. In this example we are are interested in criminal prognosis of individuals released from prison to reoffend. data(recid, package=&#39;wooldridge&#39;) # Define Dummy for UNcensored observations recid$uncensored &lt;- recid$cens==0 # Estimate censored regression model: res&lt;-survreg(Surv(log(durat),uncensored, type=&quot;right&quot;) ~ workprg+priors+ tserved+felon+alcohol+drugs+black+married+educ+age, data=recid, dist=&quot;gaussian&quot;) # Output: summary(res) ## ## Call: ## survreg(formula = Surv(log(durat), uncensored, type = &quot;right&quot;) ~ ## workprg + priors + tserved + felon + alcohol + drugs + black + ## married + educ + age, data = recid, dist = &quot;gaussian&quot;) ## Value Std. Error z p ## (Intercept) 4.099386 0.347535 11.80 &lt; 2e-16 ## workprg -0.062572 0.120037 -0.52 0.6022 ## priors -0.137253 0.021459 -6.40 1.6e-10 ## tserved -0.019331 0.002978 -6.49 8.5e-11 ## felon 0.443995 0.145087 3.06 0.0022 ## alcohol -0.634909 0.144217 -4.40 1.1e-05 ## drugs -0.298160 0.132736 -2.25 0.0247 ## black -0.542718 0.117443 -4.62 3.8e-06 ## married 0.340684 0.139843 2.44 0.0148 ## educ 0.022920 0.025397 0.90 0.3668 ## age 0.003910 0.000606 6.45 1.1e-10 ## Log(scale) 0.593586 0.034412 17.25 &lt; 2e-16 ## ## Scale= 1.81 ## ## Gaussian distribution ## Loglik(model)= -1597.1 Loglik(intercept only)= -1680.4 ## Chisq= 166.74 on 10 degrees of freedom, p= 1.3e-30 ## Number of Newton-Raphson Iterations: 4 ## n= 1445 2.4.1 The Heckman, or Sample Selection Model The models are useful when the sample selection is not random, but whehter an individual is in the sample depends on individual characteristics. For example, when studying wage determination for married women, some women are not in the labour force, therefore their wages are zero. The Heckit procedure involves two steps, estimating both the selection equation and the equation of interest. Function selection() in the sampleSelection package performs both steps; therefore, it needs both equations among its arguments. (The selection equation is, in fact, a probit model.) data(&quot;mroz&quot;, package=&#39;wooldridge&#39;) wage.heckit &lt;- selection(inlf~age+educ+I(kidslt6+kidsge6)+mtr, log(wage)~educ+exper, data=mroz, method=&quot;ml&quot;) summary(wage.heckit) ## -------------------------------------------- ## Tobit 2 model (sample selection model) ## Maximum Likelihood estimation ## Newton-Raphson maximisation, 4 iterations ## Return code 2: successive function values within tolerance limit ## Log-Likelihood: -913.5131 ## 753 observations (325 censored and 428 observed) ## 10 free parameters (df = 743) ## Probit selection equation: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.53798 0.61889 2.485 0.0132 * ## age -0.01346 0.00603 -2.232 0.0259 * ## educ 0.06278 0.02180 2.879 0.0041 ** ## I(kidslt6 + kidsge6) -0.05108 0.03276 -1.559 0.1194 ## mtr -2.20864 0.54620 -4.044 5.81e-05 *** ## Outcome equation: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.646221 0.235569 2.743 0.00623 ** ## educ 0.066462 0.016573 4.010 6.68e-05 *** ## exper 0.011972 0.004085 2.931 0.00348 ** ## Error terms: ## Estimate Std. Error t value Pr(&gt;|t|) ## sigma 0.84112 0.04302 19.55 &lt;2e-16 *** ## rho -0.82768 0.03911 -21.16 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## -------------------------------------------- References "],
["timeseries.html", "Chapter 3 Time Series 3.1 Seasonal decomposition of time Series 3.2 Differences and lags 3.3 Exponential smoothing and forecasting 3.4 Finite Distributed Lags Model 3.5 ARIMA models", " Chapter 3 Time Series To load the dataset and necessary functions: # This function 1. checks if the packages are installed. 2. It installs the packages if they were not in the list of installed packages. 3. It loads the packages into the workspace # devtools::install_github(&quot;ccolonescu/PoEdata&quot;) PACKAGES&lt;-c(&quot;PoEdata&quot;, # R data sets for &quot;Principles of Econometrics&quot; by Hill, Griffiths, an d Lim, 4e, Wiley. https://github.com/ccolonescu/PoEdata &quot;wooldridge&quot;, # Wooldrige Datasets &quot;tidyverse&quot;, # for data manipulation and ggplots &quot;broom&quot;, # Tidy regression output &quot;car&quot;, # Companion to applied regression &quot;knitr&quot;, # knit functions # &quot;kableExtra&quot;, # extended knit functions for objects exported from other packages &quot;huxtable&quot;, # Regression tables, broom compatible &quot;stargazer&quot;, # Regression tables &quot;AER&quot;, # Functions, data sets, examples, demos, and vignettes for the book Christian Kleiber and Achim Zeileis (2008) &quot;MASS&quot;, # Functions and datasets to support Venables and Ripley, &quot;Modern Applied Statistics with S&quot; &quot;eurostat&quot;, # query data from eurostat &quot;stats&quot;, &quot;dynlm&quot;, # time series regression &quot;fUnitRoots&quot;, # unit root testing &quot;uroot&quot;, &quot;urca&quot;, # unit root testing &quot;forecast&quot;, # forecast functions &quot;astsa&quot;, # applied statistcal time series analysis &quot;modelr&quot;, # model simulation/ boostraping the modern way &quot;magrittr&quot;) # pipes inst&lt;-match(PACKAGES, .packages(all=TRUE)) need&lt;-which(is.na(inst)) if (length(need)&gt;0) install.packages(PACKAGES[need]) lapply(PACKAGES, require, character.only=T) In this tutorial we discuss various time series objects in R. We will cover objects time series, zoo and xts. Base R has limited functionality for handling time series data. There are several R packages with functions for creating, manipulating and visualizing time date and time series objects. The packages that we will use are lubridate, quantmod, timeDate, timeSeries, zoo, xts, xtsExtra, tsibble. We download monthly CPI from Eurostat #Since 1996 cpi&lt;- get_eurostat(&quot;prc_hicp_midx&quot;, filters = list(geo = &quot;EE&quot;, coicop=&quot;CP00&quot;, unit=&quot;I15&quot;)) %&gt;% dplyr::filter(time&gt;=as.Date(&quot;1992-01-01&quot;)) %&gt;% dplyr::select(time, values) %&gt;% rename(cpi=values, Date=time) plot(cpi, type=&quot;l&quot;) The data.frame object is not designed to work efficiently with time series data. The default plotting methods in R are not designed for handling time series data. Depending on your data set and the purpose of your analysis you may need more flexible objects. object package explanation fts from package fts An R interfact to tslib (a time series library in C++) its its for handling irregular time series irts tseries irregular time‐series objects timeSeries timeSeries Rmetrics package of time series tools and utilities ti tis Functions for time indexes and time indexed series ts, mts stats Regularly spaced time series objects zoo zoo indexed totally ordered observations which includes irregular time series xts xts Extension of the zoo class For simple regularly spaced time series (e.g monthly, quarterly and yearly data), you may use ts object from stats package. For irregular time series data (e.g. daily data with gaps), you may prefer zoo or irtsobject if date is important for you. If the date does not matter, then simple time series object tsshould be enough. head(cpi) Date cpi 1996-01-01 42.9 1996-02-01 44.2 1996-03-01 45&nbsp;&nbsp; 1996-04-01 45.7 1996-05-01 46&nbsp;&nbsp; 1996-06-01 46.2 cpits &lt;- ts(cpi$cpi, start=c(1996,1), frequency = 12) #check structure str(cpits) ## Time-Series [1:273] from 1996 to 2019: 42.9 44.2 45 45.7 46 ... plot(cpits) #Some functions to characterise time series #Position in the cycle cycle(cpits) ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1996 1 2 3 4 5 6 7 8 9 10 11 12 ## 1997 1 2 3 4 5 6 7 8 9 10 11 12 ## 1998 1 2 3 4 5 6 7 8 9 10 11 12 ## 1999 1 2 3 4 5 6 7 8 9 10 11 12 ## 2000 1 2 3 4 5 6 7 8 9 10 11 12 ## 2001 1 2 3 4 5 6 7 8 9 10 11 12 ## 2002 1 2 3 4 5 6 7 8 9 10 11 12 ## 2003 1 2 3 4 5 6 7 8 9 10 11 12 ## 2004 1 2 3 4 5 6 7 8 9 10 11 12 ## 2005 1 2 3 4 5 6 7 8 9 10 11 12 ## 2006 1 2 3 4 5 6 7 8 9 10 11 12 ## 2007 1 2 3 4 5 6 7 8 9 10 11 12 ## 2008 1 2 3 4 5 6 7 8 9 10 11 12 ## 2009 1 2 3 4 5 6 7 8 9 10 11 12 ## 2010 1 2 3 4 5 6 7 8 9 10 11 12 ## 2011 1 2 3 4 5 6 7 8 9 10 11 12 ## 2012 1 2 3 4 5 6 7 8 9 10 11 12 ## 2013 1 2 3 4 5 6 7 8 9 10 11 12 ## 2014 1 2 3 4 5 6 7 8 9 10 11 12 ## 2015 1 2 3 4 5 6 7 8 9 10 11 12 ## 2016 1 2 3 4 5 6 7 8 9 10 11 12 ## 2017 1 2 3 4 5 6 7 8 9 10 11 12 ## 2018 1 2 3 4 5 6 7 8 9 #to check frequency frequency(cpits) ## [1] 12 #How time is represented? As a fraction of the year! time(cpits) ## Jan Feb Mar Apr May Jun Jul ## 1996 1996.000 1996.083 1996.167 1996.250 1996.333 1996.417 1996.500 ## 1997 1997.000 1997.083 1997.167 1997.250 1997.333 1997.417 1997.500 ## 1998 1998.000 1998.083 1998.167 1998.250 1998.333 1998.417 1998.500 ## 1999 1999.000 1999.083 1999.167 1999.250 1999.333 1999.417 1999.500 ## 2000 2000.000 2000.083 2000.167 2000.250 2000.333 2000.417 2000.500 ## 2001 2001.000 2001.083 2001.167 2001.250 2001.333 2001.417 2001.500 ## 2002 2002.000 2002.083 2002.167 2002.250 2002.333 2002.417 2002.500 ## 2003 2003.000 2003.083 2003.167 2003.250 2003.333 2003.417 2003.500 ## 2004 2004.000 2004.083 2004.167 2004.250 2004.333 2004.417 2004.500 ## 2005 2005.000 2005.083 2005.167 2005.250 2005.333 2005.417 2005.500 ## 2006 2006.000 2006.083 2006.167 2006.250 2006.333 2006.417 2006.500 ## 2007 2007.000 2007.083 2007.167 2007.250 2007.333 2007.417 2007.500 ## 2008 2008.000 2008.083 2008.167 2008.250 2008.333 2008.417 2008.500 ## 2009 2009.000 2009.083 2009.167 2009.250 2009.333 2009.417 2009.500 ## 2010 2010.000 2010.083 2010.167 2010.250 2010.333 2010.417 2010.500 ## 2011 2011.000 2011.083 2011.167 2011.250 2011.333 2011.417 2011.500 ## 2012 2012.000 2012.083 2012.167 2012.250 2012.333 2012.417 2012.500 ## 2013 2013.000 2013.083 2013.167 2013.250 2013.333 2013.417 2013.500 ## 2014 2014.000 2014.083 2014.167 2014.250 2014.333 2014.417 2014.500 ## 2015 2015.000 2015.083 2015.167 2015.250 2015.333 2015.417 2015.500 ## 2016 2016.000 2016.083 2016.167 2016.250 2016.333 2016.417 2016.500 ## 2017 2017.000 2017.083 2017.167 2017.250 2017.333 2017.417 2017.500 ## 2018 2018.000 2018.083 2018.167 2018.250 2018.333 2018.417 2018.500 ## Aug Sep Oct Nov Dec ## 1996 1996.583 1996.667 1996.750 1996.833 1996.917 ## 1997 1997.583 1997.667 1997.750 1997.833 1997.917 ## 1998 1998.583 1998.667 1998.750 1998.833 1998.917 ## 1999 1999.583 1999.667 1999.750 1999.833 1999.917 ## 2000 2000.583 2000.667 2000.750 2000.833 2000.917 ## 2001 2001.583 2001.667 2001.750 2001.833 2001.917 ## 2002 2002.583 2002.667 2002.750 2002.833 2002.917 ## 2003 2003.583 2003.667 2003.750 2003.833 2003.917 ## 2004 2004.583 2004.667 2004.750 2004.833 2004.917 ## 2005 2005.583 2005.667 2005.750 2005.833 2005.917 ## 2006 2006.583 2006.667 2006.750 2006.833 2006.917 ## 2007 2007.583 2007.667 2007.750 2007.833 2007.917 ## 2008 2008.583 2008.667 2008.750 2008.833 2008.917 ## 2009 2009.583 2009.667 2009.750 2009.833 2009.917 ## 2010 2010.583 2010.667 2010.750 2010.833 2010.917 ## 2011 2011.583 2011.667 2011.750 2011.833 2011.917 ## 2012 2012.583 2012.667 2012.750 2012.833 2012.917 ## 2013 2013.583 2013.667 2013.750 2013.833 2013.917 ## 2014 2014.583 2014.667 2014.750 2014.833 2014.917 ## 2015 2015.583 2015.667 2015.750 2015.833 2015.917 ## 2016 2016.583 2016.667 2016.750 2016.833 2016.917 ## 2017 2017.583 2017.667 2017.750 2017.833 2017.917 ## 2018 2018.583 2018.667 #average difference between time units (0.08333 years or =1/12) deltat(cpits) ## [1] 0.08333333 3.1 Seasonal decomposition of time Series We use stl from package stats to decompose time series. See the options of the command for details. In addition, we apply the command acf2 from package astsa to illustrate how we can use the components. #Fixed seasonal components, s.window - seasonal window plot(stl(cpits, s.window=&quot;periodic&quot;)) #vertical bars on the right reflect the scale of the components #smoothed seasonal component plot(stl(cpits, s.window=7)) #remainder component seem to follow AR process #We can make the stl components as object and analyse these separately cpicomponents &lt;- stl(cpits, s.window=7) #remainder plot(cpicomponents$time.series[,3]) #Autocorrelation function of the remainder acf2(cpicomponents$time.series[,3]) ## ACF PACF ## [1,] 0.75 0.75 ## [2,] 0.51 -0.12 ## [3,] 0.32 -0.03 ## [4,] 0.21 0.01 ## [5,] 0.15 0.04 ## [6,] 0.13 0.04 ## [7,] 0.12 0.02 ## [8,] 0.06 -0.11 ## [9,] 0.02 0.01 ## [10,] -0.06 -0.11 ## [11,] -0.22 -0.28 ## [12,] -0.43 -0.31 ## [13,] -0.43 0.21 ## [14,] -0.39 -0.07 ## [15,] -0.33 -0.06 ## [16,] -0.25 0.08 ## [17,] -0.20 -0.02 ## [18,] -0.17 0.01 ## [19,] -0.18 -0.01 ## [20,] -0.14 0.03 ## [21,] -0.10 0.07 ## [22,] -0.13 -0.21 ## [23,] -0.12 -0.11 ## [24,] -0.09 -0.15 ## [25,] -0.03 0.12 ## [26,] 0.05 0.03 ## [27,] 0.10 -0.07 ## [28,] 0.09 -0.02 ## [29,] 0.05 -0.03 ## [30,] 0.02 -0.04 ## [31,] 0.02 0.01 ## [32,] 0.01 -0.01 ## [33,] 0.00 0.06 ## [34,] 0.01 -0.15 ## [35,] 0.04 -0.01 ## [36,] 0.05 -0.12 ## [37,] 0.08 0.14 ## [38,] 0.06 0.00 ## [39,] 0.02 -0.06 ## [40,] -0.02 -0.03 ## [41,] -0.01 0.01 ## [42,] 0.02 -0.01 ## [43,] 0.03 0.03 ## [44,] 0.03 -0.09 ## [45,] 0.00 0.02 ## [46,] 0.01 -0.04 ## [47,] -0.02 -0.15 ## [48,] -0.03 -0.07 3.2 Differences and lags We can apply standard functions to time series, such as differences, seasonal differences, logs, etc. Let’s use CPI to calculate monthly and annual inflation rates. head(cpi) Date cpi 1996-01-01 42.9 1996-02-01 44.2 1996-03-01 45&nbsp;&nbsp; 1996-04-01 45.7 1996-05-01 46&nbsp;&nbsp; 1996-06-01 46.2 cpits &lt;- ts(cpi$cpi, start=c(1996,1), frequency = 12) #check structure str(cpits) ## Time-Series [1:273] from 1996 to 2019: 42.9 44.2 45 45.7 46 ... plot(cpits) #Some functions to characterise time series #Position in the cycle cycle(cpits) ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1996 1 2 3 4 5 6 7 8 9 10 11 12 ## 1997 1 2 3 4 5 6 7 8 9 10 11 12 ## 1998 1 2 3 4 5 6 7 8 9 10 11 12 ## 1999 1 2 3 4 5 6 7 8 9 10 11 12 ## 2000 1 2 3 4 5 6 7 8 9 10 11 12 ## 2001 1 2 3 4 5 6 7 8 9 10 11 12 ## 2002 1 2 3 4 5 6 7 8 9 10 11 12 ## 2003 1 2 3 4 5 6 7 8 9 10 11 12 ## 2004 1 2 3 4 5 6 7 8 9 10 11 12 ## 2005 1 2 3 4 5 6 7 8 9 10 11 12 ## 2006 1 2 3 4 5 6 7 8 9 10 11 12 ## 2007 1 2 3 4 5 6 7 8 9 10 11 12 ## 2008 1 2 3 4 5 6 7 8 9 10 11 12 ## 2009 1 2 3 4 5 6 7 8 9 10 11 12 ## 2010 1 2 3 4 5 6 7 8 9 10 11 12 ## 2011 1 2 3 4 5 6 7 8 9 10 11 12 ## 2012 1 2 3 4 5 6 7 8 9 10 11 12 ## 2013 1 2 3 4 5 6 7 8 9 10 11 12 ## 2014 1 2 3 4 5 6 7 8 9 10 11 12 ## 2015 1 2 3 4 5 6 7 8 9 10 11 12 ## 2016 1 2 3 4 5 6 7 8 9 10 11 12 ## 2017 1 2 3 4 5 6 7 8 9 10 11 12 ## 2018 1 2 3 4 5 6 7 8 9 #to check frequency frequency(cpits) ## [1] 12 #How time is represented? As a fraction of the year! time(cpits) ## Jan Feb Mar Apr May Jun Jul ## 1996 1996.000 1996.083 1996.167 1996.250 1996.333 1996.417 1996.500 ## 1997 1997.000 1997.083 1997.167 1997.250 1997.333 1997.417 1997.500 ## 1998 1998.000 1998.083 1998.167 1998.250 1998.333 1998.417 1998.500 ## 1999 1999.000 1999.083 1999.167 1999.250 1999.333 1999.417 1999.500 ## 2000 2000.000 2000.083 2000.167 2000.250 2000.333 2000.417 2000.500 ## 2001 2001.000 2001.083 2001.167 2001.250 2001.333 2001.417 2001.500 ## 2002 2002.000 2002.083 2002.167 2002.250 2002.333 2002.417 2002.500 ## 2003 2003.000 2003.083 2003.167 2003.250 2003.333 2003.417 2003.500 ## 2004 2004.000 2004.083 2004.167 2004.250 2004.333 2004.417 2004.500 ## 2005 2005.000 2005.083 2005.167 2005.250 2005.333 2005.417 2005.500 ## 2006 2006.000 2006.083 2006.167 2006.250 2006.333 2006.417 2006.500 ## 2007 2007.000 2007.083 2007.167 2007.250 2007.333 2007.417 2007.500 ## 2008 2008.000 2008.083 2008.167 2008.250 2008.333 2008.417 2008.500 ## 2009 2009.000 2009.083 2009.167 2009.250 2009.333 2009.417 2009.500 ## 2010 2010.000 2010.083 2010.167 2010.250 2010.333 2010.417 2010.500 ## 2011 2011.000 2011.083 2011.167 2011.250 2011.333 2011.417 2011.500 ## 2012 2012.000 2012.083 2012.167 2012.250 2012.333 2012.417 2012.500 ## 2013 2013.000 2013.083 2013.167 2013.250 2013.333 2013.417 2013.500 ## 2014 2014.000 2014.083 2014.167 2014.250 2014.333 2014.417 2014.500 ## 2015 2015.000 2015.083 2015.167 2015.250 2015.333 2015.417 2015.500 ## 2016 2016.000 2016.083 2016.167 2016.250 2016.333 2016.417 2016.500 ## 2017 2017.000 2017.083 2017.167 2017.250 2017.333 2017.417 2017.500 ## 2018 2018.000 2018.083 2018.167 2018.250 2018.333 2018.417 2018.500 ## Aug Sep Oct Nov Dec ## 1996 1996.583 1996.667 1996.750 1996.833 1996.917 ## 1997 1997.583 1997.667 1997.750 1997.833 1997.917 ## 1998 1998.583 1998.667 1998.750 1998.833 1998.917 ## 1999 1999.583 1999.667 1999.750 1999.833 1999.917 ## 2000 2000.583 2000.667 2000.750 2000.833 2000.917 ## 2001 2001.583 2001.667 2001.750 2001.833 2001.917 ## 2002 2002.583 2002.667 2002.750 2002.833 2002.917 ## 2003 2003.583 2003.667 2003.750 2003.833 2003.917 ## 2004 2004.583 2004.667 2004.750 2004.833 2004.917 ## 2005 2005.583 2005.667 2005.750 2005.833 2005.917 ## 2006 2006.583 2006.667 2006.750 2006.833 2006.917 ## 2007 2007.583 2007.667 2007.750 2007.833 2007.917 ## 2008 2008.583 2008.667 2008.750 2008.833 2008.917 ## 2009 2009.583 2009.667 2009.750 2009.833 2009.917 ## 2010 2010.583 2010.667 2010.750 2010.833 2010.917 ## 2011 2011.583 2011.667 2011.750 2011.833 2011.917 ## 2012 2012.583 2012.667 2012.750 2012.833 2012.917 ## 2013 2013.583 2013.667 2013.750 2013.833 2013.917 ## 2014 2014.583 2014.667 2014.750 2014.833 2014.917 ## 2015 2015.583 2015.667 2015.750 2015.833 2015.917 ## 2016 2016.583 2016.667 2016.750 2016.833 2016.917 ## 2017 2017.583 2017.667 2017.750 2017.833 2017.917 ## 2018 2018.583 2018.667 #average difference between time units (0.08333 years or =1/12) deltat(cpits) ## [1] 0.08333333 #Let&#39;s plot four graphs in 2x2 par(mfrow=c(2,2)) plot(cpits) plot(log(cpits)) #monthly growth rate plot(diff(log(cpits))) #annual growth rate plot(diff(log(cpits), lag=12)) #close the graph dev.off() ## null device ## 1 3.3 Exponential smoothing and forecasting Let’s forecast monthly inflation rate based using Holt-Winters exponential smoothing with trend and additive seasonal component. Note that we must not have missing data. dlcpi &lt;- diff(log(cpits), lag=1) #omit missing values, we had one in October 2018 dlcpi &lt;- na.omit(dlcpi) plot(dlcpi) #Additive Holt-Winters (m &lt;- HoltWinters(dlcpi, seasonal = &quot;additive&quot;)) ## Holt-Winters exponential smoothing with trend and additive seasonal component. ## ## Call: ## HoltWinters(x = dlcpi, seasonal = &quot;additive&quot;) ## ## Smoothing parameters: ## alpha: 0.2223446 ## beta : 0.01158144 ## gamma: 0.4079999 ## ## Coefficients: ## [,1] ## a 5.980743e-03 ## b 2.555965e-05 ## s1 -6.449780e-03 ## s2 -4.597118e-03 ## s3 -5.897731e-03 ## s4 -2.844368e-03 ## s5 5.123452e-03 ## s6 -3.167743e-04 ## s7 3.546006e-04 ## s8 -3.146611e-04 ## s9 1.824240e-04 ## s10 -2.654886e-03 ## s11 -2.217077e-03 ## s12 -7.249312e-03 plot(m) #Predict 12 months ahead p &lt;- predict(m, n.ahead = 12, prediction.interval = TRUE) plot(m, p) 3.4 Finite Distributed Lags Model A finite distributed lag model (FDL) assumes a linear relationship between a dependent variable \\(y\\) and several lags of an independent variable \\(x\\). Let us consider another example, the dataset phillips_aus, which containes quarterly data on unemploymnt and inflation over the period 1987Q1 to 2009Q3. \\[inf_t = \\beta_1 + \\beta_2 Du_t + e_t\\] data(&quot;phillips_aus&quot;, package=&quot;PoEdata&quot;) phill.ts &lt;- ts(phillips_aus, start=c(1987,1), end=c(2009,3), frequency=4) inflation &lt;- phill.ts[,&quot;inf&quot;] Du &lt;- diff(phill.ts[,&quot;u&quot;]) par(mfrow=c(2,1)) plot(inflation) plot(Du) dev.off() ## null device ## 1 The plots definitely show patterns in the data for both inflation and unemployment rates. But we are interested to determine if the error term in Equation phill.dyn &lt;- dynlm(inf~diff(u),data=phill.ts) ehat &lt;- resid(phill.dyn) kable(tidy(phill.dyn), caption=&quot;Summary of the `phillips` model&quot;) Table 3.1: Summary of the phillips model term estimate std.error statistic p.value (Intercept) 0.7776213 0.0658249 11.813474 0.0000000 diff(u) -0.5278638 0.2294049 -2.301014 0.0237539 Can we trust the p-value of the estimated model? Let us first create function that gives us a similar plot as Eviews. #&#39; Eviews plot #&#39; This function creates a plot similar to the the timeseries regression plot in Eviews #&#39; Source: https://stackoverflow.com/questions/12699236/how-to-create-a-graph-showing-the-predictive-model-data-and-residuals-in-r #&#39; @param y dependent variable #&#39; @param model fitted model #&#39; #&#39; @return #&#39; @export #&#39; plotModel = function(y, model) { ymodel1 = range(y, fitted(model), na.rm=TRUE) ymodel2 = c(2*ymodel1[1]-ymodel1[2], ymodel1[2]) yres1 = range(residuals(model), na.rm=TRUE) yres2 = c(yres1[1], 2*yres1[2]-yres1[1]) plot(y, type=&quot;l&quot;, col=&quot;red&quot;, lwd=2, ylim=ymodel2, axes=FALSE, ylab=&quot;&quot;, xlab=&quot;&quot;) axis(1) mtext(&quot;residuals&quot;, 1, adj=0.5, line=2.5) axis(2, at=pretty(ymodel1)) mtext(&quot;observed/modeled&quot;, 2, adj=0.75, line=2.5) lines(fitted(model), col=&quot;green&quot;, lwd=2) par(new=TRUE) plot(residuals(model), col=&quot;blue&quot;, type=&quot;l&quot;, ylim=yres2, axes=FALSE, ylab=&quot;&quot;, xlab=&quot;&quot;) axis(4, at=pretty(yres1)) mtext(&quot;residuals&quot;, 4, adj=0.25, line=2.5) abline(h=quantile(residuals(model), probs=c(0.1,0.9)), lty=2, col=&quot;gray&quot;) abline(h=0) box() } par(oma=c(1,1,1,2)) plotModel(inflation, phill.dyn) # works with models which accept &#39;predict&#39; and &#39;residuals&#39; corrgm &lt;- acf(ehat) plot(corrgm) The time series plot suggests that some patterns exists in the residuals, which is confirmed by the correlogram. The previous result suggesting that there is a significant relationship between inflation and change in unemployment rate may not be, afterall, too reliable. 3.5 ARIMA models We use functions arima from stats package, auto.arima and Arima from forecast package. We also look at autocorrelation functions and partial autocorrelation functions with acf2 command from astsa package. ARMA models are suited to describe stationary time series. Wold’s composition says that every covariance-stationary time series can be represented as a linear combination of lags of white noise (and a deterministic time series). \\[ X_t = W_t + a_1 \\times W_{-1}+ a_2 \\times W_{-2} + \\dots \\] If the time series is not stationary, we need to take first difference of the series. ARIMA models are denoted by ARIMA(p,d,q) where p is the order of AR terms, q is the order of MA terms and d is the order of integration. Full seasonal model is denoted by SARIMA(p,d,q) \\(\\times\\) (P,D,Q)S where capital letters denote respective the seasonal orders. 3.5.1 Simulation of ARIMA models To simulate ARMA models we use function arima.sim. #To simulate ARIMA process #MA(1) process xma1 &lt;- arima.sim(list(order=c(0,0,1), ma=0.9), n=100) plot(xma1) #AR(1) xar1 &lt;- arima.sim(list(order=c(1,0,0), ar=0.8), n=100) plot(xar1) #ARMA(1,1) xarma11 &lt;- arima.sim(list(order=c(1,0,1), ar=0.8, ma=0.5), n=100) plot(xarma11) #ARMA(2,1) xarma21 &lt;- arima.sim(list(order=c(2,0,1), ar=c(0.8,-0.5), ma=0.5), n=100) plot(xarma21) 3.5.2 ACF and PACF Next let’s have a quick look how to identify ARMA model from the data. Very difficult to distinguish from the graphs, therefore we also use autocorrelation functions (ACF) and partial autocorrelation functions (PACF). #Compare the following time series in the graph x &lt;- arima.sim(list(order=c(1,0,0), ar=-.7), n=200) y &lt;- arima.sim(list(order=c(0,0,1), ma=-.7), n=200) par(mfrow=c(1,2)) plot(x, main = &quot;AR(1)&quot;) plot(y, main = &quot;MA(1)&quot;) dev.off() ## null device ## 1 Autocorrelation function and partial autocorrelation function help to identify lag structure of ARMA models. If pure AR(p) process then ACF will tail off (changes to smaller) and PACF cuts off lag p. If pure MA(q) process then PACF will tail off (changes to smaller) and ACF cuts off lag q. If ARMA(p,q) then both ACF and PACF will tail off. Then start from (ARMA(1,1) and add more lags as needed). Look at the following ACF and PACF functions and see if the ACF and PACF of the simulated time series follow expected pattern. #AR(1) x &lt;- arima.sim(list(order=c(1,0,0), ar=c(0.9)), n=100) plot(x,main=&quot;AR(1)&quot;) #ACF, PACF from atsta package acf2(x) ## ACF PACF ## [1,] 0.76 0.76 ## [2,] 0.58 -0.01 ## [3,] 0.42 -0.04 ## [4,] 0.25 -0.14 ## [5,] 0.11 -0.04 ## [6,] 0.06 0.07 ## [7,] 0.04 0.06 ## [8,] -0.01 -0.13 ## [9,] -0.07 -0.10 ## [10,] -0.12 -0.05 ## [11,] -0.12 0.09 ## [12,] -0.12 0.03 ## [13,] -0.09 0.00 ## [14,] -0.07 -0.05 ## [15,] -0.08 -0.10 ## [16,] -0.12 -0.06 ## [17,] -0.16 -0.03 ## [18,] -0.18 -0.01 ## [19,] -0.15 0.07 ## [20,] -0.19 -0.20 #Look the autocorrelation coefficients #AR(2) x &lt;- arima.sim(list(order=c(2,0,0), ar=c(0.9, -0.4)), n=100) plot(x, main=&quot;AR(2)&quot;) acf2(x) ## ACF PACF ## [1,] 0.59 0.59 ## [2,] 0.04 -0.47 ## [3,] -0.30 -0.10 ## [4,] -0.27 0.11 ## [5,] -0.12 -0.11 ## [6,] -0.05 -0.11 ## [7,] -0.04 0.01 ## [8,] -0.10 -0.16 ## [9,] -0.16 -0.13 ## [10,] -0.16 -0.05 ## [11,] -0.09 -0.05 ## [12,] -0.01 -0.11 ## [13,] 0.05 0.00 ## [14,] 0.03 -0.12 ## [15,] 0.04 0.04 ## [16,] 0.07 0.01 ## [17,] 0.20 0.17 ## [18,] 0.22 -0.04 ## [19,] 0.09 -0.07 ## [20,] -0.10 -0.05 #MA(1) x &lt;- arima.sim(list(order=c(0,0,1), ma=0.4), n=100) plot(x, main=&quot;MA(1)&quot;) acf2(x) ## ACF PACF ## [1,] 0.28 0.28 ## [2,] -0.13 -0.23 ## [3,] -0.08 0.04 ## [4,] -0.02 -0.05 ## [5,] -0.02 -0.02 ## [6,] 0.02 0.03 ## [7,] -0.04 -0.08 ## [8,] -0.12 -0.09 ## [9,] -0.02 0.04 ## [10,] 0.06 0.01 ## [11,] -0.06 -0.10 ## [12,] 0.01 0.09 ## [13,] 0.06 -0.01 ## [14,] -0.10 -0.12 ## [15,] -0.07 0.02 ## [16,] 0.04 0.01 ## [17,] -0.02 -0.06 ## [18,] 0.01 0.06 ## [19,] 0.10 0.05 ## [20,] 0.00 -0.05 #ARMA(1,1) x &lt;- arima.sim(list(order=c(1,0,1), ar=0.9, ma=-0.4), n=200) plot(x, main=&quot;ARMA(1,1)&quot;) acf2(x) ## ACF PACF ## [1,] 0.76 0.76 ## [2,] 0.64 0.14 ## [3,] 0.57 0.11 ## [4,] 0.52 0.06 ## [5,] 0.43 -0.08 ## [6,] 0.32 -0.11 ## [7,] 0.27 0.02 ## [8,] 0.21 -0.02 ## [9,] 0.20 0.09 ## [10,] 0.15 -0.04 ## [11,] 0.11 -0.04 ## [12,] 0.09 0.02 ## [13,] 0.07 -0.04 ## [14,] 0.02 -0.06 ## [15,] -0.03 -0.07 ## [16,] -0.05 0.01 ## [17,] -0.09 -0.07 ## [18,] -0.10 0.04 ## [19,] -0.13 -0.04 ## [20,] -0.10 0.11 ## [21,] -0.10 -0.02 ## [22,] -0.11 -0.04 ## [23,] -0.13 -0.06 ## [24,] -0.13 -0.02 ## [25,] -0.11 0.02 3.5.3 Estimation of ARIMA models Let’s simulate a model and then estimate it with arima and sarima commands. #generate AR(2) process with mean x &lt;- arima.sim(list(order=c(2,0,0), ar=c(1.5, -.75)), n=200) + 50 plot(x) #use command sarima from astsa package #it provides a nice graph for residual diagnostics of the model x_fit &lt;- sarima(x, p=2, d=0, q=0) ## initial value 1.055937 ## iter 2 value 0.903084 ## iter 3 value 0.522531 ## iter 4 value 0.326178 ## iter 5 value 0.191550 ## iter 6 value 0.049617 ## iter 7 value 0.035394 ## iter 8 value 0.028680 ## iter 9 value 0.028215 ## iter 10 value 0.027723 ## iter 11 value 0.027661 ## iter 12 value 0.027660 ## iter 13 value 0.027659 ## iter 13 value 0.027659 ## iter 13 value 0.027659 ## final value 0.027659 ## converged ## initial value 0.032953 ## iter 2 value 0.032946 ## iter 3 value 0.032938 ## iter 4 value 0.032937 ## iter 5 value 0.032937 ## iter 6 value 0.032937 ## iter 7 value 0.032937 ## iter 7 value 0.032937 ## iter 7 value 0.032937 ## final value 0.032937 ## converged x_fit$ttable ## Estimate SE t.value p.value ## ar1 1.4669 0.0458 31.9940 0 ## ar2 -0.7528 0.0456 -16.4925 0 ## xmean 50.2442 0.2540 197.8043 0 # =&gt; residuals are white noise #alternatively may use command arima from the stats package x_fit2 &lt;- arima(x, order=c(2, 0, 0)) x_fit2 ## ## Call: ## arima(x = x, order = c(2, 0, 0)) ## ## Coefficients: ## ar1 ar2 intercept ## 1.4669 -0.7528 50.2442 ## s.e. 0.0458 0.0456 0.2540 ## ## sigma^2 estimated as 1.053: log likelihood = -290.38, aic = 588.75 #Let&#39;s have a quick look at residual diagnostics tsdiag(x_fit2) ?Arima x_fit3 &lt;- Arima(x, order=c(2,0,0)) x_fit3 ## Series: x ## ARIMA(2,0,0) with non-zero mean ## ## Coefficients: ## ar1 ar2 mean ## 1.4669 -0.7528 50.2442 ## s.e. 0.0458 0.0456 0.2540 ## ## sigma^2 estimated as 1.069: log likelihood=-290.38 ## AIC=588.75 AICc=588.96 BIC=601.94 Arima(x, order=c(2,0,0)) %&gt;% forecast(h=20) %&gt;% autoplot 3.5.4 Choosing between models Descriptive power and simplicity Use AIC or BIC criteria, which both have penalty on model errors (average[(observed-predicted)^2]) and number of parameters (k*(p+q). The larger the AIC or BIC, the worse is the model. BIC has larger penalty on model parameters (k=ln(n)) than AIC (k=2), hence prefers simpler models. Goal: find the model with the smallest AIC or BIC. Residual analysis Models residuals should be white noise, otherwise there is still some information left in the residuals. sarima() function provides graphical analysis of residuals: a) standardised residuals - are there obvious patterns in the residuals? b) ACF of residuals - any remaining correlation in residuals c) Normal Q-Q plot - if residuals are normal, then they should be along the line. It would be nice to have normal errors to rely on t-distribution, but that is not so large problem if violated (Caution: I am not an expert!) d) Ljung-Box Q-statistic p-values - points must above the line in order to have statistically insignificant test-statistics. The standardized residuals should behave as a white noise sequence with mean zero and variance one. The sample ACF of the residuals should also look like that of white noise. Normality is important assumption when fitting ARMA models. Examine the Q-Q plot for departures from normality and to identify outliers. ## to compare models use BIC() and AIC() statistics; BIC() and AIC() would choose the smallest #Let&#39;s estimate two models #AR(2) x_fit2 &lt;- arima(x, order=c(2, 0, 0)) #AR(3) x_fit3 &lt;- arima(x, order=c(3, 0, 1)) x_fit3 ## ## Call: ## arima(x = x, order = c(3, 0, 1)) ## ## Coefficients: ## ar1 ar2 ar3 ma1 intercept ## 0.9969 -0.0840 -0.3293 0.5161 50.2437 ## s.e. 0.5566 0.8242 0.4302 0.5387 0.2637 ## ## sigma^2 estimated as 1.048: log likelihood = -289.93, aic = 591.86 tsdiag(x_fit3) #Compare these models BIC(x_fit2, x_fit3) #=&gt; x_fit2 better as BIC is smaller df BIC 4 602 6 612 AIC(x_fit2, x_fit3) #=&gt; x_fit2 still better in my case df AIC 4 589 6 592 #You may compare several models at once BIC(arima(x, order=c(1, 0, 0)), arima(x, order=c(2, 0, 0)), #=&gt; this is best accoding to BIC arima(x, order=c(3, 0, 0)), arima(x, order=c(1, 0, 1)), arima(x, order=c(2, 0, 1)), arima(x, order=c(3, 0, 1))) df BIC 3 765 4 602 5 607 4 665 5 607 6 612 3.5.5 Automatic esimation You may let auto.arima function to decide automatically which model is the best one. It returns best ARIMA model according to either AIC, AICc or BIC value. The function conducts a search over possible model within the order constraints provided. See the help file for more details. The function can also be used on non-stationary time series and let it automatically test for unit roots. forecast::auto.arima(x, d=0, max.p = 5, max.q = 5, ic=&quot;aic&quot;) ## Series: x ## ARIMA(2,0,0) with non-zero mean ## ## Coefficients: ## ar1 ar2 mean ## 1.4669 -0.7528 50.2442 ## s.e. 0.0458 0.0456 0.2540 ## ## sigma^2 estimated as 1.069: log likelihood=-290.38 ## AIC=588.75 AICc=588.96 BIC=601.94 #surpisingly it ended up with ARMA(4,3) in my case when using AIC #and AR(3) when using BIC, although AR(3) is not statistically significant forecast::auto.arima(x, d=0, max.p = 5, max.q = 5, ic=&quot;bic&quot;) ## Series: x ## ARIMA(2,0,0) with non-zero mean ## ## Coefficients: ## ar1 ar2 mean ## 1.4669 -0.7528 50.2442 ## s.e. 0.0458 0.0456 0.2540 ## ## sigma^2 estimated as 1.069: log likelihood=-290.38 ## AIC=588.75 AICc=588.96 BIC=601.94 ###Prediction with ARMA models Prediction is straightforward, simply apply predict command to the estimated model. ## An example of ARIMA forecasting three periods ahead: predict(x_fit2, n.ahead= 3) ## $pred ## Time Series: ## Start = 201 ## End = 203 ## Frequency = 1 ## [1] 48.33845 48.73433 49.46398 ## ## $se ## Time Series: ## Start = 201 ## End = 203 ## Frequency = 1 ## [1] 1.026077 1.821600 2.319205 3.5.6 Testing for unit root To test for unit root we can use package fUnitRoots, which includes Augmented Dickey Fuller unit root test and Phillips-Perron test. We can also use package urca, which includes many tests. #Let&#39;s first use again simulated time series x1 &lt;- arima.sim(list(order=c(1,1,1), ar=c(0.5), ma=-0.3), n=100) plot(x1) #clearly random walk process #Let&#39;s test using adfTest #The null hypothesis is that time series is not stationary and we cannot reject that adfTest(x1) ## ## Title: ## Augmented Dickey-Fuller Test ## ## Test Results: ## PARAMETER: ## Lag Order: 1 ## STATISTIC: ## Dickey-Fuller: -0.998 ## P VALUE: ## 0.2951 ## ## Description: ## Sun Oct 28 22:35:07 2018 by user: #=&gt; we cannot reject H0 #type of the regression: &quot;nc&quot; no constant nor time trend, &quot;c&quot; with a constant but no time trend, &quot;ct&quot; with an intercept constant and a time trend. The default is &quot;c&quot;. adfTest(x1, type=&quot;ct&quot;) ## ## Title: ## Augmented Dickey-Fuller Test ## ## Test Results: ## PARAMETER: ## Lag Order: 1 ## STATISTIC: ## Dickey-Fuller: -3.0659 ## P VALUE: ## 0.1348 ## ## Description: ## Sun Oct 28 22:35:07 2018 by user: test1 &lt;- adfTest(x1, type=&quot;ct&quot;) #the model behind the test regression summary(test1@test$lm) ## ## Call: ## lm(formula = y.diff ~ y.lag.1 + 1 + tt + y.diff.lag) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.2753 -0.6501 0.0501 0.7262 3.1810 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.476773 0.263215 -1.811 0.07325 . ## y.lag.1 -0.138860 0.045291 -3.066 0.00283 ** ## tt 0.016548 0.006194 2.672 0.00888 ** ## y.diff.lag 0.263175 0.099312 2.650 0.00943 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.018 on 95 degrees of freedom ## Multiple R-squared: 0.127, Adjusted R-squared: 0.09946 ## F-statistic: 4.608 on 3 and 95 DF, p-value: 0.004695 #we can cleary drop trend (test1 &lt;- adfTest(x1, type=&quot;c&quot;)) ## ## Title: ## Augmented Dickey-Fuller Test ## ## Test Results: ## PARAMETER: ## Lag Order: 1 ## STATISTIC: ## Dickey-Fuller: -1.4869 ## P VALUE: ## 0.5049 ## ## Description: ## Sun Oct 28 22:35:07 2018 by user: #the model behind the test regression summary(test1@test$lm) ## ## Call: ## lm(formula = y.diff ~ y.lag.1 + 1 + y.diff.lag) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3327 -0.5413 0.1059 0.6469 2.6681 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.15317 0.12067 1.269 0.2074 ## y.lag.1 -0.04028 0.02709 -1.487 0.1403 ## y.diff.lag 0.23026 0.10165 2.265 0.0257 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.05 on 96 degrees of freedom ## Multiple R-squared: 0.06144, Adjusted R-squared: 0.04189 ## F-statistic: 3.142 on 2 and 96 DF, p-value: 0.04766 #And perhaps also the constant test1 &lt;- adfTest(x1, type=&quot;nc&quot;) #the model behind the test regression test1 ## ## Title: ## Augmented Dickey-Fuller Test ## ## Test Results: ## PARAMETER: ## Lag Order: 1 ## STATISTIC: ## Dickey-Fuller: -0.998 ## P VALUE: ## 0.2951 ## ## Description: ## Sun Oct 28 22:35:07 2018 by user: summary(test1@test$lm) ## ## Call: ## lm(formula = y.diff ~ y.lag.1 - 1 + y.diff.lag) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.1811 -0.4353 0.1831 0.8035 2.8305 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## y.lag.1 -0.02380 0.02385 -0.998 0.3208 ## y.diff.lag 0.22840 0.10196 2.240 0.0274 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.053 on 97 degrees of freedom ## Multiple R-squared: 0.0519, Adjusted R-squared: 0.03235 ## F-statistic: 2.655 on 2 and 97 DF, p-value: 0.0754 #=&gt; still we cannot reject H0 #McKinnons&#39;s test statistics - corrects t-statistic and p-value for possible misspecification of lags in the ADF test equation unitrootTest(x1, lags = 1, type = &quot;c&quot;) ## ## Title: ## Augmented Dickey-Fuller Test ## ## Test Results: ## PARAMETER: ## Lag Order: 1 ## STATISTIC: ## DF: -1.4869 ## P VALUE: ## t: 0.5363 ## n: 0.8346 ## ## Description: ## Sun Oct 28 22:35:08 2018 by user: ################################################### #More tests are available in urca package #Augmented Dickey--Fuller test statistic summary(ur.df(x1, type= c(&quot;drift&quot;), lags=1, selectlags = &quot;Fixed&quot;)) ## ## ############################################### ## # Augmented Dickey-Fuller Test Unit Root Test # ## ############################################### ## ## Test regression drift ## ## ## Call: ## lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3327 -0.5413 0.1059 0.6469 2.6681 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.15317 0.12067 1.269 0.2074 ## z.lag.1 -0.04028 0.02709 -1.487 0.1403 ## z.diff.lag 0.23026 0.10165 2.265 0.0257 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.05 on 96 degrees of freedom ## Multiple R-squared: 0.06144, Adjusted R-squared: 0.04189 ## F-statistic: 3.142 on 2 and 96 DF, p-value: 0.04766 ## ## ## Value of test-statistic is: -1.4869 1.3067 ## ## Critical values for test statistics: ## 1pct 5pct 10pct ## tau2 -3.46 -2.88 -2.57 ## phi1 6.52 4.63 3.81 #Kwiatkowski et al. Unit Root Test - H0 is that we do not have unit root. We reject that. summary(ur.kpss(x1)) ## ## ####################### ## # KPSS Unit Root Test # ## ####################### ## ## Test is of type: mu with 4 lags. ## ## Value of test-statistic is: 1.6031 ## ## Critical value for a significance level of: ## 10pct 5pct 2.5pct 1pct ## critical values 0.347 0.463 0.574 0.739 #Phillips &amp; Perron Unit Root Test summary(ur.pp(x1)) ## ## ################################## ## # Phillips-Perron Unit Root Test # ## ################################## ## ## Test regression with intercept ## ## ## Call: ## lm(formula = y ~ y.l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.73332 -0.68987 0.06119 0.58680 2.64451 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.13892 0.12211 1.138 0.258 ## y.l1 0.97255 0.02705 35.960 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.069 on 98 degrees of freedom ## Multiple R-squared: 0.9296, Adjusted R-squared: 0.9288 ## F-statistic: 1293 on 1 and 98 DF, p-value: &lt; 2.2e-16 ## ## ## Value of test-statistic, type: Z-alpha is: -4.0258 ## ## aux. Z statistics ## Z-tau-mu 1.178 #we cannot reject H0 that the parameter value is actually 1 #See other tests #Elliott, Rothenberg &amp; Stock Unit Root Test summary(ur.ers(x1)) ## ## ############################################### ## # Elliot, Rothenberg and Stock Unit Root Test # ## ############################################### ## ## Test of type DF-GLS ## detrending of series with intercept ## ## ## Call: ## lm(formula = dfgls.form, data = data.dfgls) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.4006 -0.5288 0.1142 0.7567 2.2365 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## yd.lag -0.02399 0.02734 -0.878 0.3824 ## yd.diff.lag1 0.21417 0.10409 2.057 0.0425 * ## yd.diff.lag2 0.15756 0.10309 1.528 0.1299 ## yd.diff.lag3 -0.15989 0.10405 -1.537 0.1278 ## yd.diff.lag4 -0.06879 0.10592 -0.649 0.5177 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.016 on 91 degrees of freedom ## Multiple R-squared: 0.1096, Adjusted R-squared: 0.06068 ## F-statistic: 2.24 on 5 and 91 DF, p-value: 0.05689 ## ## ## Value of test-statistic is: -0.8778 ## ## Critical values of DF-GLS are: ## 1pct 5pct 10pct ## critical values -2.59 -1.94 -1.62 #Schmidt &amp; Phillips Unit Root Test summary(ur.sp(x1)) ## ## ################################### ## # Schmidt-Phillips Unit Root Test # ## ################################### ## ## ## Call: ## lm(formula = sp.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.67505 -0.59993 0.06144 0.64090 3.09217 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.433923 0.265890 -1.632 0.1059 ## y.lagged 0.884919 0.044935 19.693 &lt;2e-16 *** ## trend.exp1 0.014833 0.006154 2.410 0.0178 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.044 on 97 degrees of freedom ## Multiple R-squared: 0.9335, Adjusted R-squared: 0.9322 ## F-statistic: 681.2 on 2 and 97 DF, p-value: &lt; 2.2e-16 ## ## ## Value of test-statistic is: -2.9029 ## Critical value for a significance level of 0.01 ## is: -3.61 "],
["references.html", "References", " References "]
]
