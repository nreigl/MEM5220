---
output:
  pdf_document: default
  html_document: default
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---


# Qualitative and LDV Models {#binarymodels}


To load the dataset and necessary functions: 


```{r, echo=TRUE, message=FALSE, results='hide'}
# This function 1. checks if the packages are installed. 2. It installs the packages if they were not in the list of installed packages. 3. It loads the packages into the workspace
# devtools::install_github("ccolonescu/PoEdata")
PACKAGES<-c(
            "tidyverse",  # for data manipulation and ggplots
            "broom",  # Tidy regression output
            "tidymodels", # packages for modeling and machine learning using 
            "Hmisc", # Harrell Miscellaneous functions
            "psych", # Procedures for Psychometric research
            "car", # Companion to applied regression
            "knitr", # knit functions
            "modelsummary", 
            # "kableExtra", # extended knit functions for objects exported from other packages
            "huxtable", #  Regression tables, broom compatible
            "stargazer", # Regression tables
            "AER", #  Functions, data sets, examples, demos, and vignettes for the book Christian Kleiber and Achim Zeileis (2008)
             # "PoEdata", # R data sets for "Principles of Econometrics" by Hill, Griffiths, and Lim, 4e, Wiley. https://github.com/ccolonescu/PoEdata
            "wooldridge",  # Wooldrige Datasets
            "MCMCpack", # Contains functions to perform Bayesian inference using posterior simulation for a number of ssatistical models.
            "sampleSelection", # Two-step and maximum likelihood estimation of Heckman-type sample selection models
            "scales", # scale helper functions such as percent 
            "lmtest", 
            "margins", # Stata like margin functions
            "prediction", # Type stable predictions
            "nnet", # Multinomial logit
            "survival", # Survival Analysis
            "sampleSelection", # Heckman type sample selection
            "censReg", # Censored Regression models
            "magrittr") #  pipes
inst<-match(PACKAGES, .packages(all=TRUE))
need<-which(is.na(inst))
# if (length(need)>0) pak::pkg_install(PACKAGES[need])
if (length(need)>0) install.packages(PACKAGES[need])
lapply(PACKAGES, require, character.only=T)
```


Binary dependent variables are frequently studied in applied economics. Because a dummy variable $y$ can only take values 0 and 1, its (conditional) expected value is equal to the (conditional) probability that $y=1$: 


$E(y|x) = 0 \times P(y = 0|x) + 1 \times P(y = 1|x) = P(y=1|x)$

An important class of models specifies the success probability as

$P(y = 1 | x) = G(\beta_0 + \beta_1 + \dots \beta_k x_k) = G(\boldsymbol{x} \boldsymbol{\beta})$



The following table is taken from @dalpiaz2016 and summarizes three examples of a generalized linear model:

|                 |Linear Regression | Poisson Regression | Logistic Regression |
|-----------------|------------------|--------------------|---------------------|
| $Y \mid {\bf X} = {\bf x}$ | $N(\mu({\bf x}), \sigma^2)$    | $\text{Pois}(\lambda({\bf x}))$          | $\text{Bern}(p({\bf x}))$                                              |
| **Distribution Name**                           | Normal                         | Poisson                                  | Bernoulli (Binomial)                                                   |
| $\text{E}[Y \mid {\bf X} = {\bf x}]$            | $\mu({\bf x})$                 | $\lambda({\bf x})$                       | $p({\bf x})$                                                           |
| **Support**                                     | Real: $(-\infty, \infty)$      | Integer: $0, 1, 2, \ldots$               | Integer: $0, 1$                                                        |
| **Usage**                                       | Numeric Data                   | Count (Integer) Data                     | Binary (Class ) Data                                            |
| **Link Name**                                   | Identity                       | Log                                      | Logit                                                                  |
| **Link Function**                               | $\eta({\bf x}) = \mu({\bf x})$ | $\eta({\bf x}) = \log(\lambda({\bf x}))$ | $\eta({\bf x}) = \log \left(\frac{p({\bf x})}{1 - p({\bf x})} \right)$          |
| **Mean Function**                               | $\mu({\bf x}) = \eta({\bf x})$ | $\lambda({\bf x}) = e^{\eta({\bf x})}$   | $p({\bf x}) = \frac{e^{\eta({\bf x})}}{1 + e^{\eta({\bf x})}} = \frac{1}{1 + e^{-\eta({\bf x})}}$ |

Like ordinary linear regression, we will seek to "fit" the model by estimating the $\beta$ parameters. To do so, we will use the method of maximum likelihood.

Note that a Bernoulli distribution is a specific case of a binomial distribution where the $n$ parameter of a binomial is $1$. Binomial regression is also possible, but we'll focus on the much more popular Bernoulli case.

So, in general, GLMs relate the mean of the response to a linear combination of the predictors, $\eta({\bf x})$, through the use of a link function, $g()$. That is,

$$
\eta({\bf x}) = g\left(\text{E}[Y \mid {\bf X} = {\bf x}]\right).
$$

The mean is then

$$
\text{E}[Y \mid {\bf X} = {\bf x}] = g^{-1}(\eta({\bf x})).
$$


## Linear probability models


If a dummy variable is used as the dependent variable $y$, we can still use OLS to estimate its relation to the regressors $x$. 


```{r}
data("mroz", package = "wooldridge")
```

Let us use the `describe()` function from the **Hmisc** package to use a different way to describe the datatset. 

```{r}
describe(mroz)
```

Looking at the share of women in the labour force in the sample

```{r}
prop.table(table(mroz$inlf))
```



We want to investigate how female labor market participation depends on non-wife household income, her education, age and number of small children? First, take a look how the labor force participation and the age cohorts are related. 

```{r}
plot(factor(inlf) ~ age, data = mroz, 
     ylevels = 2:1,
     ylab = "in labor force?")
```


Not so much variation with respect to age, except for the later years.


Estimate linear probability model

```{r}
linprob <- lm(inlf~nwifeinc+educ+exper+I(exper^2)+age+kidslt6+kidsge6,data=mroz)
# Regression table with heteroscedasticity-robust SE and t tests:
coeftest(linprob,vcov=hccm)
```


The estimated coefficient *educ* can be interpreted as: an additional year of schooling increases the probability that a woman is in the labor force *ceteris paribus* by `r scales::percent(coef(linprob)["educ"])`. 

One problem with linear probability models is that $P(y=1|x)$ is specified as a linear function of the regressors. By construction, there are more or less realistic combinations of regressor values that yield $\hat{y} <	0$ or $\hat{y} >	1$.   

Predictions for two "extreme" women:

```{r}
xpred <- list(nwifeinc=c(100,0),educ=c(5,17),exper=c(0,30),
              age=c(20,52),kidslt6=c(2,0),kidsge6=c(0,0))
predict(linprob,xpred)
```


We don't have to restrict ourself to two extreme values. For quite a few observations, this model predicts a probability of working which is either greater than 1, or smaller than zero.


```{r}
pr = predict(linprob)
plot(pr[order(pr)],ylab = "p(inlf = 1)")
abline(a = 0, b = 0, col = "red")
abline(a = 1, b = 0, col = "red")
```


In the case of a *saturated* model, that is when only have dummy explanatory variables, this problem does not exist for linear probability models. 

```{r}
mroz %<>% 
  # classify age into 3 and huswage into 2 classes
  mutate(age_fct = cut(age,breaks = 3,labels = FALSE),
         huswage_fct = cut(huswage, breaks = 2,labels = FALSE)) %>%
  mutate(classes = paste0("age_",age_fct,"_hus_",huswage_fct))
```

```{r}
LPM_saturated = mroz %>%
  lm(inlf ~ classes, data = .)

mroz$pred <- predict(LPM_saturated)
```


```{r}
ggplot(mroz[order(mroz$pred),], aes(x = 1:nrow(mroz),y = pred,color = classes)) + 
  geom_point() + 
  theme_bw() + 
  scale_y_continuous(limits = c(0,1), name = "p(inlf)") +
  labs(title = "LPM model in a saturated setting", 
       subtitle = "only mutually exhaustive dummy variables on the RHS.")
```

or example you see that women from the youngest age category and lowest husband income (class age_1_hus_1) have the highest probability of working (0.611).

## LPM, Probit, Logit simulation


```{r}
# Simulated data
set.seed(12345)
y <- rbinom(100, 1, 0.5)
x <- rnorm(100) + 2 * y

# Estimation 
linpr.res <- lm(y~x)
logit.res <-glm(y ~x, family = binomial(link = logit))
probit.res <-glm(y ~x, family = binomial(link = probit))

# Prediction from a regular grid of x values
xp <- seq(from= min(x), to=max(x), length=50)
linpr.p <- predict(linpr.res, list(x = xp), type = "response")
logit.p <- predict(logit.res, list(x = xp), type = "response")
probit.p <- predict(probit.res, list(x = xp), type = "response")
```


```{r fig22, fig.cap='Predictions from binary response models (simulated data)', out.width='80%', fig.asp=.75, fig.align='center'}
plot(x,y)
lines(xp, linpr.p, lwd=2, lty = 1)
lines(xp, logit.p, lwd=2, lty = 2)
lines(xp, probit.p, lwd=1, lty = 1)
legend("topleft", 
       c("linear prob.", "logit", "probit"), 
       lwd = c(2,2,1), lty = c(1,2,1))
```


## Logit 


For binary response models, the most widely used specification for G are

- the **probit** model with $G(z) = \phi(z)$, the standard cdf and 
- the **logit** model with $G(z) = \Lambda(z) = \frac{exp(z)}{1+ exp(z)}$, the cdf of the logistic distribution. 


In R  many generalized linear models can be estimated by the `glm()` command. It accepts the additional option 

- `family = binomial (link = logit)` for the logit model or 
- `family = binomial (link = probit)` for the probit model 


```{r}
# Estimate logit model
logitres<-glm(inlf~nwifeinc+educ+exper+I(exper^2)+age+kidslt6+kidsge6,
              family=binomial(link=logit),data=mroz)
# Summary of results:
summary(logitres)

# Log likelihood value:
logLik(logitres) 
# McFadden's pseudo R2:
1 - logitres$deviance/logitres$null.deviance
```


We can also extract the fitted values from the specified model and compare them the observed predicated binomial explained variable. 

```{r}
fit <- data.frame(response = mroz$inlf, predicted = round(fitted(logitres), 0))
xtabs(~ predicted + response, data = fit)
```

If you want to interpret the estimated effects as relative odds ratios (plus their confidence intervals):  


```{r}
exp(logitres$coefficients)
exp(confint(logitres)) # Lets convert the confidence intervals the same way

resultsTable <- exp(cbind(OddsRatios = coef(logitres), confint(logitres)))
round(resultsTable, digits = 5)
```


This gives you $e^\beta$, the multiplicative change in the odds ratio for $y=1$ if the covariate associated with $\beta$ increases by 1. 


## Probit 


```{r fig21, fig.cap='The shape of the probit function is the standard normal distribution', out.width='80%', fig.asp=.75, fig.align='center'}
par(mfrow=c(1,2))  
x <- seq(-3,3, .2)
plot(x, pnorm(x), type="l", xlab="b1+b2x", ylab="P[y=1]")
plot(x, dnorm(x), type="l")
```



```{r}
detach("package:PoEdata", unload=TRUE)
data("mroz", package='wooldridge')
attach(mroz)
# Estimate probit model
probitres<-glm(inlf~nwifeinc+educ+exper+I(exper^2)+age+kidslt6+kidsge6,
               family=binomial(link=probit),data=mroz)
#Summary of results:
summary(probitres)
# Log likelihood value:
logLik(probitres) 
# McFadden's pseudo R2:
1 - probitres$deviance/probitres$null.deviance
```



### Inference

We can implement the test for overall significance for the probit model using both manual and automatic calculations. 


```{r}
# Test of overall significance:
# Manual calculation of the LR test statistic:
probitres$null.deviance - probitres$deviance
```


```{r}
# Test of H0: experience and age are irrelevant
restr <- glm(inlf~nwifeinc+educ+ kidslt6+kidsge6, 
             family=binomial(link=logit),data=mroz)
```


```{r}
lrtest(restr,probitres)
```


### Predictions

The command `predict()` can calculate predicted values for the estimation sample or arbitrary sets of regressor values. 

We can calculate 

```{r}
# predictions for two "extreme" women:
xpred <- list(nwifeinc=c(100,0),educ=c(5,17),exper=c(0,30),
              age=c(20,52),kidslt6=c(2,0),kidsge6=c(0,0))
# Predictions from linear probability, probit and logit model:
predict(linprob,  xpred,type = "response")
predict(logitres, xpred,type = "response")
predict(probitres,xpred,type = "response")
```


```{r}
# Probit
# Summary of results:
summary(probitres)

# Log likelihood value:
logLik(probitres) 
# McFadden's pseudo R2:
1 - probitres$deviance/probitres$null.deviance
```

Another way to obtain the pseudo-R2 is to estimate the null model using `glm()` and extract the maximized log-likelihoods for both the null and the full model using the function logLik().

```{r}
probit_null<-glm(inlf~1,
               family=binomial(link=probit),data=mroz)

```

Compute the pseudo-R2 using 'logLik

```{r}
1 - logLik(probitres)[1]/logLik(probit_null)[1]
```


```{r}
modelsummary::modelsummary(list("Logit" = logitres,"Probit" = probitres), stars = TRUE)
```

From this table, we learn that the coefficient for age is -0.088 for logit and -0.053 for probit, respectively. In both cases, this tells us that the impact of an additional year of age on the probability of working is negative. However, we cannot straightforwardly read off the magnitude of the effect. 


## Marginal (partial) effects

Several packages provide estimates of marginal effects for different types of models. Among these are **car**, **alr3**, **mfx**, **erer**, among others. Unfortunately, none of these packages implement marginal effects correctly (i.e., correctly account for interrelated variables such as interaction terms (e.g., a:b) or power terms (e.g., I(a^2)) and the packages all implement quite different interfaces for different types of models. The **margins** and **prediction** packages are a combined effort to calculate marginal effects that include complex terms and provide a uniform interface for doing those calculations.


To know how much a variable influences the labour force participation, one has to use `margins()` command:

```{r}
effects_logit_participation <- margins::margins(logitres) 
```


```{r}
summary(effects_logit_participation)
```


```{r}
plot(effects_logit_participation)
```


If one desires subgroup effects, simply pass a subset of data to the data argument:

```{r}
effects_logit_participation_subset <- margins::margins(logitres, data = subset(mroz, kidslt6 == 0)) # no kids < 6 years
```

```{r}
summary(effects_logit_participation_subset)
```



```{r fig23, fig.cap='Logit effect plot', out.width='80%', fig.asp=.75, fig.align='center'}
ggplot(data = summary(effects_logit_participation)) +
  geom_point(aes(factor, AME)) +
  geom_errorbar(aes(x = factor, ymin = lower, ymax = upper)) +
  geom_hline(yintercept = 0, color="lightgrey") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45))
```

You can also extract the marginal effects of a single variable, with `dydx`:

```{r}
head(margins::dydx(mroz, logitres, "educ"))
```

The function `cplot()` provides the commonly needed visual summaries of predictions or average marginal effects conditional on a covariate.

```{r fig24, fig.cap='Marginal effects for logit model', out.width='80%', fig.asp=.75, fig.align='center'}
margins::cplot(logitres, x = "educ", se.type = "shade")
```


Average marginal effects

```{r}
effects_logit <- margins::margins(logitres)
effects_probit <- margins::margins(probitres)
```

```{r}
modelsummary::modelsummary(list("Logit" = effects_logit,"Probit" = effects_probit), stars = TRUE, notes = "Average marginal effects")
```


## Multinomial Logit

A relatively common R function that fits multinomial logit models is `multinom()` from package **nnet**. Let us use the dataset *nels_small* for an example of how multinom works. The variable grades in this dataset is an index, with best grades represented by lower values of grade. We try to explain the choice of a secondary institution (psechoice) only by the high school grade. The variable pschoice

can take one of three values:

- psechoice = 1 no college,
- psechoice = 2 two year college
- psechoice = 3 four year college


Grades are ranked from A (hight grade A+) to F  (lowest Grade F). 


```{r}
data("nels_small", package="PoEdata")
```

```{r}
datasummary_skim(nels_small)
```


```{r}
nels_small %>%
  dplyr::select(psechoice, grades) %>% 
  dplyr::group_by(psechoice) %>%
  datasummary_skim()
```


```{r}
nels.multinom <- nnet::multinom(psechoice~grades, data=nels_small)
summary(nels.multinom)
```

```{r}
summary(margins::marginal_effects(nels.multinom))
```


Recall that a larger numerical value of the variable grades represents a poorer academic performance. If the value of GRADES increases, the probability that high school graduates will choose a two-year or a four-year college goes down, relative to the probability of not attending college.

```{r}
medGrades <- median(nels_small$grades)
fifthPercentileGrades <- quantile(nels_small$grades, .05)
newdat <- data.frame(grades=c(medGrades, fifthPercentileGrades))
pred <- predict(nels.multinom, newdat, "probs")
pred
```


The usual function `predict` can calculate the predicted probabilities of choosing any of the three secondary education levels for two arbitrary grades: one at the median grade in the sample, and the other at the top fifth percent.



Plot cumulative predicted probabilities 

```{r}
marginsplot::cplot(nels.multinom, what ="stacked") 
```



## The Conditional Logit Model

In the multinomial logit model all individuals faced the same external conditions and each individual’s choice is only determined by an individual’s circumstances or preferences. The conditional logit model allows for individuals to face individual-specific external conditions, such as the price of a product. For example, the price of Pepsi, 7-Up, and Coke is potentially different for each customer who purchases soda because customers can shop at different supermarkets and at different times. 

Suppose we want to study the effect of price on an individual’s decision about choosing one of three brands of soft drinks:

- pepsi
- sevenup
- coke

R offers several alternatives that allow fitting conditional logit models, one of which is the function `MCMCmnl()` from the package **MCMCpack** (others are, for instance, `clogit()` in the **survival** package and `mclogit()` in the **mclogit** package). The following code is adapted from @Adkins2014.


```{r}
data("cola", package="PoEdata")
```


```{r}
cola %>%
  dplyr::select(choice, price) %>% 
  dplyr::group_by(choice) %>%
  datasummary_skim()
```


In order to estimate a multinomial logit with the MCMCpckage, the data is to brought in a specific format. It is necessary to record the individuals choice of beverage and each of the three prices he faces.


```{r}

N <- nrow(cola)
N3 <- N/3
price1 <- cola$price[seq(1,N,by=3)] # sequence of each price and choice
price2 <- cola$price[seq(2,N,by=3)]
price3 <- cola$price[seq(3,N,by=3)]


```

The first variable, pepsi, takes every third observation of choice starting at the first row. The variable will contain a one if the person chooses Pepsi and a zero otherwise since this is how the variable choice is coded in the data file. The next variable for Seven-Up starts at 2 and the sequence again increments by 3. Since Seven-Up codes as a 2 the ones and zeros generated by the sequence get multiplied by 2 (to become 2 or 0). Coke is coded as a 3 and its sequence of ones and zeros is multiplied by 3. The three variables are combined into a new one called bev.choice that takes the value of 1,2, or 3 depending on a person's choice of Pepsi, Seven-Up, or Coke.

```{r}
bchoice <- rep("1", N3)  # recode the alternatives to a single variable that takes the value of 1, 2 or 3  epending on a person's choice.
for (j in 1:N3){
  if(cola$choice[3*j-1]==1) bchoice[j] <- "2"
  if(cola$choice[3*j]==1) bchoice[j] <- "3"
}
```

```{r}
library(MCMCpack)
cola.clogit <- MCMCpack::MCMCmnl(bchoice~choicevar(price1, "b2", "1")
                                 +       choicevar(price2, "b2", "2")+
                         choicevar(price3, "b2", "3"),
                       baseline="3", mcmc.method="IndMH")
```


The output from function `multinom `gives coefficient estimates for each level of the response variable **psechoice**, except for the first level, which is the benchmark.
How a change in price affects the choice probability is different for "own price" changes and "cross-price" changes.



```{r}
sclogit <- summary(cola.clogit)
tabMCMC <- as.data.frame(sclogit$statistics)[,1:2]
row.names(tabMCMC)<- c("b2","b11","b12")
```

```{r}
kable(tabMCMC, digits=4, align="c",
      caption="Conditional logit estimates for the 'cola' problem")
```


Choice 3 (coke) being the baseline, which makes $\beta_{13}$ equal to zero.

We can also estimate the probabilities of individual $i$ choosing Pepsi or Sevenup for a given price. 

```{r}
pPepsi <- 1
pSevenup <- 1.25
pCoke <- 1.10
b13 <- 0
b2  <- tabMCMC$Mean[1]
b11 <- tabMCMC$Mean[2]
b12 <- tabMCMC$Mean[3]

# The probability that individual i chooses Pepsi:
PiPepsi <- exp(b11+b2*pPepsi)/
          (exp(b11+b2*pPepsi)+exp(b12+b2*pSevenup)+
                              exp(b13+b2*pCoke))
# The probability that individual i chooses Sevenup:
PiSevenup <- exp(b12+b2*pSevenup)/
          (exp(b11+b2*pPepsi)+exp(b12+b2*pSevenup)+
                              exp(b13+b2*pCoke))
# The probability that individual i chooses Coke:
PiCoke <- 1-PiPepsi-PiSevenup
```

The calculated probabilities are:

 - $p_{i,pepsi}=0.483$ 
 - $p_{i,sevenup}=0.227$ 
 - $p_{i,coke}=0.289$ 

The three probabilities are different for different individuals because different individuals face different prices; in a more complex model other regressors may be included, some of which may reflect individual characteristics.



## Ordered Choice Models

The order of choices in these models is meaningful, unlike the multinomial and conditional logit model we have studied so far. The following example explains the choice of higher education, when the choice variable is psechoice and the only regressor is grades; the dataset, *nels_small* , is already known to us.

The R package **MCMCpack** is again used here, with its function `MCMCoprobit()`.

We treat PSECHOICE as an ordered variable with 1 representing the least favored alternative (no college) and 3 denoting the most favored alternative (four-year college).


```{r}
nels.oprobit <- MCMCoprobit(psechoice ~ grades, 
                            data=nels_small, mcmc=10000)
sOprobit <- summary(nels.oprobit)
tabOprobit <- sOprobit$statistics[, 1:2]
kable(tabOprobit, digits=4, align="c",
      caption="Ordered probit estimates for the 'nels' problem")
```


The results from MCMCoprobit can be translated into the textbook notations as follows:
  
  - $\mu_1$ =− (Intercept)
  - $\beta$ = grades
  - $\mu_2$ = gamma2 − (Intercept) (2.9542- 0.8616)
  
The probabilities for each choice can be calculated as in the next code fragment:
    
```{r}
  mu1 <- -tabOprobit[1]
  b <- tabOprobit[2]
  mu2 <- tabOprobit[3]-tabOprobit[1]
  xGrade <- c(mean(nels_small$grades), 
              quantile(nels_small$grades, 0.05))
  
  # Probabilities:
  prob1 <- pnorm(mu1-b*xGrade)
  prob2 <- pnorm(mu2-b*xGrade)-pnorm(mu1-b*xGrade)
  prob3 <- 1-pnorm(mu2-b*xGrade)
  
  # Marginal effects:
  Dp1DGrades <- -pnorm(mu1-b*xGrade)*b
  Dp2DGrades <- (pnorm(mu1-b*xGrade)-pnorm(mu2-b*xGrade))*b
  Dp3DGrades <- pnorm(mu2-b*xGrade)*b
```
  
For instance, the marginal effect of grades on the probability of attending a four-year college for a student with average grade (6.64) and for a student in the top 5 percent (2.635) are, respectively, `r round(Dp3DGrades[1], digits = 3)` and `r round(Dp3DGrades[[2]], digits = 3)`. These are similar magnitudes as in the the multinonomial logit example.  
  
  
  
## Count data: The Poisson Regression Model 
  
Instead of just 0/1-coded binary data, count data can take any non-negative integer $0, 1, 2, \dots$. If they take very large numbers (like the number of students in a school), they can be approximated reasonably well as continuous variables in a linear models and estimated using OLS. If the numbers are relatively small, this approximation might not work well. 
  
The Poisson regression model is the most basic and convenient model explicitly model explicitly designed for count data. 
  
Poisson regression models can be estimated in R via the `glm()` function with the specification **family= poisson**
    
Estimating the model with **quasipoisson** is to adjust for potential violations of the Poisson distribution. 
  

```{r}
  data(crime1, package='wooldridge')
  
  # Estimate linear model
  lm.res      <-  lm(narr86~pcnv+avgsen+tottime+ptime86+qemp86+inc86+
                       black+hispan+born60, data=crime1)
  # Estimate Poisson model
  Poisson.res <- glm(narr86~pcnv+avgsen+tottime+ptime86+qemp86+inc86+
                       black+hispan+born60, data=crime1, family=poisson)
  # Quasi-Poisson model
  QPoisson.res<- glm(narr86~pcnv+avgsen+tottime+ptime86+qemp86+inc86+
                       black+hispan+born60, data=crime1, family=quasipoisson)
```
  
  
```{r}
  modelsummary(list("OLS" = lm.res,"Poisson" = Poisson.res,"Quasi Poisson" =QPoisson.res), stars=TRUE)
```
  
  
By construction, the parameter estimates are the same but the standard errors are larger for the QMLE. 
  
```{r}
effects_poisson <- margins::margins(Poisson.res)
effects_Qpoisson <- margins::margins(QPoisson.res)
```
  
```{r}
modelsummary(list("OLS" = lm.res,"Poisson" = effects_poisson,"Quasi Poisson" =effects_Qpoisson), stars=TRUE, notes = "Average maringal effects of Poisson and Quasi Poisson Model")
```
  
Note that the original implementation of Quasi Poisson link function does not report information criteria! You can obtain "quasi AIC" by using the **quasi** package.
  
  
## Corner Solution Response: The Tobit Model

Corner solutions describe situations where the variable of interest is continuous but restricted in range. Typically, it cannot be negative. 

The package **censReg** offers the command `censReg()` for estimating a Tobit model. 


We have already estimated labor supply modelled for the women in the dataset *mroz*, ignoring the fact that the hours worked is necessarily non-negative. 

```{r}
# Estimate Tobit model using censReg:
TobitRes1 <- censReg(hours~nwifeinc+educ+exper+I(exper^2)+ 
                       age+kidslt6+kidsge6, data=mroz )
summary(TobitRes1)
```



```{r}
# Partial Effects at the average x:
margEff(TobitRes1)
```


Another alternative for estimating Tobit models is the command **survreg** from the package survival. It is less straightforward to use but more flexible. 

```{r}
# Estimate Tobit model using survreg:
TobitRes2 <- survreg(Surv(hours, hours>0, type="left") ~ nwifeinc+educ+exper+
                       I(exper^2)+age+kidslt6+kidsge6, data=mroz, dist="gaussian")
summary(TobitRes2)
```

```{r}
TobitRes2_margin <- margins::margins(TobitRes2, vce = c("bootstrap")) # sometimes there are convergence problems in the default estimation procedure that is used for estimating variances. Here, in the face of high correlation between some of the predictors, I resort to a bootstrapping procedure.
```


## Censored and Truncated Regression Models

Censored regression models are closely related to Tobit models. In the basic Tobit model we observe $y = y^*$ in the "uncensored" cases with $y^* > 0$ and we only know an upper bound for $y^* \le 0$ if we observe $y 0 = $.  


In this example we are are interested in criminal prognosis of individuals released from prison to reoffend.  


```{r}
data(recid, package='wooldridge')

# Define Dummy for UNcensored observations
recid$uncensored <- recid$cens==0
# Estimate censored regression model:
res<-survreg(Surv(log(durat),uncensored, type="right") ~ workprg+priors+
               tserved+felon+alcohol+drugs+black+married+educ+age, 
             data=recid, dist="gaussian")
# Output:
summary(res)
```


```{r}
res_margins <- margins::margins(res, vce = c("bootstrap")) 
```


## The Heckman, or Sample Selection Model

The models are useful when the sample selection is not random, but whether an individual is in the sample depends on individual characteristics. For example, when studying wage determination for married women, some women are not in the labour force, therefore their wages are zero.

The Heckit procedure involves two steps, estimating both the selection equation and the equation of interest. Function `selection()` in the **sampleSelection** package performs both steps; therefore, it needs both equations among its arguments. (The selection equation is, in fact, a probit model.)

As comparison we estimate a linear model

```{r}
# OLS: log wage regression on LF participants only
wage.lm <- lm(log(wage)~educ+exper, 
              data=subset(mroz, inlf==1))
```

In principle, the selection equation and the wage equation could have exactly the same set of regressors. But this is usually not a good idea. To get a good estimate of the selection model, it is very desirable to have at least one variable in the selection equation that acts like an instrument: that is, a variable that one expects would affect the selection process, but not the wage process, except through selection. In this example, the kids variable might play such a role, if we can assume that women with kids may be more likely to be stay-at-home moms, but for working moms having kids would not affect their hourly pay rate.

```{r}
data("mroz", package='wooldridge')
# One step ML procedure
wage.heckit <- sampleSelection::selection(inlf~age+educ+I(kidslt6+kidsge6)+mtr, # modelling the selection
                                          log(wage)~educ+exper, # regression of interest
                                          data=mroz, method="ml")

```


```{r}
summary(wage.heckit)
```

```{r}
wage.heckit_2S <- sampleSelection::selection(inlf~age+educ+I(kidslt6+kidsge6)+mtr, # modelling the selection
                                             log(wage)~educ+exper, # regression of interest
                                             method = "2step",
                                             data=mroz)

```

```{r}
summary(wage.lm)
```

```{r}
summary(wage.heckit)
```

```{r}
summary(wage.heckit_2S)
```

As you can see, the selection corrections did not have a big effect on any of the parameters of interest. The estimated returns to education and experience are a bit larger in the selection regressions, but the difference is really small. The OLS estimates diverge more from the Heckit estimates, indicating a bias stemming from a sample selection problem. The 2-stage model also provides  the estimated coefficient on the inverse Mills ratio is given for the Heckman model. The fact that it is weakly statistically different from zero is consistent with the idea that selection bias might be a problem in this case. 

## Non-linear Panel data 

Generalized linear models are estimated using the maximum liklihood method, which requires to make strong hypothesis concerning the distribution of the response. When those hypothesis are not valid, the estimator is no longer consistent. That is a general drawback of maximum liklihood estimated adds another layer to that problem. In linear panels, the individual effect can be removed using the *within* or first differences transformation or can be directly estimated. This is not the case for limited dependent variables. When individual effects are estimated, their inconsistency "contaminates" the estimation of $\beta$, which becomes inconsistent as well. This incidental parameter problems leads to abandoning the linear fixed effects model. Three possible alternatives aer most common: 

 - RE model
 - FE model with sufficient statistic